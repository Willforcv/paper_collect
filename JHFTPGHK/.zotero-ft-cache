PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model
Juncai Peng, Yi Liu, Shiyu Tang, Yuying Hao, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Zhiliang Yu, Yuning Du, Qingqing Dang, Baohua Lai, Qiwen Liu, Xiaoguang Hu, Dianhai Yu, Yanjun Ma Baidu Inc.
{pengjuncai, liuyi22}@baidu.com

arXiv:2204.02681v1 [cs.CV] 6 Apr 2022

Abstract
Real-world applications have high demands for semantic segmentation methods. Although semantic segmentation has made remarkable leap-forwards with deep learning, the performance of real-time methods is not satisfactory. In this work, we propose PP-LiteSeg, a novel lightweight model for the real-time semantic segmentation task. Speciﬁcally, we present a Flexible and Lightweight Decoder (FLD) to reduce computation overhead of previous decoder. To strengthen feature representations, we propose a Uniﬁed Attention Fusion Module (UAFM), which takes advantage of spatial and channel attention to produce a weight and then fuses the input features with the weight. Moreover, a Simple Pyramid Pooling Module (SPPM) is proposed to aggregate global context with low computation cost. Extensive evaluations demonstrate that PP-LiteSeg achieves a superior tradeoff between accuracy and speed compared to other methods. On the Cityscapes test set, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5% mIoU/102.6 FPS on NVIDIA GTX 1080Ti. Source code and models are available at PaddleSeg: https://github.com/PaddlePaddle/PaddleSeg.
1. Introduction
Semantic segmentation aims to precisely predict the label of each pixel in an image. It has been widely applied in real-world applications, e.g. medical imaging [30], autonomous driving [10, 24], video conferencing [5], semiautomatic annotation [9]. As a fundamental task in computer vision, semantic segmentation has attracted a lot of attention from researchers [13, 16].
With the remarkable progress of deep learning, a lot of semantic segmentation methods have been proposed based on convolutional neural network [3,15,18,26,29]. FCN [18] is the ﬁrst fully convolutional network trained in an endto-end and pixel-to-pixel way. It also presents the primitively encoder-decoder architecture in semantic segmenta-

Figure 1. The comparison of segmentation accuracy (mIoU) and inference speed (FPS) on the Cityscapes test set. The red dots represent our proposed PP-LiteSeg. The testing device is NVIDIA GTX 1080Ti. The experimental results demonstrate that PPLiteSeg achieves state-of-the-art trade-off between accuracy and speed.
tion, which is widely adopted in subsequent methods. To achieve higher accuracy, PSPNet [29] utilizes a pyramid pooling module to aggregate global context and SFNet [15] proposes a ﬂow alignment module to strengthen the feature representations.
Yet, these models are not suitable for real-time applications because of their high computation cost. To accelerate the inference speed, Espnetv2 [21] utilizes light-weight convolutions to extract features from an enlarged receptive ﬁeld. BiSeNetV2 [26] proposes bilateral segmentation network and extracts the detail features and semantic features separately. STDCSeg [8] designs a new backbone named STDC to improve the computation efﬁciency. However, these models do not achieve satisfactory trade-off between accuracy and speed.
In this work, we propose a real-time and hand-craft net-

1

Figure 2. The architecture overview. PP-LiteSeg consists of three modules: encoder, aggregation and decoder. A lightweight network is used as encoder to extract the features from different levels. The Simple Pyramid Pooling Module (SPPM) is responsible for aggregating the global context. The Flexible and Lightweight Decoder (FLD) fuses the detail and semantic features from high level to low level and outputs the result. Remarkably, FLD uses the Uniﬁed Attention Fusion Module (UAFM) to strengthen feature representations.

work named PP-LiteSeg. As illustrated in Figure 2, PPLiteSeg adopts the encode-decoder architecture and consists of three novel modules: Flexible and Lightweight Decoder (FLD), Uniﬁed Attention Fusion Module (UAFM) and Simple Pyramid Pooling Module (SPPM). The motivations and details of these modules are introduced as follows.
The encoder in semantic segmentation models extracts hierarchical features, and the decoder fuses and unsamples features. For the features from low level to high level in encoder, the number of channels increases and the spatial size decreases, which is an efﬁcient design. For the features from high level to low level in decoder, the spatial size increases, while the number of channels are the same in recent models [8, 15]. Therefore, we present a Flexible and Lightweight Decoder (FLD), which gradually reduces the channels and increases the spatial size for the features. Besides, the volume of proposed decoder can be easily adjusted according to the encoder. The ﬂexible design balances the computation complexity of encoder and decoder, which makes the overall model more efﬁcient.
Strengthening feature representations is a crucial way to improve segmentation accuracy [11, 15, 25]. It is usually achieved through fusing the low-level and high-level features in a decoder. However, the fusion modules in existing methods usually suffer from high computation cost. In this work, we propose a Uniﬁed Attention Fusion Module (UAFM) to strengthen feature representations efﬁciently. As shown in Figure 4, UAFM ﬁrst takes advantage of the attention module to produce weight α, and then fuses the input features with α. In UAFM, there are two kinds of attention modules, i.e. spatial and channel attention modules, which exploit inter-spatial and inter-channel relationships of the input features.
Contextual aggregation is another key to promote seg-

mentation accuracy, but previous aggregation modules are time-consuming for real-time networks. Based on the framework of PPM [29], we design a Simple Pyramid Pooling Module (SPPM), which reduces the intermediate and output channels, removes the short-cut, and replaces the concatenate operation with an add operation. Experimental results show SPPM contributes to the segmentation accuracy with low computation cost.
We evaluate the proposed PP-LiteSeg through extensive experiments on Cityscapes and CamVid dataset. As illustrated in Figure 1, PP-LiteSeg achieves a superior tradeoff between segmentation accuracy and inference speed. Speciﬁcally, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5% mIoU/102.6 FPS on the Cityscapes test set.
Our main contributions are summarized as follows:
• We propose a Flexible and Lightweight Decoder (FLD), which mitigates the redundancy of the decoder and balances the computation cost of the encoder and decoder.
• We present a Uniﬁed Attention Fusion Module (UAFM) that utilizes channel and spatial attention to strengthen the feature representations.
• We propose a Simple Pyramid Pooling Module (SPPM) to aggregate global context. SPPM promotes the segmentation accuracy with minor extra inference time.
• Based on the above modules, we propose PP-LiteSeg, a real-time semantic segmentation model. Extensive experiments demonstrate our SOTA performance in terms of accuracy and speed.

2

2. Related Work
2.1. Semantic Segmentation
Deep learning has helped semantic segmentation make remarkable leap-forwards. FCN [18] is the ﬁrst full convolutional network for semantic segmentation. It is trained in an end-to-end and pixel-to-pixel way. Besides, images with arbitrary size can be segmented by FCN. Following the design of FCN, various methods have been proposed in later. Segnet [1] applies the indices of max-pooling operation in encoder to upsampling operation in decoder. Therefore, the information in decoder is reused and the decoder produces reﬁned features. PSPNet [29] proposes the pyramid pooling module to aggregate local and global information, which is effective for segmentation accuracy. Besides, recent semantic segmentation methods [11,17] utilize the transformer architecture to achieve better accuracy.
2.2. Real-time Semantic Segmentation
To fulﬁll the real-time demands of semantic segmentation, lots of methods have been proposed, e.g lightweight module design [8, 21], dual-branch architecture [26, 27], early-downsampling strategy [23], multiscale image cascade network [28]. ENet [23] uses an early-downsampling strategy to reduce the computation cost of processing large images and feature maps. For efﬁciency, ICNet [28] designs a multi-resolution image cascade network. Based on bilateral segmentation network, Bisenet [26] extracts the detail features and semantic features separately. The bilateral network is lightweight, so the inference speed is fast. STDCSeg [8] proposes the channel-reduced receptive ﬁeld-enlarged STDC module and designs an efﬁcient backbone, which can strengthen the feature representations with low computation cost. To eliminate the redundancy in two-branch network, STDCSeg guides the features with detailed ground truth, so the efﬁciency is further improved. Espnetv2 [21] uses group point-wise and depth-wise dilated separable convolutions to learn features from an enlarged receptive ﬁeld in a computation friendly manner.
2.3. Feature Fusion Module
The feature fusion module is commonly used in semantic segmentation to strengthen feature representations. In addition to the element-wise summation and concatenation methods, researchers propose several methods as follows. In BiSeNet [26], the BGA module employs element-wise mul method to fuse the features from the spatial and contextual branches. To enhance the features with high-level context, DFANet [14] fuses features in a stage-wise and subnet-wise way. To tackle the problem of misalignment, SFNet [15] and AlignSeg [12] ﬁrst learn the transformation offsets through a CNN module, and then apply the transformation offsets to grid sample operation to gener-

Figure 3. (a) The conventional encoder-decoder, in which the decoder keeps the features channels the same. (b) The encoder and proposed Flexible and Lightweight Decoder (FLD). FLD gradually reduces the channels for the features from high level to low level. Moreover, the volume of FLD is adjusted to conform to the encoder.
ate the reﬁned feature. In detail, SFNet designs the ﬂow alignment module. AlignSeg designs aligned feature aggregation module and the aligned context modeling module. FaPN [11] solves the feature misalignment problem by applying the transformation offsets to deformable convolution.
3. Proposed Method
In this section, we ﬁrst introduce Flexible and Lightweight Decoder (FLD), Uniﬁed Attention Fusion Module (UAFM) and Simple Pyramid Pooling Module (SPPM), respectively. Then, we present the architecture of PP-LiteSeg for real-time semantic segmentation.
3.1. Flexible and Lightweight Decoder
Encoder-decoder architecture has been proved to be effective for semantic segmentation. In general, the encoder utilizes a series of layers grouped into several stages to extract hierarchical features. For the features from low level to high level, the number of channels gradually increases and the spatial size of the features decreases. This design balances the computation cost of each stage, which ensures the efﬁciency of the encoder. The decoder also has several stages, which are responsible for fusing and upsampling features. Although the spatial size of features increases from high level to low level, the decoder in recent lightweight models keeps the feature channels the same in all levels. Therefore, the computation cost of shallow stage is much larger than that of the deep stage, which leads to the computation redundancy in shallow stage. To improve the efﬁciency of decoder, we present a Flexible and Lightweight Decoder (FLD). As illustrated in Figure 3, FLD gradually decreases the channels of the features from

3

high level to low level. FLD can easily adjusts the computation cost to achieve better balance between encoder and decoder. Although the channels of features in FLD are decreasing, our experiments show that PP-LiteSeg achieves competitive accuracy compared to other methods.
3.2. Uniﬁed Attention Fusion Module
As discussed above, fusing multi-level features is essential to achieve high segmentation accuracy. In addition to the element-wise summation and concatenation methods, researchers propose several methods, e.g. SFNet [15], FaPN [11] and AttaNet [25]. In this work, we propose a Uniﬁed Attention Fusion Module (UAFM) that applies channel and spatial attention to enrich the fused feature representations. UAFM Framework. As illustrated in Figure 4 (a), UAFM utilizes an attention module to produce the weight α, and fuses the input features with α by Mul and Add operations. In detail, the input features are denoted as Fhigh and Flow. Fhigh is the output of the deeper module, and Flow is the counterpart from the encoder. Note that they have same channels. UAFM ﬁrst makes use of bilinear interpolation operation to upsample Fhigh to the same size of Flow, while the upsampled feature is denoted as Fup. Then, the attention module takes Fup and Flow as input and produces the weight α. Note that, the attention module can be a plugin, such as spatial attention module, channel attention module, etc. After that, to obtain attention-weighted features, we apply the element-wise Mul operation to Fup and Flow, respectively. Finally, UAFM performs element-wise addition for the attention-weighted features and outputs the fused feature. We can formulate the above procedure as equation 1.

Fup = U psample(Fhigh)

α = Attention(Fup, Flow)

(1)

Fout = Fup · α + Flow · (1 − α)

Spatial Attention Module. The motivation of the spatial
attention module is exploiting the inter-spatial relationship
to produce a weight, which represents the importance of
each pixel in the input features. As shown in Figure 4 (b), given the input features, i.e., Fup ∈ RC×H×W and Flow ∈ RC×H×W , we ﬁrst perform mean and max operations along the channel axis to generates four features, of which the dimension is R1×H×W . Afterwards, these four features are concatenated to a feature Fcat ∈ R4×H×W . For the concatenated feature, the convolution and sigmoid operations are applied to output α ∈ R1×H×W . The formu-
lation of the spatial attention module is shown in equation
2. Furthermore, the spatial attention module can be ﬂexi-
ble, e.g. removing the max operation to reduce computation
cost.

Figure 4. (a) The framework of Uniﬁed Attention Fusion Module (UAFM). (b) The Spatial Attention Module. (c) The Channel Attention Module. UAFM ﬁrst uses spatial or channel attention modules to produce the weight α, and then fuses the input features with Mul and Add operation.

Fcat = Concat(M ean(Fup), M ax(Fup),

M ean(Flow), M ax(Flow))

(2)

α = Sigmoid(Conv(Fcat))

Channel Attention Module. The key concept of the chan-

nel attention module is leveraging the inter-channel rela-

tionship to generate a weight, which indicates the impor-

tance of each channel in the input features. As shown in

Figure 4 (b), the proposed channel attention module utilizes

average-pooling and max-pooling operations to squeeze the

spatial dimension of input features. This procedure generates four features with the dimension RC×1×1. Then, it

concatenates these four features along the channel axis and

performs convolution and sigmoid operations to produce a weight α ∈ RC×1×1. In short, the procedures of the chan-

nel attention module can be formulated as equation 3.

Fcat = Concat(AvgP ool(Fup), M axP ool(Fup),

AvgP ool(Flow), M axP ool(Flow))

(3)

α = Sigmoid(Conv(Fcat))

3.3. Simple Pyramid Pooling Module

As shown in Figure 5, we propose a Simple Pyramid Pooling Module (SPPM). It ﬁrst leverages the pyramid pooling module to fuse the input feature. The pyramid pooling module has three global-average-pooling operations and

4

Model

Encoder Channels in Decoder

PP-LiteSeg-T STDC1

32, 64, 128

PP-LiteSeg-B STDC2

64, 96, 128

Table 1. The details of our proposed PP-LiteSeg.

Figure 5. Simple Pyramid Pooling Module (SPPM). Conv denotes convolution, batch norm and relu operations. The bin sizes of global-average-pooling are 1 × 1, 2 × 2 and 4 × 4 respectively.
the bin sizes are 1 × 1, 2 × 2, and 4 × 4 respectively. Afterwards, the output features are followed by the convolution and upsampling operations. For the convolution operation, the kernel size is 1×1 and the output channel is smaller than the input channel. Finally, we add these upsampled features and apply a convolution operation to produce the reﬁned feature. Compared to original PPM, SPPM reduces the intermediate and output channels, removes the short-cut, and replaces the concatenate operation with an addition operation. Consequently, SPPM is more efﬁcient and suitable for real-time models.
3.4. Network Architecture
The architecture of the proposed PP-LiteSeg is demonstrated in Figure 2. PP-LiteSeg mainly comprises three modules: encoder, aggregation and decoder.
Firstly, given an input image, PP-Lite utilizes a common lightweight network as encoder to extract hierarchical features. In detail, we choose the STDCNet [8] for its outstanding performance. The STDCNet has 5 stages, the stride for each stage is 2, so the ﬁnal feature size is 1/32 of the input image. As shown in Table 1, we present two versions of PP-LiteSeg, i.e., PP-LiteSeg-T and PP-LiteSeg-B, of which the encoder is STDC1 and STDC2 respectively. The PPLiteSeg-B achieves higher segmentation accuracy, while the inference speed of PP-LiteSeg-T is faster. It is worth noting that we apply the SSLD [7] method to the training of the encoder and obtain enhanced pre-trained weights, which is beneﬁcial for the convergence of segmentation training.
Secondly, PP-LiteSeg adopts SPPM to model the longrange dependencies. Taking the output feature of the encoder as input, SPPM produces a feature that contains global context information.
Finally, PP-LiteSeg utilizes our proposed FLD to gradually fuse multi-level features and output the resulting image. Speciﬁcally, FLD consists of two UAFM and a segmentation head. For efﬁciency, we adopt spatial attention module in UAFM. Each UAFM takes two features as input, i.e., a low-level feature extracted by the stages of the encoder, a

high-level feature generated by SPPM or the deeper fusion module. The latter UAFM outputs fused features with the down-sample ratio of 1/8. In the segmentation head, we perform Conv-BN-Relu operation to reduce the channels of the 1/8 down-sample feature to the number of classes. An upsampling operation is followed to expand the feature size to the input image size, and an argmax operation predicts the label of each pixel. The cross entropy loss with Online Hard Example Mining is adopted to optimize our models.
4. Experiments
In this section, we ﬁrst introduce the datasets and implementation details. Then, we compare the experimental results in terms of accuracy and inference speed with other state-of-the-art real-time methods. Finally, we demonstrate the effectiveness of the proposed modules with the ablation study.
4.1. Datasets and Implementation Details
Cityscapes. The Cityscapes [6] is a large-scale dataset for urban segmentation. It contains 5,000 ﬁne annotated images, which are further split into 2975, 500, and 1525 images for training, validation and testing, respectively. The resolution of the images is 2048 × 1024, which poses great challenges for the real-time semantic segmentation methods. The annotated images have 30 classes and our experiments only use 19 classes for a fair comparison with other methods. CamVid. Cambridge-driving Labeled Video Database (CamVid) [2] is a small-scale dataset for road scene segmentation. There are 701 images with high-quality pixellevel annotations, in which 367, 101 and 233 images are chosen for training, validation and testing respectively. The images have the same resolution of 960 × 720. The annotated images provide 32 categories, of which the subset of 11 categories are used in our experiments. Training Settings. Following the common setting, the stochastic gradient descent (SGD) algorithm with 0.9 momentum is chosen as an optimizer. We also adopt the warm-up strategy and the “poly” learning rate scheduler. For Cityscapes, the batch size is 16, the max iterations are 160,000, the initial learning rate is 0.005, and the weight decay in the optimizer is 5e−4. For CamVid, the batch size is 24, the max iterations is 1,000, the initial learning rate is 0.01, and the weight decay is 1e−4. For data augmentation, we make use of random scaling, random cropping, random

5

Model
ENet [23] ICNet [28] ESPNet [20] ESPNetV2 [21] SwiftNet [22] BiSeNetV1 [27] BiSeNetV1-L [27] BiSeNetV2 [26] BiSeNetV2-L [26] FasterSeg [4] SFNet [15] STDC1-Seg50 [8] STDC2-Seg50 [8] STDC1-Seg75 [8] STDC2-Seg75 [8] PP-LiteSeg-T1 PP-LiteSeg-B1 PP-LiteSeg-T2 PP-LiteSeg-B2

Encoder
PSPNet50 ESPNet ESPNetV2 ResNet18 Xception39 ResNet18
DF1 STDC1 STDC2 STDC1 STDC2 STDC1 STDC2 STDC1 STDC2

Resolution
512 × 1024 1024 × 2048 512 × 1024 512 × 1024 1024 × 2048 768 × 1536 768 × 1536 512 × 1024 512 × 1024 1024 × 2048 1024 × 2048 512 × 1024 512 × 1024 768 × 1536 768 × 1536 512 × 1024 512 × 1024 768 × 1536 768 × 1536

mIoU (%) val test - 58.3 - 69.5 - 60.3 66.4 66.2 75.4 75.5 69.0 68.4 74.8 74.7 73.4 72.6 75.8 75.3 73.1 71.5 - 74.5 72.2 71.9 74.2 73.4 74.5 75.3 77.0 76.8 73.1 72.0 75.3 73.9 76.0 74.9 78.2 77.5

FPS
76.9 30.3 112.9
39.9 105.8 65.5 156 47.3 163.9 121 250.4 188.6 126.7 97.0 273.6 195.3 143.6 102.6

Table 2. The comparisons with state-of-the-art real-time methods on Cityscapes. The training and inference setting refer to the implementation details.

horizontal ﬂipping, random color jittering and normalization. The random scale ranges in [0.125, 1.5], [0.5, 2.5] for Cityscapes and Camvid respectively. The cropped resolution of Cityscapes is 1024×512, and the cropped resolution of CamVid is 960 × 720. All of our experiments are conducted on Tesla V100 GPU using PaddlePaddle1 [19]. Code and pretrained models are available at PaddleSeg2 [16]. Inference Settings. For a fair comparison, we export PPLiteSeg to ONNX and utilize TensorRT to execute the model. Similar to other methods [8, 26], an image from Cityscapes is ﬁrst resized to 1024 × 512 and 1536 × 768, then the inference model takes the scaled image and produces the predicted image, ﬁnally, the predicted image is resized to the original size of the input image. The cost of the three steps is counted as the inference time. For CamVid, the inference model takes the original image as input, while the resolution is 960 × 720. We conduct all inference experiments under CUDA 10.2, CUDNN 7.6, TensorRT 7.1.3 on NVIDIA 1080Ti GPU. We employ the standard mIoU for segmentation accuracy comparison and FPS for inference speed comparison.
4.2. Experiments on Cityscapes
4.2.1 Comparisons with State-of-the-arts
With the training and inference setting mentioned above, we compare the proposed PP-LiteSeg with previous stateof-the-art real-time models on Cityscapes. For fair comparison, we evaluate PP-LiteSeg-T and PP-LiteSeg-B at two resolutions, i.e., 512 × 1024 and 768 × 1536. Table 2
1https://github.com/PaddlePaddle/Paddle 2https://github.com/PaddlePaddle/PaddleSeg

Model

FLD SPPM UAFM mIoU(%) FPS

Baseline

77.50 110.9

PP-LiteSeg-B2

77.67 109.7

PP-LiteSeg-B2

77.76 106.3

PP-LiteSeg-B2

77.89 105.5

PP-LiteSeg-B2

78.21 102.6

Table 3. Ablation study for our proposed modules on the validation set of Cityscapes. The baseline model is PP-LiteSeg-B2 without the proposed modules.

presents the model information, input resolution, mIoU, and FPS of various approaches. Figure 1 provides an intuitive comparison of segmentation accuracy and inference speed. The experimental evaluations demonstrate that the proposed PP-LiteSeg achieves a state-of-the-art trade-off between accuracy and speed against other methods. Speciﬁcally, we can observe PP-LiteSeg-T1 achieves 273.6 FPS and 72.0% mIoU, which means the fastest inference speed and competitive accuracy. With the resolution of 768 × 1536, PPLiteSeg-B2 achieves the best accuracy, i,e. 78.2% mIoU for the validation set, 77.5% mIoU for the test set. In addition, with same encoder and input resolution as STDC-Seg, PPLiteSeg shows better performance.
4.2.2 Ablation study
Ablation experiments are conducted to demonstrate the effectiveness of the proposed modules. The experiments choose PP-LiteSeg-B2 in the comparison and use the same training and inference setting. The baseline model is the PP-LiteSeg-B2 without the proposed modules, while the number of features channels is 96 in decoder and the fusion method is element-wise summation. Table 3 presents the quantitative results of our ablation study. We can ﬁnd that the FLD in PP-LiteSeg-B2 improves the mIoU by 0.17%. Adding SPPM and UAFM also improve the segmentation accuracy, while the inference speed slightly decreases. Based on three proposed modules, PP-LiteSeg-B2 achieves 78.21 mIoU with 102.6 FPS. The mIoU is boosted by 0.71% compared to the baseline model. Figure 6 provides the qualitative comparisons. We can observe that the predicted image becomes more consistent with the ground truth when adding FLD, SPPM and UAFM one by one. In short, our proposed modules are effective for semantic segmentation.
4.3. Experiments on CamVid
To further demonstrate the capability of PP-LiteSeg, we also conduct experiments on the CamVid dataset. Similar to other works, the input resolution for training and inference is 960 × 720. As shown in Table 4, PP-LiteSeg-T achieves 222.3 FPS, which is over 12.5% faster than other methods. PP-LiteSeg-B achieves the best accuracy, i.e., 75.0% mIoU

6

Figure 6. The qualitative comparison on the Cityscapes validation set. (a)-(e) represent the predicted image of baseline, baseline+FLD, baseline+FLD+SPPM, baseline+FLD+UAFM and baseline+FLD+SPPM+UAFM respectively, (f) denotes the ground truth.

Model ENet [23] ICNet [28] DFANet A [14] SwiftNet [22] BiSeNetV1 [27] BiSeNetV1-L [27] BiSeNetV2 [26] BiSeNetV2-L [26] STDC1-Seg [8] STDC2-Seg [8] PP-LiteSeg-T PP-LiteSeg-B

Encoder -
PSPNet50 Xception A ResNet18 Xception39 ResNet18
STDC1 STDC2 STDC1 STDC2

mIoU (%) 51.3 67.1 64.7 72.58 65.6 68.7 72.4 73.2 73.0 73.9 73.3 75.0

FPS 61.2 34.5 120
175 116.3 124.5 32.7 197.6 152.2 222.3 154.8

Table 4. The comparisons with state-of-the-art real-time methods on CamVid test set. The input resolution of all methods is 960 × 720.

with 154.8 FPS. Overall, the comparisons show PP-LiteSeg achieves a state-of-the-art trade-off between accuracy and speed on Camvid.
5. Conclusions
In this paper, we focus on designing a novel real-time network for semantic segmentation. First, Flexible and Lightweight Decoder (FLD) is proposed to improve the efﬁciency of previous decoder. Then, we present a Uniﬁed Attention Fusion Module (UAFM), which is effective for strengthening feature representations. Furthermore, we propose a Simple Pyramid Pooling Module (SPPM) to aggregate global context with low computation cost. Based on these novel modules, we propose PP-LiteSeg, a realtime semantic segmentation network. Extensive experiments demonstrate that PP-LiteSeg achieves a state-of-theart trade-off between segmentation accuracy and inference

speed. In future work, we plan to apply our methods into more tasks, such as matting and interactive segmentation.
References
[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481–2495, 2017. 3
[2] Gabriel J Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recognition using structure from motion point clouds. In European conference on computer vision, pages 44–57. Springer, 2008. 5
[3] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801–818, 2018. 1
[4] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang Wang. Fasterseg: Searching for faster real-time semantic segmentation. arXiv preprint arXiv:1912.10917, 2019. 6
[5] Lutao Chu, Yi Liu, Zewu Wu, Shiyu Tang, Guowei Chen, Yuying Hao, Juncai Peng, Zhiliang Yu, Zeyu Chen, Baohua Lai, et al. Pp-humanseg: Connectivity-aware portrait segmentation with a large-scale teleconferencing video dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 202–209, 2022. 1
[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213–3223, 2016. 5
[7] Cheng Cui, Ruoyu Guo, Yuning Du, Dongliang He, Fu Li, Zewu Wu, Qiwen Liu, Shilei Wen, Jizhou Huang, Xiaoguang Hu, et al. Beyond self-supervision: A simple yet effective network distillation alternative to improve backbones. arXiv preprint arXiv:2103.05959, 2021. 5

7

[8] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, and Xiaolin Wei. Rethinking bisenet for real-time semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9716–9725, 2021. 1, 2, 3, 5, 6, 7
[9] Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeﬂow: Achieving practical interactive segmentation with edge-guided ﬂow. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1551–1560, 2021. 1
[10] Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change Loy. Learning lightweight lane detection cnns by self attention distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1013–1021, 2019. 1
[11] Shihua Huang, Zhichao Lu, Ran Cheng, and Cheng He. Fapn: Feature-aligned pyramid network for dense image prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 864–873, 2021. 2, 3, 4
[12] Zilong Huang, Yunchao Wei, Xinggang Wang, Wenyu Liu, Thomas S Huang, and Humphrey Shi. Alignseg: Featurealigned segmentation networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):550–557, 2021. 3
[13] Fahad Lateef and Yassine Ruichek. Survey on semantic segmentation using deep learning techniques. Neurocomputing, 338:321–348, 2019. 1
[14] Hanchao Li, Pengfei Xiong, Haoqiang Fan, and Jian Sun. Dfanet: Deep feature aggregation for real-time semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9522– 9531, 2019. 3, 7
[15] Xiangtai Li, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang, Kuiyuan Yang, Shaohua Tan, and Yunhai Tong. Semantic ﬂow for fast and accurate scene parsing. In European Conference on Computer Vision, pages 775–793. Springer, 2020. 1, 2, 3, 4, 6
[16] Yi Liu, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Baohua Lai, and Yuying Hao. Paddleseg: A high-efﬁcient development toolkit for image segmentation. arXiv preprint arXiv:2101.06175, 2021. 1, 6
[17] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883, 2021. 3
[18] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431–3440, 2015. 1, 3
[19] Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. Paddlepaddle: An open-source deep learning platform from industrial practice. Frontiers of Data and Domputing, 1(1):105–115, 2019. 6
[20] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh Hajishirzi. Espnet: Efﬁcient spatial pyramid of dilated convolutions for semantic segmentation.

In Proceedings of the european conference on computer vision (ECCV), pages 552–568, 2018. 6
[21] Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi. Espnetv2: A light-weight, power efﬁcient, and general purpose convolutional neural network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9190–9200, 2019. 1, 3, 6
[22] Marin Orsic, Ivan Kreso, Petra Bevandic, and Sinisa Segvic. In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12607–12616, 2019. 6, 7
[23] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural network architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147, 2016. 3, 6, 7
[24] Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand, and Hong Zhang. A comparative study of real-time semantic segmentation for autonomous driving. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 587–597, 2018. 1
[25] Qi Song, Kangfu Mei, and Rui Huang. Attanet: Attentionaugmented network for fast and accurate scene parsing. In AAAI, 2021. 2, 4
[26] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, and Nong Sang. Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation. International Journal of Computer Vision, 129(11):3051–3068, 2021. 1, 3, 6, 7
[27] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 325–341, 2018. 3, 6, 7
[28] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proceedings of the European conference on computer vision (ECCV), pages 405– 420, 2018. 3, 6, 7
[29] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881–2890, 2017. 1, 2, 3
[30] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support, pages 3–11. Springer, 2018. 1

8

