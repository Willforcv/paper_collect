This paper has been accepted for publication in the IEEE Transactions on Robotics.

Please cite the paper as: H. Yang, J. Shi, and L. Carlone,

1

“TEASER: Fast and Certiﬁable Point Cloud Registration”, IEEE Transactions on Robotics (T-RO), 2020.

TEASER: Fast and Certiﬁable Point Cloud Registration

Heng Yang, Jingnan Shi, Luca Carlone

Correspondence-free Correspondence-based

arXiv:2001.07715v2 [cs.RO] 17 Oct 2020

Abstract—We propose the ﬁrst fast and certiﬁable algorithm for the registration of two sets of 3D points in the presence of large amounts of outlier correspondences. A certiﬁable algorithm is one that attempts to solve an intractable optimization problem (e.g., robust estimation with outliers) and provides readily checkable conditions to verify if the returned solution is optimal (e.g., if the algorithm produced the most accurate estimate in the face of outliers) or bound its sub-optimality or accuracy.
Towards this goal, we ﬁrst reformulate the registration problem using a Truncated Least Squares (TLS) cost that makes the estimation insensitive to a large fraction of spurious correspondences. Then, we provide a general graph-theoretic framework to decouple scale, rotation, and translation estimation, which allows solving in cascade for the three transformations. Despite the fact that each subproblem (scale, rotation, and translation estimation) is still non-convex and combinatorial in nature, we show that (i) TLS scale and (component-wise) translation estimation can be solved in polynomial time via an adaptive voting scheme, (ii) TLS rotation estimation can be relaxed to a semideﬁnite program (SDP) and the relaxation is tight, even in the presence of extreme outlier rates, and (iii) the graph-theoretic framework allows drastic pruning of outliers by ﬁnding the maximum clique. We name the resulting algorithm TEASER (Truncated least squares Estimation And SEmideﬁnite Relaxation). While solving large SDP relaxations is typically slow, we develop a second fast and certiﬁable algorithm, named TEASER++, that uses graduated nonconvexity to solve the rotation subproblem and leverages DouglasRachford Splitting to efﬁciently certify global optimality.
For both algorithms, we provide theoretical bounds on the estimation errors, which are the ﬁrst of their kind for robust registration problems. Moreover, we test their performance on standard benchmarks, object detection datasets, and the 3DMatch scan matching dataset, and show that (i) both algorithms dominate the state of the art (e.g., RANSAC, branch-&-bound, heuristics) and are robust to more than 99% outliers when the scale is known, (ii) TEASER++ can run in milliseconds and it is currently the fastest robust registration algorithm, and (iii) TEASER++ is so robust it can also solve problems without correspondences (e.g., hypothesizing all-to-all correspondences) where it largely outperforms ICP and it is more accurate than Go-ICP while being orders of magnitude faster. We release a fast open-source C++ implementation of TEASER++.
Index Terms—3D registration, scan matching, point cloud alignment, robust estimation, certiﬁable algorithms, outliersrobust estimation, object pose estimation, 3D robot vision.
SUPPLEMENTARY MATERIAL
• Video: https://youtu.be/xib1RSUoeeQ
• Code: https://github.com/MIT-SPARK/TEASER-plusplus
I. INTRODUCTION
Point cloud registration (also known as scan matching or
point cloud alignment) is a fundamental problem in robotics
H. Yang, J. Shi, and L. Carlone are with the Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA 02139, USA, Email: {hankyang,jnshi,lcarlone}@mit.edu
The authors would like to thank the associate editor and the anonymous reviewers for their constructive feedback, and Álvaro Parra and Russ Tedrake for providing comments on an early draft of this paper. This work was partially funded by ARL DCIST CRA W911NF-17-2-0181, ONR RAIDER N00014-18-1-2828, Lincoln Laboratory “Resilient Perception in Degraded Environments”, and the Google Daydream Research Program.

(a) Input

(b) RANSAC

(c) TEASER++

(d) Input

(e) ICP

(f) TEASER++

Object Localization

(g) Correspondences

(h) TEASER++

Scan Matching

(i) Correspondences

(j) TEASER++

Fig. 1. We address 3D registration in the realistic case where many point-topoint correspondences are outliers due to incorrect matching. (a) Bunny dataset (source cloud in blue and target cloud in magenta) with 95% outliers (shown as red lines) and 5% inliers (shown as green lines). Existing algorithms, such as RANSAC (b), can produce incorrect estimates without notice even after running for 10,000 iterations. Our certiﬁable algorithm, TEASER, largely outperforms the state of the art in terms of robustness and accuracy, and a fast implementation, named TEASER++ (c), computes accurate estimates in milliseconds even with extreme outlier rates and ﬁnds the small set of inliers (shown as green dots). The unprecedented robustness of TEASER++ enables the solution of correspondence-free registration problems (d), where ICP (e) fails without a good initial guess, while TEASER++ (f) succeeds without requiring an initial guess. We test our approach in challenging object localization (g-h) and scan matching (i-j) RGB-D datasets, using both traditional features (i.e., FPFH [1]) and deep-learned features (i.e., 3DSmoothNet [2]).

and computer vision and consists in ﬁnding the best transformation (rotation, translation, and potentially scale) that aligns two point clouds. It ﬁnds applications in motion estimation and 3D reconstruction [3], [4], [5], [6], object recognition and localization [7], [8], [9], [10], panorama stitching [11], and

2

medical imaging [12], [13], to name a few. When the ground-truth correspondences between the point
clouds are known and the points are affected by zero-mean Gaussian noise, the registration problem can be readily solved, since elegant closed-form solutions [14], [15] exist for the case of isotropic noise. In practice, however, the correspondences are either unknown, or contain many outliers, leading these solvers to produce poor estimates. Large outlier rates are typical of 3D keypoint detection and matching [16].
Commonly used approaches for registration with unknown or uncertain correspondences either rely on the availability of an initial guess for the unknown transformation (e.g., the Iterative Closest Point, ICP [17]), or implicitly assume the presence of a small set of outliers (e.g., RANSAC [18]1). These algorithms may fail without notice (Fig. 1 (b),(e)) and may return estimates that are arbitrarily far from the ground-truth transformation. In general, the literature is divided between heuristics, that are fast but brittle, and global methods that are guaranteed to be robust, but run in worst-case exponential time (e.g., branch-&-bound methods, such as Go-ICP [20]).
This paper is motivated by the goal of designing an approach that (i) can solve registration globally (without relying on an initial guess), (ii) can tolerate extreme amounts of outliers (e.g., when 99% of the correspondences are outliers), (iii) runs in polynomial time and is fast in practice (i.e., can operate in real-time on a robot), and (iv) provides formal performance guarantees. In particular, we look for a posteriori guarantees, e.g., conditions that one can check after executing the algorithm to assess the quality of the estimate. This leads to the notion of certiﬁable algorithms, i.e., an algorithm that attempts to solve an intractable problem and provides checkable conditions on whether it succeeded [21], [22], [23], [24], [25], [26]. The interested reader can ﬁnd a broader discussion in Appendix A.
Contribution. This paper proposes the ﬁrst certiﬁable algorithm for 3D registration with outliers. We reformulate the registration problem using a Truncated Least Squares (TLS) cost (presented in Section IV), which is insensitive to a large fraction of spurious correspondences but leads to a hard, combinatorial, and non-convex optimization.
The ﬁrst contribution (Section V) is a general framework to decouple scale, rotation, and translation estimation. The idea of decoupling rotation and translation has appeared in related work, e.g., [27], [28], [19]. The novelty of our proposal is fourfold: (i) we develop invariant measurements to estimate the scale ([28], [19] assume the scale is given), (ii) we make the decoupling formal under the assumption of unknown-butbounded noise [29], [30], (iii) we provide a general graphtheoretic framework to derive these invariant measurements, and (iv) we show that this framework allows pruning a large amount of outliers by ﬁnding the maximum clique of the graph deﬁned by the invariant measurements (Section VI).
The decoupling allows solving in cascade for scale, rotation, and translation. However, each subproblem is still combinatorial in nature. Our second contribution is to show that (i)
1RANSAC’s runtime grows exponentially with the outlier ratio [19] and it typically performs poorly with large outlier rates (see Sections II and XI).

in the scalar case TLS estimation can be solved exactly in polynomial time using an adaptive voting scheme, and this enables efﬁcient estimation of the scale and the (componentwise) translation (Section VII); (ii) we can formulate a tight semideﬁnite programming (SDP) relaxation to estimate the rotation and establish a posteriori conditions to check the quality of the relaxation (Section VIII). We remark that the rotation subproblem addressed in this paper is in itself a foundational problem in vision (where it is known as rotation search [31]) and aerospace (where it is known as the Wahba problem [32]). Our SDP relaxation is the ﬁrst certiﬁable algorithm for robust rotation search.
Our third contribution (Section IX) is a set of theoretical results certifying the quality of the solution returned by our algorithm, named Truncated least squares Estimation And SEmideﬁnite Relaxation (TEASER). In the noiseless case, we provide easy-to-check conditions under which TEASER recovers the true transformation between the point clouds in the presence of outliers. In the noisy case, we provide bounds on the distance between the ground-truth transformation and TEASER’s estimate. To the best of our knowledge these are the ﬁrst non-asymptotic error bounds for geometric estimation problems with outliers, while the literature on robust estimation in statistics (e.g., [33]) typically studies simpler problems in Euclidean space and focuses on asymptotic bounds.
Our fourth contribution (Section X) is to implement a fast version of TEASER, named TEASER++, that uses graduated non-convexity (GNC) [34] to estimate the rotation without solving a large SDP. We show that TEASER++ is also certiﬁable, and in particular we leverage Douglas-Rachford Splitting [35] to design a scalable optimality certiﬁer that can assert global optimality of the estimate returned by GNC. We release a fast open-source C++ implementation of TEASER++.
Our last contribution (Section XI) is an extensive evaluation in both standard benchmarks and on real datasets for object detection [36] and scan matching [37]. In particular, we show that (i) both TEASER and TEASER++ dominate the state of the art (e.g., RANSAC, branch-&-bound, heuristics) and are robust to more than 99% outliers when the scale is known, (ii) TEASER++ can run in milliseconds and it is currently the fastest robust registration algorithm, (iii) TEASER++ is so robust it can also solve problems without correspondences (e.g., hypothesizing all-to-all correspondences) where it largely outperforms ICP and it is more accurate than Go-ICP while being orders of magnitude faster, and (iv) TEASER++ can boost registration performance when combined with deep-learned keypoint detection and matching.
Novelty with respect to [38], [39]. In our previous works, we introduced TEASER [38] and the quaternion-based relaxation of the rotation subproblem [39] (named QUASAR). The present manuscript brings TEASER to maturity by (i) providing explicit theoretical results on TEASER’s performance (Section IX), (ii) providing a fast optimality certiﬁcation method (Section VIII-C), (iii) developing a fast algorithm, TEASER++, that uses GNC to estimate the rotation without solving an SDP, while still being certiﬁable (Section X), and (iv) reporting a more comprehensive experimental evaluation, including real tests on the 3DMatch dataset and examples of registration

3

without correspondences (Sections XI-C and XI-E). These are major improvements both on the theoretical side ([38], [39] only certiﬁed performance on each subproblem, rather than end-to-end, and required solving a large SDP) and on the practical side (TEASER++ is more than three orders of magnitude faster than our proposal in [38]).
II. RELATED WORK
There are two popular paradigms for the registration of 3D point clouds: Correspondence-based and Simultaneous Pose and Correspondence (i.e., correspondence-free) methods.
A. Correspondence-based Methods
Correspondence-based methods ﬁrst detect and match 3D keypoints between point clouds using feature descriptors [16], [1], [7], [40], and then use an estimator to infer the transformation from these putative correspondences. 3D keypoint matching is known to be less accurate compared to 2D counterparts like SIFT and ORB, thus causing much higher outlier rates, e.g., having 95% spurious correspondences is considered common [19]. Therefore, a robust backend that can deal with extreme outlier rates is highly desirable.
Registration without Outliers. Horn [14] and Arun [15] show that optimal solutions (in the maximum likelihood sense) for scale, rotation, and translation can be computed in closed form when the correspondences are known and the points are affected by isotropic zero-mean Gaussian noise. Olsson et al. [41] propose a method based on Branch-&bound (BnB) that is globally optimal and allows point-to-point, point-to-line, and point-to-plane correspondences. Briales and Gonzalez-Jimenez [42] propose a tight semideﬁnite relaxation to solve the same registration problem as in [41].
If the two sets of points only differ by an unknown rotation (i.e., we do not attempt to estimate the scale and translation), we obtain a simpliﬁed version of the registration problem that is known as rotation search in computer vision [31], or Wahba problem in aerospace [32]. In aerospace, the vector observations are typically the directions to visible stars observed by sensors onboard the satellite. Closedform solutions to the Wahba problem are known using both quaternion [14], [43] and rotation matrix [44], [15] representations. The Wahba problem is also related to the well-known Orthogonal Procrustes problem [45] where one searches for orthogonal matrices (rather than rotations), for which a closedform solution also exists [46]. The computer vision community has investigated the rotation search problem in the context of point cloud registration [17], [20], image stitching [11], motion estimation and 3D reconstruction [4], [5]. In particular, the closed-form solutions from Horn [14] and Arun et al. [15] can be used for (outlier-free) rotation search with isotropic Gaussian noise. Ohta and Kanatani [47] propose a quaternionbased optimal solution, and Cheng and Crassidis [48] develop a local optimization algorithm, for the case of anisotropic Gaussian noise. Ahmed et al. [49] develop an SDP relaxation for the case with bounded noise and no outliers.
Robust Registration. Probably the most widely used robust registration approach is based on RANSAC [18], which has

enabled several early applications in vision and robotics [50], [51]. Despite its efﬁciency in the low-noise and low-outlier regime, RANSAC exhibits slow convergence and low accuracy with large outlier rates [19], where it becomes harder to sample a “good” consensus set. Other approaches resort to Mestimation, which replaces the least squares objective function with robust costs that are less sensitive to outliers [52], [53], [54]. Zhou et al. [55] propose Fast Global Registration (FGR) that uses the Geman-McClure cost function and leverages graduated non-convexity to solve the resulting non-convex optimization. Despite its efﬁciency, FGR offers no optimality guarantees. Indeed, FGR tends to fail when the outlier ratio is high (>80%), as we show in Section XI. Enqvist et al. [56] propose to optimally ﬁnd correct correspondences by solving the vertex cover problem and demonstrate robustness against over 95% outliers in 3D registration. Parra and Chin [19] propose a Guaranteed Outlier REmoval (GORE) technique, that uses geometric operations to signiﬁcantly reduce the amount of outlier correspondences before passing them to the optimization backend. GORE has been shown to be robust to 95% spurious correspondences [19]. In an independent effort, Parra et al. [57] ﬁnd pairwise-consistent correspondences in 3D registration using a practical maximum clique (PMC) algorithm. While similar in spirit to our original proposal [38], both GORE and PMC do not estimate the scale of the registration and are typically slower since they rely on BnB (see Algorithm 2 in [19]). Yang and Carlone [38] propose the ﬁrst certiﬁable algorithm for robust registration, which however requires solving a large-scale SDP hence being limited to small problems. Recently, physics-based registration has been proposed in [58], [59], [60].
Similar to the registration problem, rotation search with outliers has also been investigated in the computer vision community. Local techniques for robust rotation search are again based on RANSAC or M-estimation, but are brittle and do not provide performance guarantees. Global methods (that guarantee to compute globally optimal solutions) are based on Consensus Maximization [61], [62], [63], [64], [65], [66] and BnB [67]. Hartley and Kahl [31] ﬁrst proposed using BnB for rotation search, and Bazin et al. [68] adopted consensus maximization to extend their BnB algorithm with a robust formulation. BnB is guaranteed to return the globally optimal solution, but it runs in exponential time in the worst case. Another class of global methods for Consensus Maximization enumerates all possible subsets of measurements with size no larger than the problem dimension (3 for rotation search) to analytically compute candidate solutions, and then verify global optimality using computational geometry [69], [70]. Similarly, Ask et al. [71] show the TLS estimation can also be solved globally by enumerating subsets with size no larger than the model dimension. However, these methods require exhaustive enumeration and become intractable when the problem dimension is large (e.g., 6 for 3D registration). In [39], we propose the ﬁrst quaternion-based certiﬁable algorithm for robust rotation search (reviewed in Section VIII).

4

B. Simultaneous Pose and Correspondence Methods
Simultaneous Pose and Correspondence (SPC) methods alternate between ﬁnding the correspondences and computing the best transformation given the correspondences.
Local Methods. The Iterative Closest Point (ICP) algorithm [17] is considered a milestone in point cloud registration. However, ICP is prone to converge to local minima and only performs well given a good initial guess. Multiple variants of ICP [72], [73], [74], [75] have proposed to use robust cost functions to improve convergence. Probabilistic interpretations have also been proposed to improve ICP as a minimization of the Kullback-Leibler divergence between two Mixture models [76], [77]. Clark et al. [78] alignn point clouds as continuous functions using Riemannian optimization. Le et al. [79] use semideﬁnite relaxation to generate hypotheses for randomized methods. All these methods do not provide global optimality guarantees.
Global Methods. Global SPC approaches compute a globally optimal solution without initial guesses, and are usually based on BnB. A series of geometric techniques have been proposed to improve the bounding tightness [31], [80], [20], [81] and increase the search speed [82], [20]. However, the runtime of BnB increases exponentially with the size of the point cloud and is made worse by the explosion of the number of local minima resulting from high outlier ratios [19]. Global SPC registration can be also formulated as a mixedinteger program [83], though the runtime remains exponential. Maron et al. [84] design a tight semideﬁnite relaxation, which, however, only works for SPC registration with full overlap.
Deep Learning Methods. The success of deep learning on 3D point clouds (e.g., PointNet [85] and DGCNN [86]) opens new opportunities for learning point cloud registration from data. Deep learning methods ﬁrst learn to embed points clouds in a high-dimensional feature space, then learn to match keypoints to generate correspondences, after which optimization over the space of rigid transformations is performed for the best alignment. PointNetLK [87] uses PointNet to learn feature representations and then iteratively align the features representations, instead of the 3D coordinates. DCP [88] uses DGCNN features for correspondence matching and Horn’s method for registration in an end-to-end fashion (Horn’s method is differentiable). PRNet [89] extends DCP to aligning partially overlapping point clouds. Scan2CAD [90] and its improvement [91] apply similar pipelines to align CAD models to RGB-D scans. 3DSmoothNet [2] uses a siamese deep learning architecture to establish keypoint correspondences between two point clouds. FCGF [40] leverages sparse highdimensional convolutions to extract dense feature descriptors from point clouds. Deep global registration [92] uses FCGF feature descriptors for point cloud registration. In Section XI-E we show that (i) our approach provides a robust back-end for deep-learned keypoint matching algorithms (we use [2]), and (ii) that current deep learning approaches still struggle to produce acceptable inlier rates in real problems.
Remark 1 (Reconciling Correspondence-based and SPC Methods). Correspondence-based and SPC methods are tightly coupled. First of all, approaches like ICP alter-

nate between ﬁnding the correspondences and solving a correspondence-base problem. More importantly, one can always reformulate an SPC problem as a correspondencebased problem by hypothesizing all-to-all correspondences, i.e., associating each point in the ﬁrst point cloud to all the points in the second. To the best of our knowledge, only [56], [93] have pursued this formulation since it leads to an extreme number of outliers. In this paper, we show that our approach can indeed solve the SPC problem thanks to its unprecedented robustness to outliers (Section XI-C).

III. NOTATIONS AND PRELIMINARIES

Scalars, Vectors, Matrices. We use lowercase characters

(e.g., s) to denote real scalars, bold lowercase characters

(e.g., v) for real vectors, and bold uppercase characters

(e.g., M ) for real matrices. Mij denotes the i-th row and j-th column scalar entry of matrix M ∈ Rm×n, and [M ]ij,d (or

simply [M ]ij when d is clear from the context) denotes the ith row and j-th column d×d block of the matrix M ∈ Rmd×nd.

Id is the identity matrix of size d. We use “⊗” to denote

the Kronecker product. For a square matrix M , det(M ) and

tr (M ) denote its determinant and trace. The 2-norm of a

vector is denoted as · . The Frobenious norm of a matrix is

denoted as · F. For a symmetric matrix M of size n × n, we use λ1 ≤ . . . ≤ λn to denote its real eigenvalues.

Sets. We use calligraphic fonts to denote sets (e.g., S).

We use Sn (resp. Sn) to denote the group of real sym-

mS+netri=c.

(resp. skew-symmetric) matrices {M ∈ Sn : M 0} denotes

symmetric positive semideﬁnite matrices.

with size

the set SO(d)

=o. f

n × n. n×n {R ∈

Rd×d : RTR = Id, det(R) = +1} denotes the d-dimensional special orthogonal group, while Sd−1 = {u ∈ Rd : u = 1}

denotes the d-dimensional unit sphere.

Quaternions. Unit quaternions are a representation for a

3D rotation R ∈ SO(3). We denote a unit quaternion as a unit-norm column vector q = [vT s]T ∈ S3, where v ∈ R3

is the vector part of the quaternion and the last element s is

the scalar part. We also use q = [q1 q2 q3 q4]T to denote the

four entries of the quaternion. Each quaternion represents a

3D rotation and the composition of two rotations qa and qb can be computed using the quaternion product qc = qa ◦ qb:

qc = qa ◦ qb = Ω1(qa)qb = Ω2(qb)qa,

(1)

where Ω1(q) and Ω2(q) are deﬁned as follows:

 q4

Ω1 (q)= 

q3 −q2

−q1

−q3 q4 q1
−q2

q2 −q1
q4 −q3

q1 

q2 q3

, 

q4

 q4

Ω2 (q)= 

−q3 q2

−q1

q3 q4 −q1 −q2

−q2 q1 q4
−q3

q1 

q2 q3

. 

q4

(2)

The inverse of a quaternion q = [vT s]T is deﬁned as q−1 = [−vT s]T, where one simply reverses the sign of the vector part. The rotation of a vector a ∈ R3 can be expressed in
terms of quaternion product. Formally, if R is the (unique)
rotation matrix corresponding to a unit quaternion q, then:

Ra 0

= q ◦ aˆ ◦ q−1,

(3)

where aˆ = [aT 0]T is the homogenization of a, obtained by
augmenting a with an extra entry equal to zero. The set of unit quaternions, i.e., the 4-dimensional unit sphere S3, is a

5

double cover of SO(3) since q and −q represent the same rotation (this fact can be easily seen by examining eq. (3)).

IV. ROBUST REGISTRATION WITH TRUNCATED LEAST SQUARES COST

In the robust registration problem, we are given two 3D point clouds A = {ai}Ni=1 and B = {bi}Ni=1, with ai, bi ∈ R3. We consider a correspondence-based setup, where we are
given putative correspondences (ai, bi), i = 1, . . . , N , that
obey the following generative model:

bi = s◦R◦ai + t◦ + oi + i,

(4)

where s◦ > 0, R◦ ∈ SO(3), and t◦ ∈ R3 are the unknown
(to-be-computed) scale, rotation, and translation, i models the measurement noise, and oi is a vector of zeros if the pair (ai, bi) is an inlier, or a vector of arbitrary numbers for outlier correspondences. In words, if the i-th correspondence
(ai, bi) is an inlier correspondence, bi corresponds to a 3D transformation of ai (plus noise i), while if (ai, bi) is an outlier correspondence, bi is just an arbitrary vector.
Registration without Outliers. When i is a zero-mean Gaussian noise with isotropic covariance σi2I3, and all the correspondences are correct (i.e., oi = 0, ∀i), the Maximum Likelihood estimator of (s◦, R◦, t◦) can be computed by
solving the following nonlinear least squares problem:

N1

min
s>0,R∈SO(3),t∈R3

i=1

σi2

bi − sRai − t

2 .

(5)

Although (5) is a non-convex problem, due to the nonconvexity of the set SO(3), its optimal solution can be computed in closed form by decoupling the estimation of the scale, rotation, and translation, using Horn’s [14] or Arun’s method [15]. A key contribution of the present paper is to provide a way to decouple scale, rotation, and translation in the more challenging case with outliers.
In practice, a large fraction of the correspondences are outliers, due to incorrect keypoint matching. Despite the elegance of the closed-form solutions [14], [15], they are not robust to outliers, and a single “bad” outlier can compromise the correctness of the resulting estimate. Hence, we propose a truncated least squares registration formulation that can tolerate extreme amounts of spurious data.
Truncated Least Squares Registration. We depart from the Gaussian noise model and assume the noise is unknown but bounded [29]. Formally, we assume the inlier noise i in (4) is such that i ≤ βi, where βi is a given bound.
Then we adopt the following Truncated Least Squares (TLS) Registration formulation:

N

min

min

s>0,R∈SO(3),t∈R3 i=1

1 βi2

bi − sRai − t 2, c¯2

, (6)

which computes a least squares solution of measure-

ments

with

small

residuals

(

1 βi2

bi − sRai − t 2

≤

c¯2),

while discarding measurements with large residuals (when

1 βi2

bi − sRai − t 2 > c¯2 the i-th summand becomes a

constant and does not inﬂuence the optimization). Note that

one can always divide each summand in (6) by c¯2: therefore,

one can safely assume c¯2 to be 1. For the sake of generality, in the following we keep c¯2 since it provides a more direct “knob” to be stricter or more lenient towards potential outliers.
The noise bound βi is fairly easy to set in practice and can be understood as a “3-sigma” noise bound or as the maximum error we expect from an inlier. The interested reader can ﬁnd a more formal discussion on how to set βi and c¯ in Appendix B. We remark that while we assume to have a bound on the maximum error we expect from the inliers (βi), we do not make assumptions on the generative model for the outliers, which is typically unknown in practice.

Remark 2 (TLS vs. Consensus Maximization). TLS estimation is related to Consensus Maximization [61], a popular robust estimation approach in computer vision. Consensus Maximization looks for an estimate that maximizes the number of inliers, while TLS simultaneously computes a least squares estimate for the inliers. The two methods are not guaranteed to produce the same choice of inliers in general, since TLS also penalizes inliers with large errors. Appendix C provides a toy example to illustrate the potential mismatch between the two techniques and provides necessary conditions under which the two formulations ﬁnd the same set of inliers.

Despite being insensitive to outlier correspondences, the truncated least squares formulation (6) is much more challenging to solve globally, compared to the outlier-free case (5). This is the case even in simpler estimation problems where the feasible set of the unknowns is convex, as stated below.

Remark 3 (Hardness of TLS Estimation [94], [95]).

The minimization of a sum of truncated convex functions,

N i=1

min(fi(x), c¯2),

over

a

convex

feasible

set

x

∈

X

⊆

Rd,

is NP-hard in the dimension d [94], [95]. A simple exhaustive

search can obtain a globally optimal solution in exponential

time O(2N ) [95].

Remark 3 states the NP-hardness of TLS estimation over a convex feasible set. The TLS problem (6) is even more challenging due to the non-convexity of SO(3). While problem (6) is hard to solve directly, in the next section, we show how to decouple the estimation of scale, rotation, and translation using invariant measurements.

V. DECOUPLING SCALE, ROTATION, AND TRANSLATION ESTIMATION

We propose a general approach to decouple the estimation of scale, translation, and rotation in problem (6). The key insight is that we can reformulate the measurements (4) to obtain quantities that are invariant to a subset of the transformations (scaling, rotation, translation).

A. Translation Invariant Measurements (TIMs)
While the absolute positions of the points in B depend on the translation t, the relative positions are invariant to t. Mathematically, given two points bi and bj from (4), the relative position of these two points is:

bj − bi = sR(aj − ai) + (oj − oi) + ( j − i),

(7)

6

where

|

s ij

|≤

s ij

=.

˜ij a¯ ij

, and osij

δij/ a¯ij since |˜ij|≤

=. δij .

o˜ij a¯ ij
We

. It is deﬁne

easy to see that αij =. δij / a¯ij .

Eq. (TRIM) describes a Translation and Rotation Invariant

Measurement (TRIM) whose generative model is only function

of the unknown scale s.

Fig. 2. TIMs generated from a complete graph in the Bunny dataset [97].

where the translation t cancels out in the subtraction. Therefboyrec,owmepcuatinngoba¯taiijn=.a Tarja−nslaaitioanndInb¯viajri=a. nbtjM−eabsiu,raenmdentht e(TTIMIM) satisﬁes the following generative model:

b¯ij = sRa¯ij + oij + ij ,

(TIM)

where oij =. measurements

oj − oi is zero if both the i-th and the are inliers (or arbitrary otherwise), while

ijj-t=.h

j − i is the i ≤ βi and

measurement j ≤ βj, then

noise.
ij

It ≤

βiis+eaβsjy =.toδisje. e

that

if

The advantage of the TIMs in eq. (TIM) is that their generative

model only depends on two unknowns, s and R. The number

of TIMs is upper-bounded by (N2 ) = N (N − 1)/2, where pairwise relative measurements between all pairs of points

are computed. Theorem 4 below connects the TIMs with the

topology of a graph deﬁned over the 3D points.

Theorem 4 (Translation Invariant Measurements). Deﬁne the vectors a ∈ R3N (resp. b ∈ R3N ), obtained by concatenating all vectors ai (resp. bi) in a single column vector. Moreover, deﬁne an arbitrary graph G with nodes {1, . . . , N } and
an arbitrary set of edges E. Then, the vectors a¯ = (A ⊗ I3)a and b¯ = (A ⊗ I3)b are TIMs, where A ∈ R|E|×N is the incidence matrix of G [96].

A proof of the theorem is given in Appendix D. TIMs generated from a complete graph on the Bunny dataset [97] are illustrated in Fig. 2.

B. Translation and Rotation Invariant Measurements (TRIMs)

While the relative locations of pairs of points (TIMs) still depend on the rotation R, their distances are invariant to both R and t. Therefore, to build rotation invariant measurements, we compute the norm of each TIM vector:

b¯ij = sRa¯ij + oij + ij .

(8)

We now note that for the inliers (oij = 0) it holds (using ij ≤ δij and the triangle inequality):

Remark 5 (Novelty of Invariant Measurements). Ideas similar to the translation invariant measurements (TIMs) have been used in recent work [98], [19], [28], [99], [100] while (i) the novel graph-theoretic interpretation of Theorem 4 generalizes previously proposed methods and allows pruning outliers as described in Section VI-D, and (ii) the notion of translation and rotation invariant measurements (TRIMs) is completely new. We also remark that while related work uses invariant measurements to ﬁlter-out outliers [19] or to speed up BnB [98], [28], we show that they also allow decoupling the estimation of scale, rotation, and translation.
A summary table of the invariant measurements and the corresponding noise bounds is given in Appendix E.
VI. TRUNCATED LEAST SQUARES ESTIMATION AND SEMIDEFINITE RELAXATION (TEASER): OVERVIEW
We propose a decoupled approach to solve in cascade for the scale, the rotation, and the translation in (6). The approach, named Truncated least squares Estimation And SEmideﬁnite Relaxation (TEASER), works as follows:
1) we use the TRIMs to estimate the scale sˆ 2) we use sˆ and the TIMs to estimate the rotation Rˆ 3) we use sˆ and Rˆ to estimate the translation tˆ from (ai, bi)
in the original TLS problem (6).
We state each subproblem in the following subsections.

A. Robust Scale Estimation

The generative model (TRIM) describes linear scalar mea-

surements sij of the unknown scale s, affected by bounded

noise

|

s ij

|≤

αij

including

potential

outliers

(when

osij

=

0).

Again, we estimate the scale given the measurements sij and

the bounds αij using a TLS estimator:

K
sˆ = arg min min
s k=1

(s − sk)2 αk2

,

c¯2

,

(11)

where for simplicity we numbered the invariant measurements from 1 to K = |E| and adopted the notation sk instead of sij. Section VII shows that (11) can be solved exactly and in polynomial time via adaptive voting (Algorithm 2).

sRa¯ij −δij ≤ sRa¯ij + ij ≤ sRa¯ij +δij ,

(9)

hence we can write (8) equivalently as:

b¯ij = sRa¯ij +o˜ij + ˜ij ,

(10)

with |˜ij|≤ δij, and o˜ij = 0 if both i and j are inliers or is an arbitrary scalar otherwise. Recalling that the norm is rotation

invariant and that s > 0, and dividing both sides of (10) by

a¯ij , we obtain new measurements sij =.

: b¯ij
a¯ ij

sij = s + osij +

s ij

,

(TRIM)

B. Robust Rotation Estimation

Given the scale estimate sˆ produced by the scale estimation, the generative model (TIM) describes measurements b¯ij affected by bounded noise ij ≤ δij including potential outliers (when oij = 0). Again, we compute R from the estimated scale sˆ, the TIM measurements (a¯ij, b¯ij) and the bounds δij
using a TLS estimator:

K
Rˆ = arg min min
R∈SO(3) k=1

b¯k − sˆRa¯k δk2

2

,

c¯2

,

(12)

7

where for simplicity we numbered the measurements from 1 to K = |E| and adopted the notation a¯k, b¯k instead of a¯ij, b¯ij. Problem (12) is known as the Robust Wahba or Robust Rotation Search problem [39]. Section VIII shows that (12)
can be solved exactly and in polynomial time (in practical
problems) via a tight semideﬁnite relaxation.

C. Robust Component-wise Translation Estimation

After obtaining the scale and rotation estimates sˆ and Rˆ by solving (11)-(12), we can substitute them back into problem (6) to estimate the translation t. Although (6) operates
on the 2 norm of the vector, we propose to solve for the translation component-wise, i.e., we compute the entries t1, t2, t3 of t independently:

N

tˆj = arg min min

tj

i=1

(tj

−

[bi

− sˆRˆ ai]j)2 βi2

,

c¯2

,

(13)

for j = 1, 2, 3, and where [·]j denotes the j-th entry of a vector. Since bi − sˆRˆai is a known vector in this stage, it is easy to see that (13) is a scalar TLS problem. Therefore, similarly
to (11), Section VII shows that (13) can be solved exactly and in polynomial time via adaptive voting (Algorithm 2).
The interested reader can ﬁnd a discussion on component-wise
versus full TLS translation estimation in Appendix Q,

D. Boosting Performance: Max Clique Inlier Selection (MCIS)
While in principle we could simply execute the cascade of scale, rotation, and translation estimation described above, our graph-theoretic interpretation of Theorem 4 affords further opportunities to prune outliers.
Consider the TRIMs as edges in the complete graph G(V, E) (where the vertices V are the correspondences and the edge set E induces the TIMs and TRIMs per Theorem 4). After estimating the scale (11) (discussed in Section VII), we can prune the edges (i, j) in the graph whose associated TRIM sij have been classiﬁed as outliers by the TLS formulation (i.e., |sij − sˆ|> c¯αij). This allows us to obtain a pruned graph G (V, E ), with E ⊆ E, where gross outliers are discarded. The following result ensures that inliers form a clique in the graph G (V, E ), enabling an even more substantial rejection of outliers.

E. Pseudocode of TEASER The pseudocode of TEASER is summarized in Algorithm 1.

Algorithm 1: Truncated least squares Estimation And SEmideﬁnite Relaxation (TEASER).

1 Input: points (ai, bi) and bounds βi (i = 1, . . . , N ), threshold c¯2 (default: c¯2 = 1), graph G(V, E) (default:

G describes the complete graph);

2 Output: sˆ, Rˆ, tˆ;

3 % Compute TIM and TRIM

4 b¯ij = bj−bi , a¯ij = aj−ai , δij = βi+βj, ∀(i, j) ∈ E

5 sij =

b¯ij a¯ ij

, αij =

δij a¯ ij

,

∀(i, j) ∈ E

6 % Decoupled estimation of s, R, t

7 sˆ = estimate_s({sij, αij : ∀(i, j) ∈ E}, c¯2)

8 G (V , E ) = maxClique(G(V, E )) % prune outliers
9 Rˆ = estimate_R({a¯ij, b¯ij, δij : ∀(i, j) ∈ E }, c¯2, sˆ) 10 tˆ = estimate_t({ai, bi, βi : i ∈ V }, c¯2, sˆ, Rˆ) 11 return: sˆ, Rˆ, tˆ

The following Sections VII-VIII describe how to implement the functions in lines 7, 9, 10 of Algorithm 1. In particular, we show how to obtain global and robust estimates of scale (estimate_s) and translation (estimate_t) in Section VII, and rotation (estimate_R) in Section VIII.
VII. ROBUST SCALE AND TRANSLATION ESTIMATION: ADAPTIVE VOTING
In this section, we propose an adaptive voting algorithm to solve exactly the robust scale estimation and the robust component-wise translation estimation.
A. Adaptive Voting for Scalar TLS Estimation
Both the scale estimation (11) and the component-wise translation estimation (13) resort to ﬁnding a TLS estimate of an unknown scalar given a set of outlier-corrupted measurements. Using the notation for scale estimation (11), the following theorem shows that one can solve scalar TLS estimation in polynomial time by a simple enumeration.

Theorem 6 (Maximal Clique Inlier Selection). Edges corresponding to inlier TIMs form a clique in E , and there is at least one maximal clique in E that contains all the inliers.
A proof of Theorem 6 is presented in Appendix F. Theorem 6 allows us to prune outliers by ﬁnding the maximal cliques of G (V, E ). Similar idea has been explored for rigid body motion segmentation [101]. Although ﬁnding the maximal cliques of a graph takes exponential time in general, there exist efﬁcient approximation algorithms that scale to graphs with millions of nodes [102], [103], [104]. Under high outlier rates, the graph G (V, E ) is sparse and the maximal clique problem can be solved quickly in practice [105]. Therefore, in this paper, after performing scale estimation and removing the corresponding gross outliers, we compute the maximal clique with largest cardinality, i.e., the maximum clique, as the inlier set to pass to rotation estimation. Section XI-A shows that this method drastically reduces the number of outliers.

Theorem 7 (Optimal Scalar TLS Estimation). Consider the

scalar TLS problem in (11). For a given s ∈ R, deﬁne the

consensus

set

of

s

as

I (s)

=

{k

:

(s−sk )2 α2k

≤ c¯2}.

Then,

for

any s ∈ R, there are at most 2K − 1 different non-empty

consensus sets. If we name these sets I1, . . . , I2K−1, then the

solution of (11) can be computed by enumeration as:


 sˆ = arg min fs(sˆi) : sˆi =


1 k∈Ii αk2

−1



k∈Ii

sk αk2

,

 ∀i


,

(14)

where fs(·) is the objective function of (11).

Theorem 7, whose proof is given in Appendix G, is based on the insight that the consensus set can only change at the boundaries of the intervals [sk − αkc¯, sk + αkc¯] (Fig. 3(a)) and there are at most 2K such boundaries. The theorem also suggests a straightforward adaptive voting algorithm to solve (11), with pseudocode given in Algorithm 2. The

8

Fig. 3. (a) conﬁdence interval for each measurement sk (every s in the k-th

interval

satisﬁes

(s−sk )2 α2k

≤

c¯2; (b) cardinality

of

the consensus set

for every

s and middle-points mi for each interval with constant consensus set.

Algorithm 2: Adaptive Voting.

1 Input: sk, αk, c¯;

2 Output: sˆ, scale estimate solving (11);

3 % Deﬁne boundaries and sort

4 v = sort([s1 −α1c¯, s1 +α1c¯, . . . , sK −αK c¯, sK +αK c¯])

5 % Compute middle points

6

mi =

vi +vi+1 2

for i = 1, . . . , 2K − 1

7 % Voting

8 for i = 1, . . . , 2K − 1 do

9 Ii = ∅ 10 for k = 1, . . . , K do

11

if mi ∈ [sk − αkc¯, sk + αkc¯] then

12

Ii = Ii ∪ {k} % add to consensus set

13

end

14 end

15 end

16 % Enumerate consensus sets and return best

17 return: sˆ from Eq. (14).

algorithm ﬁrst builds the boundaries of the intervals shown in Fig. 3(a) (line 4). Then, for each interval, it evaluates the consensus set (line 12, see also Fig. 3(b)). Since the consensus set does not change within an interval, we compute it at the interval centers (line 6, see also Fig. 3(b)). Finally, the cost of each consensus set is computed and the smallest cost is returned as optimal solution (line 17).
Remark 8 (Adaptive Voting). The adaptive voting algorithm generalizes the histogram voting method of Scaramuzza [106] (i) to adaptively adjust the bin size in order to obtain an optimal solution and (ii) to solve a TLS (rather than Consensus Maximization) formulation. Adaptive voting can be also used for Consensus Maximization, by simply returning the largest consensus set Ii in Algorithm 2. We also refer the interested reader to the paper of Liu and Jiang [94], who recently developed a similar algorithm in an independent effort and provide a generalization to 2D TLS estimation problems.
In summary, the function estimate_s in Algorithm 1 calls Algorithm 2 to compute the optimal scale estimate sˆ, and the function estimate_t in Algorithm 1 calls Algorithm 2 three times (one for each entry of t) and returns the translation estimate tˆ = [tˆ1 tˆ2 tˆ3]T.
VIII. ROBUST ROTATION ESTIMATION: SEMIDEFINITE RELAXATION AND FAST CERTIFICATES
This section describes how to compute an optimal solution to problem (12) or certify that a given rotation estimate is globally optimal. TLS estimation is NP-hard according to

Remark 3 and the robust rotation estimation (12) has the additional complexity of involving a non-convex domain (i.e., SO(3)). This section provides a surprising result: we can compute globally optimal solutions to (3) (or certify that a given estimate is optimal) in polynomial time in virtually all practical problems using a tight semideﬁnite (SDP) relaxation. In other words, while the NP-hardness implies the presence of worst-case instances that are not solvable in polynomial time, these instances are not observed to frequently occur in practice.
We achieve this goal in three steps. First, we show that the robust rotation estimation problem (12) can be reformulated as a Quadratically Constrained Quadratic Program (QCQP) by adopting a quaternion formulation and using a technique we name binary cloning (Section VIII-A). Second, we show how to obtain a semideﬁnite relaxation and relax the nonconvex QCQP into a convex SDP with redundant constraints (Section VIII-B). The SDP relaxation enables solving (12) with global optimality guarantees in polynomial time. Third, we show that the relaxation also enables a fast optimality certiﬁer, which, given a rotation guess, can test if the rotation is the optimal solution to (12) (Section VIII-C). The latter will be instrumental in developing a fast and certiﬁable registration approach that circumvents the time-consuming task of solving a large relaxation with existing SDP solvers. The derivation in Sections VIII-A and VIII-B is borrowed from our previous work [39], while the fast certiﬁcation in Section VIII-C has not been presented before.

A. Robust Rotation Estimation as a QCQP
This section rewrites problem (3) as a Quadratically Constrained Quadratic Program (QCQP). Using the quaternion preliminaries introduced in Section III –and in particular eq. (3)– it is easy to rewrite (12) using unit quaternions:

K
min min
q∈S3 k=1

bˆk − q ◦ aˆk ◦ q−1 δk2

2

,

c¯2

,

(15)

where we deﬁned aˆk =. [sˆa¯Tk 0]T and bˆk =. [b¯Tk 0]T, and “◦” denotes the quaternion product. The main advantage of using (15) is that we replaced the set SO(3) with a simpler set, the 4-dimensional unit sphere S3.
From TLS to Mixed-Integer Programming. Problem (15)
is hard to solve globally, due to the non-convexity of both the cost function and the domain S3. As a ﬁrst step towards obtaining a QCQP, we expose the non-convexity of the cost by
rewriting the TLS cost using binary variables. In particular, we rewrite the inner “min” in (15) using the following property, that holds for any pair of scalars x and y:

1+θ 1−θ

min(x, y) = min

x+

y.

(16)

θ∈{+1,−1} 2

2

Eq. (16) can be veriﬁed to be true by inspection: the righthand-side returns x (with minimizer θ = +1) if x < y, and y (with minimizer θ = −1) if x > y. This enables us to rewrite problem (15) as a mixed-integer program including

9

the quaternion q and binary variables θk, i = 1, . . . , K:

min K 1 + θk

q∈S3 θk ={±1}

k=1

2

bˆk − q ◦ aˆk ◦ q−1 δk2

2

+

1 − θk c¯2. 2

(17)

The reformulation is related to the Black-Rangarajan duality

between robust estimation and line processes [53]: the TLS

cost is an extreme case of robust function that results in a binary line process. Intuitively, the binary variables {θk}Nk=1 in problem (17) decide whether a given measurement k is an

inlier (θk = +1) or an outlier (θk = −1). Exposing the non-

convexity of TLS cost function using binary variables makes

the TLS cost more favorable than other non-convex robust costs

(e.g., the Geman-McClure cost used in FGR [55]), because

it enables binary cloning (Proposition 9) and semideﬁnite

relaxation (Section VIII-B).

From Mixed-Integer to Quaternions. Now we convert the

mixed-integer program (17) to an optimization over K + 1

qnuioantesrqnkio=n.s.θkTqh,ewientcuaitniornewisritteha(1t,7)ifaswae

deﬁne extra function of q

quaterand qk

(k = 1, . . . , K). This is a key step towards getting a quadratic

cost (Proposition 10) and is formalized as follows.

Proposition 9 (Binary Cloning). The mixed-integer pro-

gram (17) is equivalent (in the sense that they admit the same optimal solution q) to the following optimization:

K
min
qkq=∈{S±3q}k=1

bˆk −q ◦ aˆk ◦ q−1 + qTqkbˆk −q ◦ aˆk ◦ qk−1 2 4δk2

+ 1 − qTqk c¯2.

(18)

2

which involves K + 1 quaternions (q and qk, i = 1, . . . , K).

While a formal proof is given in Appendix H, it is fairly easy to see that if qk = {±q}, or equivalently, qk = θkq with θk ∈ {±1}, then qkTq = θk, and q ◦ aˆk ◦ qk−1 = θk(q ◦ aˆk ◦ q−1) which exposes the relation between (17) and (18). We dubbed the re-parametrization (18) binary cloning since now we created a “clone” qk for each measurement, such that qk = q for inliers (recall qk = θkq) and qk = −q for outliers.
From Quaternions to QCQP. We conclude this section
by showing that (18) can be actually written as a QCQP. This
observation is non-trivial since (18) has a quartic cost and qk = {±q} is not in the form of a quadratic constraint. The re-formulation as a QCQP is given in the following. Proposition 10 (Binary Cloning as a QCQP). Deﬁne a single column vector x = [qT q1T . . . qKT ]T stacking all variables in Problem (18). Then, Problem (18) is equivalent (in the sense that they admit the same optimal solution q) to the following
Quadratically-Constrained Quadratic Program:

min
x∈R4(K+1)

xTQx

(19)

s.t. xTq xq = 1

xqk xTqk = xqxTq , ∀k = 1, . . . , K.

where Q ∈ S4(K+1) is a known symmetric matrix that depends on the TIM measurements a¯k and b¯k (the explicit expression is
given in Appendix I), and the notation xq (resp. xqk ) denotes the 4D subvector of x corresponding to q (resp. qk).

A complete proof of Proposition 10 is given in Appendix I. Intuitively, (i) we developed the squares in the cost function (18), (ii) we used the properties of unit quaternions (Section III) to simplify the expression to a quadratic cost, and (iii) we adopted the more compact notation afforded by the vector x to obtain (19).

B. Semideﬁnite Relaxation

Problem (19) writes the TLS rotation estimation problem (12) as a QCQP. Problem (19) is still a non-convex problem (quadratic equality constraints are non-convex). Here we develop a tight convex semideﬁnite programming (SDP) relaxation for problem (19).
The crux of the relaxation consists in rewriting problem (19) as a function of the following matrix:

 qqT qq1T · · · qqKT 



Z = xxT =   

...

q1q1T ...

··· ...

q1qKT ...

  

∈

S+4(K+1).

(20)



· · · qK qKT

For this purpose we note that the objective function of (19) is a linear function of Z:

xTQx = tr QxxT = tr (QZ) ,

(21)

and that xqxTq = [Z]00, where [Z]00 denotes the top-left 4 × 4 diagonal block of Z, and xqk xTqk = [Z]kk, where [Z]kk denotes the k-th diagonal block of Z (we number
the row and column block index from 0 to K for notation convenience). Since any matrix in the form Z = xxT is a
positive-semideﬁnite rank-1 matrix, we obtain:

Proposition 11 (Matrix Formulation of Binary Cloning). Problem (19) is equivalent (in the sense that optimal solutions of a problem can be mapped to optimal solutions of the other) to the following non-convex rank-constrained program:

min

tr (QZ)

(22)

Z0

s.t.

tr ([Z]00) = 1

[Z]kk = [Z]00, ∀k = 1, . . . , K

rank (Z) = 1.

The proof is given in Appendix J. At this point we are ready to develop our SDP relaxation by dropping the non-convex rank-1 constraint and adding redundant constraints.

Proposition 12 (SDP Relaxation with Redundant Constraints). The following SDP is a convex relaxation of (22):

min

tr (QZ)

(23)

Z0

s.t.

tr ([Z]00) = 1

[Z]kk = [Z]00, ∀k = 1, . . . , K

[Z]ij = [Z]Tij, ∀0 ≤ i < j ≤ K.

The redundant constraints in the last row of eq. (23) enforce all the off-diagonal 4 × 4 blocks of Z to be symmetric, which is a redundant constraint for (19), since:

[Z]ij = qiqjT = (θiq)(θj q)T = θiθj qqT

(24)

10

is indeed a signed copy of qqT and must be symmetric. Although being redundant for the QCQP (19), these constraints are critical in tightening the SDP relaxation (cf. [39]). Appendix K provides a different approach to develop the convex relaxation using Lagrangian duality theory.
The following theorem provides readily checkable a posteriori conditions under which (23) computes an optimal solution to robust rotation estimation.
Theorem 13 (Global Optimality Guarantee). Let Z be the optimal solution of (23). If rank (Z ) = 1, then the SDP relaxation is said to be tight, and:
1) the optimal cost of (23) matches the optimal cost of the QCQP (19),
2) Z can be written as Z = (x )(x )T, where x = [(q )T, (q1 )T, . . . , (qK )T]T, with qk = θkq , θk ∈ {±1},
3) ±x are the two unique global minimizers of the original QCQP (19).
A formal proof is given in Appendix K. Theorem 13 states that if the SDP (23) admits a rank-1 solution, then one can extract the unique global minimizer (remember: q and −q represent the same rotation) of the non-convex QCQP (19) from the rank-1 decomposition of the SDP solution.
In practice, we observe that (23) always returns a rank1 solution: in Section XI, we empirically demonstrate the tightness of (23) in the face of noise and extreme outlier rates (> 95% [39]), and show its superior performance compared to a similar SDP relaxation using a rotation matrix parametrization (see eq. (13) in Section V.B of [38]). As it is often the case, we can only partially justify with theoretical results the phenomenal performance observed in practice. The interested reader can ﬁnd a formal proof of tightness in the low-noise and outlier-free case in Theorem 7 and Proposition 8 of [39]
C. Fast Global Optimality Certiﬁcation
Despite the superior robustness and (a posteriori) global optimality guarantees, the approach outlined in the previous section requires solving the large-scale SDP (23), which is known to be computationally expensive (e.g., solving the SDP (23) for K = 100 takes about 1200 seconds with MOSEK [107]). On the other hand, there exist fast heuristics that compute high-quality solutions to the non-convex TLS rotation problem (12) in milliseconds. For example, the graduated non-convexity (GNC) approach in [34] computes globally optimal TLS solutions with high probability when the outlier ratio is below 80%, while not being certiﬁable.
Therefore, in this section, we ask the question: given an estimate from a fast heuristic, such as GNC,2 can we certify the optimality of the estimate or reject it as suboptimal? In other words: can we make GNC certiﬁable? The answer is yes: we can leverage the insights of Section VIII-B to obtain a fast optimality certiﬁer that is orders of magnitude faster than
2Our certiﬁcation approach also applies to RANSAC, which however has the downside of being non-deterministic and typically less accurate than GNC.

Algorithm 3: Optimality Certiﬁcation.

1 Input: a feasible solution (qˆ, θˆ1, . . . , θˆK ) attaining

cost µˆ in the TLS rotation estimation problem (15); homogeneous and scaled TIMs (aˆk, bˆk), noise bounds
δk and c¯2; maximum number of iterations T (default

T = 200); desired relative suboptimality gap η¯

(default η¯ = 0.1%); initial suboptimality gap

η = +∞; relaxation parameter γ (0 < γ < 2)

2 Output: bound on relative suboptimality gap η

3 % Compute data matrix Q (Proposition 10)

4 Q = get_Q(aˆk, bˆk, δk, c¯2) 5 % Compute x¯, Q¯ (Appendix L)

6 x¯ = [1, θˆ1, . . . , θˆK ]T ⊗ [0 0 0 1]T, Q¯ = Ωˆ Tq QΩˆ q 7 % Compute initial dual certiﬁcate M¯ (0) ∈ L¯

8 M¯ (0) = Q¯ − µˆJ + get_Delta0(aˆk, bˆk, δk, c¯2, Q¯, x¯)

9 % Douglas-Rachford Splitting

10 for t = 0, . . . , T do

11

M¯ S(t+) = ΠS+ (M¯ (t)) % Project to S+

12

M¯ L(¯t) = ΠL¯(2M¯ S(t+) − M¯ (t)) % Project to L¯

13

M¯ (t+1) = M¯ (t) + γ(M¯ L(¯t) − M¯ S(t+)) % Relaxation

14 % Compute relative suboptimality bound

15

λ(1t)

=

get_min_eig(M¯ L(¯t)),

η(t)

=

|λ(1t) |(K +1) µˆ

16 if η(t) < η then η = η(t) end

17 if η < η¯ then break end

18 end

19 return: η

solving the relaxation (23).3 The pseudocode is presented in Algorithm 3 and the following theorem proves its soundness.

Theorem 14 (Optimality Certiﬁcation). Given a feasible (but not necessarily optimal) solution (qˆ, θˆ1, . . . , θˆK ) of the
TLS rotation estimation problem (15), denote with xˆ = [qˆT, θˆ1qˆT, . . . , θˆK qˆT]T, and µˆ = xˆTQxˆ, the corresponding
solution and cost of the QCQP (19). Then, Algorithm 3
produces a sub-optimality bound η that satisﬁes:

µˆ − µ

≤ η,

(25)

µˆ

where µ is the (unknown) global minimum of the non-convex
TLS rotation estimation problem (15) and the QCQP (19).
Moreover, when the relaxation (23) is tight and the parameter γ in line 13 satisﬁes 0 < γ < 2, and if qˆ = q , θˆk = θk is a global minimizer of the TLS problem (15), then the suboptimality bound η(t) (line 15) converges to zero.

Although a complete derivation of Algorithm 3 and the
proof of Theorem 14 is postponed to Appendix L, now we de-
scribe our key intuitions. The ﬁrst insight is that, given a glob-
ally optimal solution (q , θ1, . . . , θK ) to the non-convex TLS problem (15), whenever the SDP relaxation (23) is tight, Z = (x )(x )T, where x = [(q )T, (θ1q )T, . . . , (θK q )T], is a globally optimal solution to the convex SDP (23). Therefore,

3GNC (or RANSAC) can be seen as a fast primal solver that returns a rank-1 guess of (23) by estimating x (i.e., q and θk, k = 1, . . . , K) and Z = xxT, and the optimality certiﬁer can be seen as a fast dual-from-primal solver that
attempts to compute a dual optimality certiﬁcate for the primal rank-1 guess.

11

we can compute a certiﬁcate of global optimality from the Lagrangian dual SDP of the primal SDP (23) [108, Section 5].4
The second insight is that Lagrangian duality shows that the dual certiﬁcate is a matrix at the intersection between the positive semideﬁnite cone S+ and an afﬁne subspace L¯ (whose expression is derived in Appendix L). Algorithm 3 looks for such a matrix (M¯ L(¯t) in line (12)) using Douglas-Rachford Splitting (DRS) [109], [110], [111], [35], [112], [113] (line 1113), starting from a cleverly chosen initial guess (line 8). Since DRS guarantees convergence to a point in S+ ∩ M from any initial guess whenever the intersection is non-empty, Algorithm 3 guarantees to certify global optimality if provided with an optimal solution, whenever the relaxation (23) is tight.
The third insight is that even when the provided solution is not globally optimal, or when the relaxation is not tight, Algorithm 3 can still compute a sub-optimality bound from the minimum eigenvalue of the dual certiﬁcate matrix (line 15), which is informative of the quality of the candidate solution. To the best of our knowledge, this is the ﬁrst result that can assess the sub-optimality of outlier-robust estimation beyond the instances commonly investigated in statistics [33].
Finally, we further speed up computation by exploiting the problem structure. Appendix M shows that the projections on lines 11 and 12 of Algorithm 3 can be computed in closed form, and Appendix V provides the expression of the initial guess in line 8. Section XI shows the effectiveness of Algorithm 3 in certifying the solutions from GNC [34].
IX. PERFORMANCE GUARANTEES
This section establishes theoretical guarantees for TEASER. While Theorems 7, 13, and 14 establish when we obtain a globally optimal solution from optimization problems involved in TEASER, this section uses these theorems to bound the distance between the estimate produced by TEASER and the ground truth. Section IX-A investigates the noiseless (but outlier-corrupted) case, while Section IX-B considers the general case with noise and outliers.

where for an inlier i, it holds bi = s◦R◦ai + t◦, and (s◦, R◦, t◦) denotes the ground-truth transformation we want to recover. Assume (ii) the outliers are in generic position (e.g., they are perturbed by random noise). If (iii) the inliers computed by TEASER (in each subproblem) have zero residual error for the corresponding subproblem, and (iv) the rotation subproblem produces a valid certiﬁcate (in the sense of Theorems 13 and 14), then with probability 1 the output (sˆ, Rˆ, tˆ) of TEASER exactly recovers the ground truth, i.e., sˆ = s◦, Rˆ = R◦, tˆ = t◦.
A proof of Theorem 15 is given in Appendix N. Conditions (iii) and (iv) can be readily checked using the solution computed by TEASER, in the spirit of certiﬁable algorithms. Assumptions (i) and (ii) only ensure that the problem is wellposed. In particular assumption (i) is the same assumption required to compute a unique transformation without outliers [14]. Assumption (ii) is trickier since it requires assuming the outliers to be “generic” rather than adversarial: intuitively, if we allow an adversary to perturb an arbitrary number of points, s/he can create an identical copy of the points in A with an arbitrary transformation and induce any algorithm into producing an incorrect estimate. Clearly, in such a setup outlier rejection is ill-posed and no algorithm can recover the correct solution.6
This fundamental limitation is captured by the following theorem, which focuses on the case with adversarial outliers.
Theorem 16 (Estimation Contract with Noiseless Inliers and Adversarial Outliers). Assume (i) the set of correspondences is such that the number of inliers Nin and the number of outliers Nout satisfy Nin ≥ Nout + 3 and no three inliers are collinear. If (ii) the inliers computed by TEASER (in each subproblem) have zero residual error for the corresponding subproblem, and (iii) the rotation subproblem produces a valid certiﬁcate (in the sense of Theorems 13 and 14), and (iv) TEASER returns a consensus set that satisﬁes (i), then the output (sˆ, Rˆ, tˆ) of TEASER exactly recovers the ground truth transformation, i.e., sˆ = s◦, Rˆ = R◦, tˆ = t◦.

A. Exact Recovery with Outliers and Noiseless Inliers
We start by analyzing the case in which the inliers are noiseless since it allows deriving stronger performance guarantees and highlights the fundamental difference the outlier generation mechanism can make. As we show below, in a noiseless case with randomly outliers generated, TEASER is able to recover the ground-truth transformation from only 3 inliers (and an arbitrary number of outliers!). On the other hand, when the outlier generation is adversarial, TEASER (and any other approach) requires that more than 50% of the measurements are inliers in order to retrieve the ground truth.

A proof of Theorem 16 is given in Appendix O. We observe that now Theorem 16 provides an even clearer “contract” for TEASER: as long as the number of inliers is sufﬁciently larger than the number of outliers, TEASER provides readilycheckable conditions on whether it recovered the groundtruth transformation. If the percentage of inliers is below 50% (i.e., Nin ≤ Nout), TEASER will attempt to retrieve the transformation that is consistent with the largest set of inliers (in the proof, we show that under the conditions of the theorem, TEASER produces a maximum consensus solution).
To the best of our knowledge, Theorems 15-16 provide the ﬁrst exact recovery results for registration with outliers.

Theorem 15 (Estimation Contract with Noiseless Inliers and Random Outliers). Assume (i) the set of correspondences contains at least 3 noiseless non-collinear and distinct5 inliers,
4The tight SDP relaxation ensures certifying global optimality of the nonconvex problem (NP-hard) is equivalent to certifying global optimality of the convex SDP (tractable).
5Distinct in the sense that no two points occupy the same 3D location.

B. Approximate Recovery with Outliers and Noisy Inliers
The case with noisy inliers is strictly more challenging than the noise-free case. Intuitively, the larger the noise, the more
6In such a case, it might still be feasible to enumerate all potential solutions, but the optimal solution of Consensus Maximization or the robust estimator (6) can no longer be guaranteed to be close to the ground truth.

12

blurred is the boundary between inliers and outliers. From the theoretical standpoint, this translates into weaker guarantees.

Theorem 17 (Estimation Contract with Noisy Inliers and Adversarial Outliers). Assume (i) the set of correspondences contains at least 4 non-coplanar inliers, where for any inlier i,
bi − s◦R◦ai − t◦ ≤ β, and (s◦, R◦, t◦) denotes the groundtruth transformation. Assume (ii) the inliers belong to the maximum consensus set in each subproblem7, and (iii) the
second largest consensus set is “sufﬁciently smaller” than
the maximum consensus set (as formalized in Lemma 23).
If (iv) the rotation subproblem produces a valid certiﬁcate (Theorems 13-14), the output (sˆ, Rˆ, tˆ) of TEASER satisﬁes:

|sˆ − s◦| ≤ 2 max αij,

(26)

ij

s◦R◦ − sˆRˆ F

≤

√ 23

maxij αij

(27)

m√inijhk σmin(Uijhk)

tˆ− t◦ ≤ (9 + 3 3)β,

(28)

where σmin(·) denotes the smallest singular value of a matrix,

and Uijhk is the 3 × 3 matrix Uijhk =

a¯ ij a¯ ij

a¯ ih a¯ ih

. a¯ ik
a¯ ik

Moreover, σmin(Uijhk) is nonzero as long as (every 4 points

ai chosen among) the inliers are not coplanar.

A proof of Theorem 17 is given in Appendix P. To the best of our knowledge, Theorem (17) provides the ﬁrst performance bounds for noisy registration problems with outliers. Condition (i) is similar to the standard assumption that guarantees the existence of a unique alignment in the outlier-free case, with the exception that we now require 4 non-coplanar inliers. While conditions (ii)-(iii) seem different and less intuitive than the ones in Theorem 16, they are a generalization of conditions (i) and (ii) in Theorem 16. Condition (ii) in Theorem 17 ensures that the inliers form a large consensus set (the same role played by condition (i) in Theorem 16). Condition (iii) in Theorem 17 ensures that the largest consensus set cannot be confused with other large consensus sets (the assumption of zero residual played a similar role in Theorem 16). These conditions essentially limit the amount of “damage” that outliers can do by avoiding that they form large sets of mutually consistent measurements.

Remark 18 (Interpretation of the Bounds). The bounds (26)-(28) have a natural geometric interpretation. First of all, we recall from Section V that we deﬁned αij = δij/ a¯ij . Hence αij can be understood as (the inverse of) a signal to noise ratio: δij measures the amount of noise, while a¯ij = aj − ai measures the size of the point cloud (in terms of distance between two points). Therefore, the scale estimate will worsen when the noise becomes large when compared to the size of the point cloud. The parameter αij also inﬂuences the rotation bound (27), which, however, also depends on the smallest singular value of the matrix σmin(Uijhk). The latter measures how far is the point cloud from degenerate conﬁgurations. Finally, the translation bound does not depend on the scale of the point cloud, since for

7In other words: the inliers belong to the largest set S such that for any i, j ∈ S, |sij − s|≤ αij , b¯ij − sRa¯ij ≤ δij , and |[bi − sRai − t]l| ≤ βi, ∀l = 1, 2, 3 for any transformation (s, R, t).

a given scale and rotation a single point would sufﬁce to compute a translation (in other words, the distance among points does not play a role there).
The interested reader can ﬁnd tighter (but more expensive to compute and less intelligible) bounds, under similar assumptions as Theorem 17, in Appendix W.
X. TEASER++: A FAST C++ IMPLEMENTATION
In order to showcase the real-time capabilities of TEASER in real robotics, vision, and graphics applications, we have developed a fast C++ implementation of TEASER, named TEASER++. TEASER++ has been released as an open-source library and can be found at https://github.com/MIT-SPARK/ TEASER-plusplus. This section describes the algorithmic and implementation choices we made in TEASER++.
TEASER++ follows the same decoupled approach described in Section VI. The only exception is that it circumvents the need to solve a large-scale SDP and uses the GNC approach described in [34] for the TLS rotation estimation problem. As described in Section VIII-C, we essentially run GNC and certify a posteriori that it retrieved the globally optimal solution using Algorithm 3. This approach is very effective in practice since –as shown in the experimental section– the scale estimation (Section VI-A) and the max clique inlier selection (Section VI-D) are already able to remove a large number of outliers, hence GNC only needs to solve a more manageable problem with less outliers (a setup in which it has been shown to be very effective [34]). Therefore, by combining a fast heuristic (GNC) with a scalable optimality certiﬁer (DRS), TEASER++ is the ﬁrst fast and certiﬁable registration algorithm in that (i) it enjoys the performance guarantees of Section IX, and (ii) it runs in milliseconds in practical problem instances.
TEASER++ uses Eigen3 for fast linear algebra operations. To further optimize for performance, we have implemented a large portion of TEASER++ using shared-memory parallelism. Using OpenMP [114], an industry-standard interface for scalable parallel programming, we have parallelized Algorithm 2, as well as the computation of the TIMs and TRIMs. In addition, we use a fast exact parallel maximum clique ﬁnder algorithm [115] to ﬁnd the maximum clique for outlier pruning.
To facilitate rapid prototyping and visualization, in addition to the C++ library, we provide Python and MATLAB bindings. Furthermore, we provide a ROS [116] wrapper to enable easy integration and deployment in real-time robotics applications.
XI. EXPERIMENTS AND APPLICATIONS
The goal of this section is to (i) test the performance of each module presented in this paper, including scale, rotation, translation estimation, the MCIS pruning, and the optimality certiﬁcation Algorithm 3 (Section XI-A), (ii) show that TEASER and TEASER++ outperform the state of the art on standard benchmarks (Section XI-B), (iii) show that TEASER++ also solves Simultaneous Pose and Correspondence (SPC) problems and dominates existing methods, such as ICP (Section XI-C), (iv) show an application of TEASER for object localization in an RGB-D robotics dataset (Section XI-D), and (v) show an application of TEASER++ to difﬁcult scan-matching problems with deep-learned correspondences (Section XI-E).

13

Scale Error

Rotation Error [deg]

TLS

100

Max Consensus

10-1

10-2

10-3

10-4

0

20

40

60

70

80

90

Outlier Rate (%)

(a) Scale Estimation

102

SDP: Quaternion

SDP: Rotation Matrix

GNC

101

Outlier Rate (%)

100

90

80

After MCIS

70

Before MCIS

60

50

40

30

20

10

0 50 60 70 80 90 91 92 93 94 95 96 97 98 99 Outlier Rate (%)
(b) MCIS

100

TLS

Max Consensus

10-1

Translation Error [m]

10-2 100

10-1

10-3

0

20

40

60

80

90

Outlier Rate (%)

(c) Rotation Estimation

0

20

40

60

70

80

90

Outlier Rate (%)

(d) Translation Estimation

Fig. 4. Results for scale, rotation, translation estimation, and impact of maximal clique inlier selection (MCIS) for increasing outlier rates.

Implementation Details. TEASER++ is implemented as discussed in Section X. TEASER is implemented in MATLAB, uses cvx [117] to solve the convex relaxation (23), and uses the algorithm in [105] to ﬁnd the maximum clique in the pruned TIM graph (see Theorem 6). In all tests we set c¯ = 1. All tests are run on a laptop with an i7-8850H CPU and 32GB of RAM.

A. Testing TEASER’s and TEASER++’s Modules
Testing Setup. We use the Bunny point cloud from the Stanford 3D Scanning Repository [97]. The bunny is downsampled to N = 40 points (unless speciﬁed otherwise) and resized to ﬁt inside the [0, 1]3 cube to create point cloud A. To create point cloud B with N correspondences, ﬁrst a random transformation (s, R, t) (with 1 ≤ s ≤ 5, t ≤1, and R ∈ SO(3)) is applied according to eq. (4), and then random bounded noise i’s are added (we sample i ∼ N (0, σ2I) until i ≤βi). We set σ = 0.01 and βi = 5.54σ such that P i 2> βi2 ≤ 10−6 (cf. Remark 20 in Appendix B). To generate outlier correspondences, we replace a fraction of the bi’s with vectors uniformly sampled inside the sphere of radius 5. We test increasing outlier rates from 0% (all inliers) to 90%; we test up to 99% outliers when relevant (e.g., we omit testing above 90% if failures are already observed at 90%). All statistics are computed over 40 Monte Carlo runs unless mentioned otherwise.
Scale Estimation. Given two point clouds A and B, we ﬁrst create N (N − 1)/2 TIMs corresponding to a complete graph and then use Algorithm 2 to solve for the scale. We compute both Consensus Maximization [118] and TLS estimates of the scale. Fig. 4(a) shows box plots of the scale error with increasing outlier ratios. The scale error is computed as |sˆ−s◦|, where sˆ is the scale estimate and s◦ is the ground-truth. We observe the TLS solver is robust against 80% outliers, while Consensus Maximization failed twice in that regime.
Maximal Clique Inlier Selection (MCIS). We downsample Bunny to N = 1000 and ﬁx the scale to s = 1 when applying the random transformation. We ﬁrst prune the outlier TIMs/TRIMs (edges) that are not consistent with the scale s = 1,

GNC with rotation error less than 1 degree 0.1% sub-optimality certified within 200 iterations 100

Percentage [%]

80

60

40

20

Fig. 5.

0 0 10 20 30 40 50 60 70 80 90 Outlier ratio [%]
GNC and optimality certiﬁcation for TLS robust rotation search.

while keeping all the points (vertices), to obtain the graph G(V, E ). Then we compute the maximum clique in G(V, E ) and remove all edges and vertices outside the maximum clique, obtaining a pruned graph G (V , E ) (cf. line 8 in Algorithm 1). Fig. 4(b) shows the outlier ratio in G (label: “Before MCIS”) and G (label: “After MCIS”). MCIS effectively reduces outlier rate to below 10%, facilitating rotation and translation estimation, which, in isolation, can already tolerate more than 90% outliers (< 90% when using GNC).
Rotation Estimation. We simulate TIMs by applying a random rotation R to the Bunny, and ﬁxing s = 1 and t = 0 (we set the number of TIMs to K = 40). We compare three approaches to solve the TLS rotation estimation problem (12): (i) the quaternion-based SDP relaxation in eq. (23) and [39] (SDP: Quaternion), (ii) the rotation-matrix-based SDP relaxation we proposed in [38] (SDP: Rotation Matrix), and (iii) the GNC heuristic in [34]. For each approach, we evaluate the rotation error as arccos tr RˆTR◦ − 1 /2 , which is the geodesic distance between the rotation estimate Rˆ (produced by each approach) and the ground-truth R◦ [119].
Fig. 4(c) reports the rotation error for increasing outlier rates. The GNC heuristic performs well below 80% outliers and then starts failing. The two relaxations ensure similar performance, while the quaternion-based relaxation proposed in this paper is slightly more accurate at high outlier rates. Results in Appendix R show that the quaternion-based relaxation is always tighter than the relaxation in [38], which often translates into better estimates. The reader can ﬁnd more experiments using our quaternion-based relaxation in [39], where we also demonstrate its robustness against over 95% outliers and discuss applications to panorama stitching.
While the quaternion-based relaxation dominates in terms of accuracy and robustness, it requires solving a large SDP. Therefore, in TEASER++, we opted to use the fast GNC heuristic instead: this choice is motivated by the observation that MCIS is already able to remove most of the outliers (Fig. 4(b)) hence within TEASER++ the rotation estimation only requires solving a problem with less than 10% outliers.
Translation Solver. We apply a random translation t to the Bunny, and ﬁx s = 1 and R = I3. Fig. 4(d) shows that component-wise translation estimation using both Consensus Maximization and TLS are robust against 80% outliers. The translation error is deﬁned as tˆ − t◦ , the 2-norm of the difference between the estimate tˆ and the ground-truth t◦. As mentioned above, most outliers are typically removed before translation estimation, hence enabling TEASER and TEASER++ to be robust to extreme outlier rates (more in Section XI-B).
Optimality Certiﬁcation for Rotation Estimation. We

14

now test the effectiveness of Algorithm 3 in certifying optimality of a rotation estimate. We consider the same setup we used for testing rotation estimation but with K = 100 TIMs. We use the GNC scheme in [34] to solve problem (12) and then use Algorithm 3 to certify the solution of GNC. Fig. 5 shows the performance of GNC and the certiﬁcation algorithm under increasing outlier rates, where 100 Monte Carlo runs are performed at each outlier rate. The blue bars show the percentage of runs for which GNC produced a solution with less than 1 degree rotation error with respect to the ground truth; the green bars show the percentage of runs for which Algorithm 3 produced a relative sub-optimality bound lower than 0.1% in less than 200 iterations of DRS. Fig. 5 demonstrates that: (i) the GNC scheme typically produces accurate solutions to the TLS rotation search problem with < 80% outliers (a result that conﬁrms the errors we observed in Fig. 4(c)); (ii) the optimality certiﬁcation algorithm 3 can certify all the correct GNC solutions and reject all incorrect GNC solutions within 200 iterations. On average, the certiﬁcation algorithm takes 24 iterations to obtain < 0.1% sub-optimality bound, where each DRS iteration takes 50ms in C++.
B. Benchmarking on Standard Datasets
Testing Setup. We benchmark TEASER and TEASER++ against two state-of-the-art robust registration techniques: Fast Global Registration (FGR) [55] and Guaranteed Outlier REmoval (GORE) [19]. In addition, we test two RANSAC variants (with 99% conﬁdence): a fast version where we terminate RANSAC after a maximum of 1,000 iterations (RANSAC1K) and a slow version where we terminate RANSAC after 60 seconds (RANSAC1min). We use four datasets, Bunny, Armadillo, Dragon, and Buddha, from the Stanford 3D Scanning Repository [97] and downsample them to N = 100 points. The tests below follow the same protocol of Section XI-A. Here we focus on the results on the Bunny dataset and we postpone the (qualitatively similar) results obtained on the other three datasets to Appendix S. Appendix S also showcases the proposed algorithms on registration problems with high noise (σ = 0.1), and with up to 10,000 correspondences.
Known Scale. We ﬁrst evaluate the compared techniques with known scale s = 1. Fig. 6(a) shows the rotation error, translation error, and timing for increasing outlier rates on the Bunny dataset. From the rotation and translation errors, we note that TEASER, TEASER++, GORE, and RANSAC1min are robust against up to 90% outliers, although TEASER and TEASER++ tend to produce more accurate estimates than GORE, and RANSAC1min typically requires over 105 iterations for convergence at 90% outlier rate. FGR can only resist 70% outliers and RANSAC1K starts breaking at 90% outliers. TEASER++’s performance is on par with TEASER for all outlier rates, which is expected from the observations in Section X. The timing subplot at the bottom of Fig. 6(a) shows that TEASER is impractical for real-time robotics applications. On the other hand, TEASER++ is one of the fastest techniques across the spectrum, and is able to solve problems with large number of outliers in less than 10ms on a standard laptop.
Extreme Outlier Rates. We further benchmark the performance of TEASER and TEASER++ under extreme outlier

rates from 95% to 99% with known scale and N = 1000 correspondences on the Bunny. We replace RANSAC1K with RANSAC10K, since RANSAC1K already performs poorly at 90% outliers. Fig. 6(b) shows the boxplots of the rotation errors, translation errors, and timing. TEASER, TEASER++, and GORE are robust against up to 99% outliers, while RANSAC1min with 60s timeout can resist 98% outliers with about 106 iterations. RANSAC10K and FGR perform poorly under extreme outlier rates. While GORE, TEASER and TEASER++ are both robust against 99% outliers, TEASER, and TEASER++ produce lower estimation errors, with TEASER++ being one order of magnitude faster than GORE (bottom subﬁgure). We remark that TEASER++’s robustness against 99% outliers is due in large part to the drastic reduction of outlier rates by MCIS.
Unknown Scale. GORE is unable to solve for the scale, hence we only benchmark TEASER and TEASER++ against FGR,8 RANSAC1K, and RANSAC1min. Fig. 6(c) plots scale, rotation, translation error and timing for increasing outliers on the Bunny dataset. All the compared techniques perform well when the outlier ratio is below 60%. FGR has the lowest breakdown point and fails at 70%. RANSAC1K, TEASER, and TEASER++ only fail at 90% outlier ratio when the scale is unknown. Although RANSAC1min with 60s timeout outperforms other methods at 90% outliers, it typically requires more than 105 iterations to converge, which is not practical for real-time applications. TEASER++ consistently runs in less than 30ms.
C. Simultaneous Pose and Correspondences (SPC)
Here we provide a proof-of-concept of how to use TEASER++ in the case where we do not have correspondences.
Testing Setup. We obtain the source point cloud A by downsampling the Bunny dataset to 100 points. Then, we create the point cloud B by applying a random rotation and translation to A. Finally. we remove a percentage of the points in B to simulate partial overlap between A and B. For instance, when the overlap is 80%, we discard 20% (randomly chosen) points from B. We compare TEASER++ against (i) ICP initialized with the identity transformation; and (ii) Go-ICP [20] with 30%, 60% and 90% trimming percentages to be robust to partial overlap. For TEASER++, we generate all possible correspondences: in other words, for each point in A we add all points in B as a potential correspondence (for a total of |A|·|B| correspondences). We then feed the correspondences to TEASER++ that computes a transformation without the need for an initial guess. Clearly, most of the correspondences fed to TEASER are outliers, but we rely on TEASER++ to ﬁnd the small set of inliers.
Fig. 7(a)-(b) show the rotation and translation errors for different levels of overlap between A and B. ICP fails to compute the correct transformation in practically all instances, since the initial guess is not in the basin of convergence of the optimal solution. Go-ICP is a global method which does not require an initial guess and performs much better than ICP. However, Go-ICP requires the user to set a trimming percentage to deal with partial overlap, and Fig. 7(a)-
8Although the original algorithm in [55] did not solve for the scale, we extend it by using Horn’s method to compute the scale at each iteration

15

TEASER++ TEASER

FGR RANSAC1min

RANSAC1K

10 0

Scale Error

10 -2

TEASER++ TEASER

GORE FGR

RANSAC1min RANSAC1K

10 2

TEASER++ TEASER

GORE FGR

RANSAC1min RANSAC10K

10 2

10 -4

60

70

80

Outlier Rate (%)

TEASER++ TEASER

FGR RANSAC1min

10 2

90 RANSAC1K

Rotation Error [deg]

Rotation Error [deg]

Rotation Error [deg]

10 1

10 1

10 1

10 0

10 0

10 0

10 -1

0

20

60

70

80

90

Outlier Rate (%)

TEASER++ TEASER

GORE FGR

RANSAC1min RANSAC1K

10 -1

95

96

97

98

99

Outlier Rate (%)

TEASER++ TEASER

GORE FGR

RANSAC1min RANSAC10K

10 -1

60
TEASER++ TEASER

70

80

Outlier Rate (%)

FGR RANSAC1min

90 RANSAC1K

Translation Error [m]

Translation Error [m]

Translation Error [m]

10 0 10 -1 10 -2

10 0 10 -1 10 -2

10 0 10 -1 10 -2

10 -3

0

20

60

70

80

90

Outlier Rate (%)

TEASER++ TEASER

GORE FGR

RANSAC1min RANSAC1K

10 2

10 -3

95

96

97

98

99

Outlier Rate (%)

TEASER++ TEASER

GORE FGR

RANSAC1min RANSAC10K

10 2

10 -3 60

70

80

Outlier Rate (%)

TEASER++ TEASER

FGR RANSAC1min

10 2

90 RANSAC1K

Time [s]

10 0

10 1

Time [s]

Time [s]

10 0

10 -2

10 0

10 -4 0

20

60

70

80

90

Outlier Rate (%)

10 -1

95

96

97

98

99

Outlier Rate (%)

10 -2

60

70

80

90

Outlier Rate (%)

(a) Known Scale

(b) Known Scale (Extreme Outliers)

(c) Unknown Scale

Fig. 6. Benchmark results. (a) Boxplots of rotation errors, translation errors, and timing for the six compared methods on the Bunny dataset with known scale (the top ﬁgure shows a registration example with 50% outlier correspondences). (b) Same as before, but for outlier rates between 95% and 99% (the top ﬁgure shows an example with 95% outlier correspondences). (c) Boxplots of scale, rotation, translation errors and timing for the ﬁve registration methods that support scale estimation on the Bunny dataset with unknown scale.

(b) show the performance of Go-ICP is highly sensitive to the trimming percentage (Go-ICP (60%) performs the best but is still less accurate than TEASER++). Moreover, GoICP takes 16 seconds on average due to its usage of BnB. TEASER++ computes a correct solution across the spectrum without the need for an initial guess. TEASER++ only starts failing when the overlap drops below 10%. The price we pay for this enhanced robustness is an increase in runtime. We feed |A|·|B|≈ 104 correspondences to TEASER++, which increases the runtime compared to the correspondence-based setup in which the number of correspondences scales linearly

(rather than quadratically) with the point cloud size. Fig. 7(c) reports the timing breakdown for the different modules in TEASER++. From the ﬁgure, we observe that (i) for small overlap, TEASER++ is not far from real-time, (ii) the timing is dominated by the maximum clique computation and scale estimation, where the latter includes the computation of the translation and rotation invariant measurements (TRIMs), and (iii) TEASER++ is orders of magnitude faster than Go-ICP when the overlap is low (e.g., below 40%).

16

Rotation Error [deg] Runtime Breakdown [s]
Translation Error [m]

TEASER++ Go-ICP (30%) Go-ICP (60%)

Go-ICP (90%) ICP

10 2

10 0 10 -1

TEASER++ Go-ICP (30%) Go-ICP (60%)

Go-ICP (90%) ICP

10 0

10 -2

100

80

60

40

20

Overlap (%)

(a) Rotation Error

25

20

15

10 -3

10

100

80

60

40

20

10

Overlap (%)

(b) Translation Error

TRIMs & Scale Estimation Max Clique TIMs & Rotation Estimation Translation Estimation

10

5

0 100 80 60 40 20 10 Overlap (%)
(c) Timing Breakdown
Fig. 7. (a)-(b) Rotation and translation errors for TEASER++, ICP, and GoICP in a correspondence-free problem. (c) Timing breakdown for TEASER++.

D. Application 1: Object Pose Estimation and Localization

Testing Setup. We use the large-scale point cloud datasets from [36] to test TEASER in object pose estimation and localization applications. We ﬁrst use the ground-truth object labels to extract the cereal box/cap out of the scene and treat it as the object, then apply a random transformation to the scene, to get an object-scene pair. To register the object-scene pair, we ﬁrst use FPFH feature descriptors [1] to establish putative correspondences. Given correspondences from FPFH, TEASER is used to ﬁnd the relative pose between the object and scene. We downsample the object and scene using the same ratio (about 0.1) to make the object have 2,000 points.
Results. Fig. 8 shows the noisy FPFH correspondences, the inlier correspondences obtained by TEASER, and successful localization and pose estimation of the cereal box. Another example is given in Fig. 1(g)-(h). Qualitative results for eight scenes are given in Appendix T. The inlier correspondence ratios for cereal box are all below 10% and typically below 5%. TEASER is able to compute a highly accurate estimate of the pose using a handful of inliers. Table I shows the mean and standard deviation (SD) of the rotation and translation errors, the number of FPFH correspondences, and the inlier ratio estimated by TEASER on the eight scenes.

E. Application 2: Scan Matching
Testing Setup. TEASER++ can also be used in robotics applications that need robust scan matching, such as 3D reconstruction and loop closure detection in SLAM [120]. We evaluate TEASER++’s performance in such scenarios using the 3DMatch dataset [37], which consists of RGB-D scans from 62 real-world indoor scenes. The dataset is divided into 54 scenes for training, and 8 scenes for testing. The authors provide 5,000 randomly sampled keypoints for each scan. On average, there are 205 pairs of scans per scene (maximum: 519 in the Kitchen scene, minimum: 54 in the Hotel 3 scene).

We use 3DSmoothNet [2], a state-of-the-art neural network, to compute local descriptors for each 3D keypoint, and generate correspondences using nearest-neighbor search. We then feed the correspondences to TEASER++ and RANSAC (implemented in Open3D [121]) and compare their performance in terms of percentage of successfully matched scans and runtime. Due to the large number of pairs we need to test, we run the experiments on a server with a Xeon Platinum 8259CL CPU at 2.50GHz, and allocate 12 threads for each algorithm under test. Two scans are successful matched when the transformation computed by a technique has (i) rotation error smaller than 10°, and (ii) translation error less than 30 cm. We report RANSAC’s results with maximum number of iterations equal to 1,000 (RANSAC1K) and 10,000 (RANSAC10K). We also compare the percentage of successful registrations out of the cases where TEASER++ certiﬁed the rotation estimation as optimal, a setup we refer to as TEASER++ (CERT). The latter executes Algorithm 3 to evaluate the poses from TEASER++ and uses a desired sub-optimality gap η¯ = 3% to certify optimality. We use βi = β = 5 cm for TEASER++ in all tests.
Results. Table II shows the percentage of successfully matched scans and the average timing for the four compared techniques. TEASER dominates both RANSAC variants with exception of the Lab scene. RANSAC1K has a success rate up to 12% worst than TEASER++. RANSAC10K is an optimized C++ implementation and, while running slower than TEASER++, it cannot reach the same accuracy for most scenes. These results further highlight that TEASER++ can be safely used as a faster and more robust replacement for RANSAC in SLAM pipelines. This conclusion is further reinforced by the last row in Table II, where we show the success rate for the poses certiﬁed as optimal by TEASER++. The success rate strictly dominates both RANSAC variants and TEASER++, since TEASER++ (CERT) is able to identify and reject unreliable registration results. This is a useful feature when scan matching is used for loop closure detection in SLAM, since bad registration results lead to incorrect loop closures and can compromise the quality of the resulting map (see [54], [34]). While running Algorithm 3 in TEASER++ (CERT) requires more than 200 seconds in average, the majority of instances are solved by TEASER++ (CERT) within 100 seconds and problems with fewer than 50 TIMs can be certiﬁed in 1.12 seconds. Fig. 9 compares the runtime of our certiﬁcation approach against MOSEK (which directly solves the SDP relaxation) for 50, 100, and 110 TIMs. We can see that TEASER++ (CERT) is orders of magnitude faster, and can certify large-scale problems beyond the reach of MOSEK, which runs out of memory for over 150 TIMs.
Why is TEASER++ not able to solve 100% of the tests in Table II? This should not come as a surprise from the statements in Theorem 15-17. The estimation contracts discussed in Section IX require a minimum number of inliers: TEASER++ can mine a small number of inliers among a large number of outliers, but it cannot solve problems where no inliers are given or where the inliers are not enough to identify a unique registration! For the experiments in this section, it is not uncommon to have no or fewer than 3 inliers in a scene due to the quality of the keypoint descriptors. Moreover, it is not uncommon to have symmetries in the scene, which

17

Fig. 8. Successful object pose estimation by TEASER on a real RGB-D dataset. Blue lines are the original FPFH [1] correspondences with outliers, green lines are the inlier correspondences computed by TEASER, and the ﬁnal registered object is highlighted in red.

Rotation error [rad] Translation error [m] # of FPFH correspondences FPFH inlier ratio [%]

Mean 0.066 0.069 525 6.53

SD 0.043 0.053 161 4.59

TABLE I REGISTRATION RESULTS ON EIGHT SCENES OF THE
RGB-D DATASET [36].

250

10 4

Scenes

# of Registration Problems Average Runtime [s]
Kitchen (%) Home 1 (%) Home 2 (%) Hotel 1 (%) Hotel 2 (%) Hotel 3 (%) Study (%) MIT Lab (%) Avg. Runtime [s]

200 10 3

150

10 2

100

TEASER++ (CERT)

50

MOSEK

10 1

0 0

500

1000

# of TIMs

1500

10 0

Fig. 9. # of registration problems (histogram) and

average runtime (scatter plot, compared with MOSEK)

for TEASER++ (CERT) w.r.t. # of TIMs passed to

certiﬁcation in 3DMatch.

RANSAC-1K 91.3 89.1 74.5 94.2 84.6 90.7 86.3 81.8 0.008

RANSAC-10K 97.2 92.3 79.3 96.5 86.5 94.4 90.4 85.7 0.074

TEASER++

98.6 92.9 86.5 97.8 89.4 94.4 91.1 83.1 0.059

TEASER++ (CERT) 99.4 94.1 88.7 98.2 91.9 94.4 94.3 88.6 238.136

TABLE II PERCENTAGE OF CORRECT REGISTRATION RESULTS AND AVERAGE RUNTIME USING
TEASER++, TEASER++ CERTIFIED, AND RANSAC ON THE 3DMatch DATASET.

make the registration non-unique. To intuitively highlight this

issue, Fig. 10 shows the rotation errors of TEASER++, with

different markers for certiﬁed (blue dots) and non-certiﬁed (red

crosses) solutions. The ﬁgure shows that (i) TEASER++ (CERT)

is able to reject a large number of incorrect estimates (red

crosses with large errors), and (ii) some of the incorrect but

certiﬁed solutions exhibit errors around 90° and 180° which

correspond to symmetries of the scene. A visual example of the phenomenon is shown in Fig. 11. These results highlight the need for better keypoint detectors, since even state-of-the-art

(b) Ground truth (side view)

deep-learning-based methods struggle to produce acceptable

outlier rates in real problems.

Rotation error [deg]

180
160
140
120
100
80
60
40
20
0 Kitchen Home 1 Home 2 Hotel 1 Hotel 2 Hotel 3 Study MIT Lab Scenes
Fig. 10. Rotation errors for each scene (data points correspond to pairs of scans in the scene), with certiﬁed TEASER++ solutions (blue dots) vs. noncertiﬁed (red crosses).
XII. CONCLUSION
We propose the ﬁrst fast and certiﬁable algorithm for correspondence-based registration with extreme outlier rates. We leverage insights from estimation theory (e.g., unknownbut-bounded noise), geometry (e.g., invariant measurements), graph theory (e.g., maximum clique for inlier selection), and optimization (e.g., tight SDP relaxations). These insights lead to the design of two certiﬁable registration algorithms. TEASER is accurate and robust but requires solving a large SDP. TEASER++ has similar performance in practice but circumvents the need to solve an SPD and can run in milliseconds.

(a) Ground truth (top-down view)

(c) TEASER++’s estimate (top-down view)

Fig. 11. Example from the Hotel 3 scene of the 3DMatch dataset [37]. The scene pictures a hotel shower where symmetries in the keypoint distribution cause TEASER++ to fail. (a)-(b) Ground-truth alignment (top view and side view). The ground-truth alignment admits 3 inlier correspondences (in green). (c) TEASER++’s estimate of the alignment. TEASER++ is able to ﬁnd an estimate with 57 inliers. The symmetries of the scene allow for multiple potential registrations, and the ground truth inliers are not part of the maximum clique of the TIM graph. In this case, the max clique selected by TEASER++ contains an incorrect solution that differs from the ground truth by 180°.

TEASER++ is also certiﬁable by leveraging Douglas-Rachford Splitting to compute a dual optimality certiﬁcate. For both algorithms, we provide theoretical bounds on the estimation errors, which are the ﬁrst of their kind for robust registration problems. Moreover, we test their performance on standard benchmarks, object detection datasets, and the 3DMatch scan matching dataset and show that (i) both algorithms dominate the state of the art (e.g., RANSAC, BnB, heuristics) and are robust to more than 99% outliers, (ii) TEASER++ can run in milliseconds and it is currently the fastest robust registration algorithm at high outlier rates, (iii) TEASER++ is so robust it can also solve problems without correspondences (e.g.,

18

hypothesizing all-to-all correspondences) where it outperforms
ICP and Go-ICP. We release a fast open-source C++ implemen-
tation of TEASER++.
While not central to the technical contribution, this paper
also establishes the foundations of certiﬁable perception, as
discussed in Appendix A. This research area offers many
research opportunities and the resulting progress has the po-
tential to boost trustworthiness and reliability in safety-critical
application of robotics and computer vision. Future work
includes developing certiﬁable algorithms for other spatial
perception problems (ideally leading to certiﬁable approaches
for all the applications discussed in [34] and more). Another
avenue for future research is the use of the certiﬁably robust
algorithms presented in this paper for self-supervision of deep
learning methods for keypoint detection and matching.
REFERENCES
[1] R. Rusu, N. Blodow, and M. Beetz, “Fast point feature histograms (fpfh) for 3d registration,” in IEEE Intl. Conf. on Robotics and Automation (ICRA). Citeseer, 2009, pp. 3212–3217.
[2] Z. Gojcic, C. Zhou, J. D. Wegner, and A. Wieser, “The perfect match: 3d point cloud matching with smoothed densities,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5545–5554.
[3] P. Henry, M. Krainin, E. Herbst, X. Ren, and D. Fox, “Rgb-d mapping: Using kinect-style depth cameras for dense 3d modeling of indoor environments,” Intl. J. of Robotics Research, vol. 31, no. 5, pp. 647– 663, 2012.
[4] G. Blais and M. D. Levine, “Registering multiview range data to create 3d computer objects,” IEEE Trans. Pattern Anal. Machine Intell., vol. 17, no. 8, pp. 820–824, 1995.
[5] S. Choi, Q. Y. Zhou, and V. Koltun, “Robust reconstruction of indoor scenes,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 5556–5565.
[6] J. Zhang and S. Singh, “Visual-lidar odometry and mapping: Lowdrift, robust, and fast,” in IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2015, pp. 2174–2181.
[7] B. Drost, M. Ulrich, N. Navab, and S. Ilic, “Model globally, match locally: Efﬁcient and robust 3D object recognition,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 998– 1005.
[8] J. M. Wong, V. Kee, T. Le, S. Wagner, G. L. Mariottini, A. Schneider, L. Hamilton, R. Chipalkatty, M. Hebert, D. M. S. Johnson et al., “Segicp: Integrated deep semantic segmentation and pose estimation,” in IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 5784–5789.
[9] A. Zeng, K. T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, and J. Xiao, “Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge,” in IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2017, pp. 1386–1383.
[10] P. Marion, P. R. Florence, L. Manuelli, and R. Tedrake, “Label fusion: A pipeline for generating ground truth labels for real rgbd data of cluttered scenes,” in IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2018, pp. 1–8.
[11] J. Bazin, Y. Seo, R. Hartley, and M. Pollefeys, “Globally optimal inlier set maximization with unknown rotation and focal length,” in European Conf. on Computer Vision (ECCV), 2014, pp. 803–817.
[12] M. A. Audette, F. P. Ferrie, and T. M. Peters, “An algorithmic overview of surface registration techniques for medical imaging,” Med. Image Anal., vol. 4, no. 3, pp. 201–217, 2000.
[13] G. K. L. Tam, Z. Q. Cheng, Y. K. Lai, F. C. Langbein, Y. Liu, D. Marshall, R. R. Martin, X. F. Sun, and P. L. Rosin, “Registration of 3d point clouds and meshes: a survey from rigid to nonrigid.” IEEE Trans. Vis. Comput. Graph., vol. 19, no. 7, pp. 1199–1217, 2013.
[14] B. K. P. Horn, “Closed-form solution of absolute orientation using unit quaternions,” J. Opt. Soc. Amer., vol. 4, no. 4, pp. 629–642, Apr 1987.
[15] K. Arun, T. Huang, and S. Blostein, “Least-squares ﬁtting of two 3-D point sets,” IEEE Trans. Pattern Anal. Machine Intell., vol. 9, no. 5, pp. 698–700, sept. 1987.
[16] F. Tombari, S. Salti, and L. D. Stefano, “Performance evaluation of 3d keypoint detectors,” Intl. J. of Computer Vision, vol. 102, no. 1-3, pp. 198–220, 2013.

[17] P. J. Besl and N. D. McKay, “A method for registration of 3-D shapes,” IEEE Trans. Pattern Anal. Machine Intell., vol. 14, no. 2, 1992.
[18] M. Fischler and R. Bolles, “Random sample consensus: a paradigm for model ﬁtting with application to image analysis and automated cartography,” Commun. ACM, vol. 24, pp. 381–395, 1981.
[19] Á. Parra Bustos and T. J. Chin, “Guaranteed outlier removal for point cloud registration with correspondences,” IEEE Trans. Pattern Anal. Machine Intell., vol. 40, no. 12, pp. 2868–2882, 2018.
[20] J. Yang, H. Li, D. Campbell, and Y. Jia, “Go-ICP: A globally optimal solution to 3D ICP point-set registration,” IEEE Trans. Pattern Anal. Machine Intell., vol. 38, no. 11, pp. 2241–2254, Nov. 2016.
[21] A. Bandeira, “A note on probably certiﬁably correct algorithms,” Comptes Rendus Mathematique, vol. 354, no. 3, pp. 329–333, 2016.
[22] L. Carlone and F. Dellaert, “Duality-based veriﬁcation techniques for 2D SLAM,” in IEEE Intl. Conf. on Robotics and Automation (ICRA), 2015, pp. 4589–4596, (pdf) (code).
[23] D. Rosen, L. Carlone, A. Bandeira, and J. Leonard, “SE-Sync: a certiﬁably correct algorithm for synchronization over the Special Euclidean group,” Intl. J. of Robotics Research, 2018, accepted, arxiv preprint: 1611.00128, (pdf).
[24] Q. M. Rahman, N. Sünderhauf, and F. Dayoub, “Did you miss the sign? A false negative alarm system for trafﬁc sign detectors,” CoRR, vol. abs/1903.06391, 2019. [Online]. Available: http://arxiv.org/abs/1903.06391
[25] S. Seshia and D. Sadigh, “Towards veriﬁed artiﬁcial intelligence,” ArXiv, vol. abs/1606.08514, 2016.
[26] A. Desai, T. Dreossi, and S. Seshia, “Combining model checking and runtime veriﬁcation for safe robotics,” in International Conference on Runtime Veriﬁcation. Springer, 2017, pp. 172–189.
[27] A. Makadia, A. Patterson, and K. Daniilidis, “Fully automatic registration of 3d point clouds,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), vol. 1, 2006, pp. 1297–1304.
[28] Y. Liu, C. Wang, Z. Song, and M. Wang, “Efﬁcient global point cloud registration by matching rotation invariant features through translation search,” in European Conf. on Computer Vision (ECCV), September 2018.
[29] M. Milanese, Estimation and Prediction in the Presence of Unknown but Bounded Uncertainty: A Survey. Boston, MA: Springer US, 1989, pp. 3–24.
[30] D. Bertsekas, “Control of uncertain systems with a set-membership description of the uncertainty,” Ph.D. dissertation, Massachusetts Institute of Technology, 1971.
[31] R. Hartley and F. Kahl, “Global optimization through rotation space search,” Intl. J. of Computer Vision, vol. 82, no. 1, pp. 64–79, 2009.
[32] G. Wahba, “A least squares estimate of satellite attitude,” SIAM review, vol. 7, no. 3, pp. 409–409, 1965.
[33] I. Diakonikolas, G. Kamath, D. Kane, J. Li, A. Moitra, and A. Stewart, “Robust estimators in high dimensions without the computational intractability,” in IEEE 57th Annual Symposium on Foundations of Computer Science. IEEE, 2016, pp. 655–664.
[34] H. Yang, P. Antonante, V. Tzoumas, and L. Carlone, “Graduated nonconvexity for robust spatial perception: From non-minimal solvers to global outlier rejection,” IEEE Robotics and Automation Letters (RA-L), vol. 5, no. 2, pp. 1127–1134, 2020, arXiv preprint arXiv:1909.08605 (with supplemental material), (pdf).
[35] H. Yang and L. Carlone, “One ring to rule them all: Certiﬁably robust geometric perception with outliers,” in Advances in Neural Information Processing Systems (NIPS), 2020.
[36] K. Lai, L. Bo, X. Ren, and D. Fox, “A large-scale hierarchical multiview RGB-D object dataset,” in IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2011, pp. 1817–1824.
[37] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser, “3dmatch: Learning the matching of local 3d geometry in range scans,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, vol. 1, no. 2, 2017, p. 4.
[38] H. Yang and L. Carlone, “A polynomial-time solution for robust registration with extreme outlier rates,” in Robotics: Science and Systems (RSS), 2019, (pdf), (video), (media), (media), (media).
[39] ——, “A quaternion-based certiﬁably optimal solution to the Wahba problem with outliers,” in Intl. Conf. on Computer Vision (ICCV), 2019, (Oral Presentation, accept rate: 4%), Arxiv version: 1905.12536, (pdf).
[40] C. Choy, J. Park, and V. Koltun, “Fully convolutional geometric features,” in Intl. Conf. on Computer Vision (ICCV), 2019, pp. 8958– 8966.
[41] C. Olsson, F. Kahl, and M. Oskarsson, “Branch-and-bound methods for euclidean registration problems,” IEEE Trans. Pattern Anal. Machine Intell., vol. 31, no. 5, pp. 783–794, 2009.

19

[42] J. Briales and J. Gonzalez-Jimenez, “Convex Global 3D Registration with Lagrangian Duality,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017.
[43] F. L. Markley and J. L. Crassidis, Fundamentals of spacecraft attitude determination and control. Springer, 2014, vol. 33.
[44] F. L. Markley, “Attitude determination using vector observations and the singular value decomposition,” The Journal of the Astronautical Sciences, vol. 36, no. 3, pp. 245–258, 1988.
[45] J. Gower and G. Dijksterhuis, “Procrustes problems,” Procrustes Problems, Oxford Statistical Science Series, vol. 30, 01 2005.
[46] P. Schonemann, “A generalized solution of the orthogonal procrustes problem,” Psychometrika, vol. 31, pp. 1–10, 1966.
[47] N. Ohta and K. Kanatani, “Optimal estimation of three-dimensional rotation and reliability evaluation,” IEICE TRANSACTIONS on Information and Systems, vol. 81, no. 11, pp. 1247–1252, 1998.
[48] Y. Cheng and J. L. Crassidis, “A total least-squares estimate for attitude determination,” in AIAA Scitech 2019 Forum, 2019, p. 1176.
[49] S. Ahmed, E. C. Kerrigan, and I. M. Jaimoukha, “A semideﬁnite relaxation-based algorithm for robust attitude estimation,” IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 3942–3952, 2012.
[50] R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision. Cambridge University Press, 2000.
[51] P. Meer, D. Mintz, A. Rosenfeld, and D. Y. Kim, “Robust regression methods for computer vision: A review,” Intl. J. of Computer Vision, vol. 6, no. 1, pp. 59–70, Apr 1991.
[52] K. M. Tavish and T. D. Barfoot, “At all costs: A comparison of robust cost functions for camera correspondence outliers,” in Computer and Robot Vision (CRV), 2015 12th Conference on. IEEE, 2015, pp. 62– 69.
[53] M. J. Black and A. Rangarajan, “On the uniﬁcation of line processes, outlier rejection, and robust statistics with applications in early vision,” Intl. J. of Computer Vision, vol. 19, no. 1, pp. 57–91, 1996.
[54] P. Lajoie, S. Hu, G. Beltrame, and L. Carlone, “Modeling perceptual aliasing in SLAM via discrete-continuous graphical models,” IEEE Robotics and Automation Letters (RA-L), 2019, extended ArXiv version: (pdf), Supplemental Material: (pdf).
[55] Q. Zhou, J. Park, and V. Koltun, “Fast global registration,” in European Conf. on Computer Vision (ECCV). Springer, 2016, pp. 766–782.
[56] O. Enqvist, K. Josephson, and F. Kahl, “Optimal correspondences from pairwise constraints,” in Intl. Conf. on Computer Vision (ICCV), 2009, pp. 1295–1302.
[57] A. Parra Bustos, T.-J. Chin, F. Neumann, T. Friedrich, and M. Katzmann, “A practical maximum clique algorithm for matching with pairwise constraints,” arXiv preprint arXiv:1902.01534, 2019.
[58] V. Golyanik, S. Aziz Ali, and D. Stricker, “Gravitational approach for point set registration,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5802–5810.
[59] P. Jauer, I. Kuhlemann, R. Bruder, A. Schweikard, and F. Ernst, “Efﬁcient registration of high-resolution feature enhanced point clouds,” IEEE Trans. Pattern Anal. Machine Intell., vol. 41, no. 5, pp. 1102– 1115, 2018.
[60] H. Yang, “A dynamical perspective on point cloud registration,” arXiv preprint arXiv:2005.03190, 2020.
[61] T. J. Chin and D. Suter, “The maximum consensus problem: recent algorithmic advances,” Synthesis Lectures on Computer Vision, vol. 7, no. 2, pp. 1–194, 2017.
[62] F. Wen, R. Ying, Z. Gong, and P. Liu, “Efﬁcient algorithms for maximum consensus robust ﬁtting,” IEEE Trans. Robotics, 2019.
[63] Z. Cai, T.-J. Chin, and V. Koltun, “Consensus maximization tree search revisited,” in Intl. Conf. on Computer Vision (ICCV), 2019, pp. 1637– 1645.
[64] H. M. Le, T.-J. Chin, A. Eriksson, T.-T. Do, and D. Suter, “Deterministic approximate methods for maximum consensus robust ﬁtting,” IEEE Trans. Pattern Anal. Machine Intell., 2019.
[65] V. Tzoumas, P. Antonante, and L. Carlone, “Outlier-robust spatial perception: Hardness, general-purpose algorithms, and guarantees,” in IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2019, extended arxiv version: 1903.11683, (pdf).
[66] T.-J. Chin, Z. Cai, and F. Neumann, “Robust ﬁtting in computer vision: Easy or hard?” in European Conf. on Computer Vision (ECCV), 2018.
[67] D. Campbell, L. Petersson, L. Kneip, and H. Li, “Globally-optimal inlier set maximisation for simultaneous camera pose and feature correspondence,” in Intl. Conf. on Computer Vision (ICCV), 2017, pp. 1–10.
[68] J. C. Bazin, Y. Seo, and M. Pollefeys, “Globally optimal consensus set maximization through rotation search,” in Asian Conference on Computer Vision. Springer, 2012, pp. 539–551.

[69] C. Olsson, O. Enqvist, and F. Kahl, “A polynomial-time bound for matching and registration with outliers,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE, 2008, pp. 1–8.
[70] O. Enqvist, E. Ask, F. Kahl, and K. Åström, “Robust ﬁtting for multiple view geometry,” in European Conf. on Computer Vision (ECCV). Springer, 2012, pp. 738–751.
[71] E. Ask, O. Enqvist, and F. Kahl, “Optimal geometric ﬁtting under the truncated L2-norm,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 1722–1729.
[72] S. Granger and X. Pennec, “Multi-scale EM-ICP: A fast and robust approach for surface registration,” in European Conf. on Computer Vision (ECCV), 2002.
[73] L. Maier-Hein, A. M. Franz, T. R. dos Santos, M. Schmidt, M. Fangerau, H. P. Meinzer, and J. M. Fitzpatrick, “Convergent iterative closest-point algorithm to accomodate anisotropic and inhomogenous localization error,” IEEE Trans. Pattern Anal. Machine Intell., vol. 34, no. 8, pp. 1520–1532, 2012.
[74] D. Chetverikov, D. Stepanov, and P. Krsek, “Robust euclidean alignment of 3D point sets: the trimmed iterative closest point algorithm,” Image and Vision Computing, vol. 23, no. 3, pp. 299–309, 2005.
[75] S. Kaneko, T. Kondo, and A. Miyamoto, “Robust matching of 3D contours using iterative closest point algorithm improved by Mestimation,” Pattern Recognition, vol. 36, no. 9, pp. 2041–2047, 2003.
[76] A. Myronenko and X. Song, “Point set registration: Coherent point drift,” IEEE Trans. Pattern Anal. Machine Intell., vol. 32, no. 12, pp. 2262–2275, 2010.
[77] B. Jian and B. C. Vemuri, “Robust point set registration using gaussian mixture models,” IEEE Trans. Pattern Anal. Machine Intell., vol. 33, no. 8, pp. 1633–1645, 2011.
[78] W. Clark, M. Ghaffari, and A. Bloch, “Nonparametric continuous sensor registration,” arXiv preprint arXiv:2001.04286, 2020.
[79] H. M. Le, T.-T. Do, T. Hoang, and N.-M. Cheung, “SDRSAC: Semideﬁnite-based randomized approach for robust point cloud registration without correspondences,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 124–133.
[80] T. M. Breuel, “Implementation techniques for geometric branch-andbound matching methods,” Comput. Vis. Image Underst., vol. 90, no. 3, pp. 258–294, 2003.
[81] Á. Parra Bustos, T. J. Chin, and D. Suter, “Fast rotation search with stereographic projections for 3d registration,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 3930– 3937.
[82] H. Li and R. Hartley, “The 3D-3D registration problem revisited,” in Intl. Conf. on Computer Vision (ICCV). IEEE, 2007, pp. 1–8.
[83] G. Izatt, H. Dai, and R. Tedrake, “Globally optimal object pose estimation in point clouds with mixed-integer programming,” in Proc. of the Intl. Symp. of Robotics Research (ISRR), 2017.
[84] H. Maron, N. Dym, I. Kezurer, S. Kovalsky, and Y. Lipman, “Point registration via efﬁcient convex relaxation,” ACM Transactions on Graphics (TOG), vol. 35, no. 4, pp. 1–12, 2016.
[85] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on point sets for 3D classiﬁcation and segmentation,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 652–660.
[86] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, “Dynamic graph CNN for learning on point clouds,” ACM Transactions on Graphics (TOG), vol. 38, no. 5, p. 146, 2019.
[87] Y. Aoki, H. Goforth, R. A. Srivatsan, and S. Lucey, “Pointnetlk: Robust & efﬁcient point cloud registration using PointNet,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7163– 7172.
[88] Y. Wang and J. M. Solomon, “Deep Closest Point: Learning Representations for Point Cloud Registration,” in Intl. Conf. on Computer Vision (ICCV), 2019.
[89] ——, “PRNet: Self-Supervised Learning for Partial-to-Partial Registration,” in Advances in Neural Information Processing Systems (NIPS), 2019, pp. 8812–8824.
[90] A. Avetisyan, M. Dahnert, A. Dai, M. Savva, A. X. Chang, and M. Nießner, “Scan2CAD: Learning CAD model alignment in RGBD scans,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 2614–2623.
[91] A. Avetisyan, A. Dai, and M. Nießner, “End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans,” in Intl. Conf. on Computer Vision (ICCV), 2019.
[92] C. Choy, W. Dong, and V. Koltun, “Deep global registration,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020.

20

[93] J. Fredriksson, V. Larsson, C. Olsson, and F. Kahl, “Optimal relative pose with unknown correspondences,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 1728–1736.
[94] T.-Y. Liu and H. Jiang, “Minimizing sum of truncated convex functions and its applications,” Journal of Computational and Graphical Statistics, vol. 28, no. 1, pp. 1–10, 2019.
[95] S. Barratt, G. Angeris, and S. Boyd, “Minimizing a sum of clipped convex functions,” arXiv preprint arXiv:1910.12342, 2019.
[96] F. Chung, Spectral Graph Theory. American Mathematical Soc., CBMS Regional Conference Series in Mathematics, No. 92, 1996.
[97] B. Curless and M. Levoy, “A volumetric method for building complex models from range images,” in SIGGRAPH, 1996, pp. 303–312.
[98] X. Li, Y. Liu, Y. Wang, C. Wang, M. Wang, and Z. Song, “Fast and globally optimal rigid registration of 3d point sets by transformation decomposition,” arXiv preprint arXiv:1812.11307, 2019.
[99] S. Agarwal, V. Shree, and S. Chakravorty, “RFM-SLAM: Exploiting relative feature measurements to separate orientation and position estimation in slam,” in IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE, 2017, pp. 6307–6314.
[100] L. Carlone, R. Aragues, J. Castellanos, and B. Bona, “A fast and accurate approximation for planar pose graph optimization,” Intl. J. of Robotics Research, vol. 33, no. 7, pp. 965–987, 2014, (pdf) (ppt) (code) (video) (datasets: (web)).
[101] S. Perera and N. Barnes, “Maximal cliques based rigid body motion segmentation with a rgb-d camera,” in Asian Conf. on Computer Vision (ACCV). Springer, 2012, pp. 120–133.
[102] C. Bron and J. Kerbosch, “Algorithm 457: ﬁnding all cliques of an undirected graph,” Communications of the ACM, vol. 16, no. 9, pp. 575–577, 1973.
[103] B. Pattabiraman, M. M. A. Patwary, A. H. Gebremedhin, W. K. Liao, and A. Choudhary, “Fast algorithms for the maximum clique problem on massive graphs with applications to overlapping community detection,” Internet Mathematics, vol. 11, no. 4-5, pp. 421–448, 2015.
[104] Q. Wu and J. Hao, “A review on algorithms for maximum clique problems,” European Journal of Operational Research, vol. 242, no. 3, pp. 693–709, 2015.
[105] D. Eppstein, M. Löfﬂer, and D. Strash, “Listing all maximal cliques in sparse graphs in near-optimal time,” in International Symposium on Algorithms and Computation. Springer, 2010, pp. 403–414.
[106] D. Scaramuzza, “1-point-ransac structure from motion for vehiclemounted cameras by exploiting non-holonomic constraints,” Intl. J. of Computer Vision, pp. 1–12, 2011.
[107] M. ApS, The MOSEK optimization toolbox for MATLAB manual. Version 8.1., 2017. [Online]. Available: http://docs.mosek.com/8.1/ toolbox/index.html
[108] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge University Press, 2004.
[109] D. Henrion and J. Malick, “Projection methods in conic optimization,” in Handbook on Semideﬁnite, Conic and Polynomial Optimization. Springer, 2012, pp. 565–600.
[110] H. H. Bauschke and J. M. Borwein, “On projection algorithms for solving convex feasibility problems,” SIAM review, vol. 38, no. 3, pp. 367–426, 1996.
[111] N. J. Higham, “Computing a nearest symmetric positive semideﬁnite matrix,” Linear algebra and its applications, vol. 103, pp. 103–118, 1988.
[112] P. L. Combettes and J.-C. Pesquet, “Proximal splitting methods in signal processing,” in Fixed-point algorithms for inverse problems in science and engineering. Springer, 2011, pp. 185–212.
[113] S. Jegelka, F. Bach, and S. Sra, “Reﬂection methods for user-friendly submodular optimization,” in Advances in Neural Information Processing Systems (NIPS), 2013, pp. 1313–1321.
[114] OpenMP Architecture Review Board, “OpenMP application program interface version 3.0,” May 2008. [Online]. Available: http://www. openmp.org/mp-documents/spec30.pdf
[115] R. A. Rossi, D. F. Gleich, and A. H. Gebremedhin, “Parallel maximum clique algorithms with applications to network analysis,” SIAM Journal on Scientiﬁc Computing, vol. 37, no. 5, pp. C589–C616, 2015.
[116] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng, “Ros: an open-source robot operating system,” in ICRA workshop on open source software, vol. 3, no. 3.2. Kobe, Japan, 2009, p. 5.
[117] M. Grant and S. Boyd, “CVX: Matlab software for disciplined convex programming.” [Online]. Available: http://cvxr.com/cvx
[118] P. Speciale, D. P. Paudel, M. R. Oswald, T. Kroeger, L. V. Gool, and M. Pollefeys, “Consensus maximization with linear matrix inequality

constraints,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 5048–5056. [119] R. Hartley, J. Trumpf, Y. Dai, and H. Li, “Rotation averaging,” IJCV, vol. 103, no. 3, pp. 267–305, 2013. [120] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. Leonard, “Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age,” IEEE Trans. Robotics, vol. 32, no. 6, pp. 1309–1332, 2016, arxiv preprint: 1606.05830, (pdf). [121] Q.-Y. Zhou, J. Park, and V. Koltun, “Open3D: A modern library for 3D data processing,” arXiv:1801.09847, 2018. [122] S. Jegelka and J. Bilmes, “Submodularity beyond submodular energies: Coupling edges in graph cuts,” in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE, 2011, pp. 1897–1904. [123] J.-B. Lasserre, Moments, positive polynomials and their applications. World Scientiﬁc, 2010, vol. 1.
Heng Yang is a PhD candidate in the Department of Mechanical Engineering and the Laboratory for Information & Decision Systems (LIDS) at the Massachusetts Institute of Technology, where he is working with Prof. Luca Carlone at the SPARK Lab. He has obtained a B.S. degree in Mechanical Engineering (with honors) from the Tsinghua University, Beijing, China, in 2015; and an S.M. degree in Mechanical Engineering from the Massachusetts Institute of Technology in 2017. His research interests include convex optimization, semideﬁnite and sums-of-squares relaxation, robust estimation and machine learning, applied to robotic perception and computer vision. His work includes developing robust and certiﬁable algorithms for geometric understanding. Heng Yang is a recipient of the Best Paper Award in Robot Vision at the 2020 IEEE International Conference on Robotics and Automation (ICRA).
Jingnan Shi is a Master student in the Department of Aeronautics and Astronautics at the Massachusetts Institute of Technology. He is currently a member of the SPARK lab, led by Professor Luca Carlone. He has obtained a B.S. degree in Engineering from Harvey Mudd College in 2019. His research interests include robust perception and estimation, with applications speciﬁc to various robotic systems.
Luca Carlone is the Leonardo CD Assistant Professor in the Department of Aeronautics and Astronautics at the Massachusetts Institute of Technology, and a Principal Investigator in the Laboratory for Information & Decision Systems (LIDS). He joined LIDS as a postdoctoral associate (2015) and later as a Research Scientist (2016), after spending two years as a postdoctoral fellow at the Georgia Institute of Technology (2013-2015). He has obtained a B.S. degree in mechatronics from the Polytechnic University of Turin, Italy, in 2006; an S.M. degree in mechatronics from the Polytechnic University of Turin, Italy, in 2008; an S.M. degree in automation engineering from the Polytechnic University of Milan, Italy, in 2008; and a Ph.D. degree in robotics also from the Polytechnic University of Turin in 2012. His research interests include nonlinear estimation, numerical and distributed optimization, and probabilistic inference, applied to sensing, perception, and decision-making in single and multi-robot systems. His work includes seminal results on certiﬁably correct algorithms for localization and mapping, as well as approaches for visualinertial navigation and distributed mapping. He is a recipient of the 2020 RSS Early Career Award, the 2017 Transactions on Robotics King-Sun Fu Memorial Best Paper Award, the Best Paper Award in Robot Vision at ICRA 2020, the Best Paper award at WAFR 2016, and the Best Student Paper award at the 2018 Symposium on VLSI Circuits.

21

APPENDICES
A. Manifesto of Certiﬁable Perception
This section provides a broader context for the algorithms developed in this paper, and motivates our efforts towards perception algorithms with formal performance guarantees.
What is a certiﬁable algorithm? Outlier-robust estimation is a hard combinatorial problem, where one has to separate inliers from outliers while computing an estimate for the variables of interest. Many algorithms, such as RANSAC and ICP, are heuristic solvers for such a combinatorial problem [61]. These algorithms are not certiﬁable (formalized later) in the sense that the estimate they return can be arbitrarily far from the optimal solution of the combinatorial problem, and there is no easy way to check how suboptimal an estimate is.
Then a natural question is: can we directly compute an optimal solution for outlier-robust estimation or develop algorithms that always compute near-optimal solutions? Unfortunately, in general the answer is no. A recent set of papers [65], [66] has shown that a broad family of robust estimation problems (including Consensus Maximization [61] and the Truncated Least Squares formulation considered in this paper) are inapproximable in polynomial-time: there exist worst-case instances in which no polynomial-time algorithm can compute a near-optimal solution (see [66] for a more formal treatment and [65] for even more pessimistic inapproximability results ruling our quasi-polynomial-time algorithms).
This fundamental intractability calls for a paradigm shift: since it is not possible to solve every robust estimation problem in polynomial time, we claim that a useful goal is to design algorithms that perform well in typical instances and are able to certify the correctness of the resulting estimates, but at the same time can detect worst-case instances and declare “failure” on those rather than blindly returning an invalid estimate.
We formalize this notion in the deﬁnition below.
Deﬁnition 19 (Certiﬁable Algorithms). Given an optimization problem P(D) that depends on input data D, we say that an algorithm A is certiﬁable if, after solving P(D), algorithm A either provides a certiﬁcate for the quality of its solution (e.g., a proof of optimality, a ﬁnite bound on its sub-optimality, or a ﬁnite bound on the distance of the estimate from the optimal solution), or declares failure otherwise.
The notion of certiﬁable algorithms is inspired by (and is indeed a particularization of) the notion of Probably Certiﬁably Correct (PCC) Algorithm introduced by Bandeira in [21]. We observe that a certiﬁable algorithm is sound (the algorithm does not certify incorrect solutions), but is not necessarily complete (the algorithm may declare failure in a problem that can be solved in polynomial time by a different algorithm). In this paper, we provide the ﬁrst certiﬁable algorithm for registration (and for rotation search, one of the subproblems we encounter) and what we believe are the ﬁrst certiﬁable algorithms for outlier-robust estimation in robotics and vision. We sometimes refer to the algorithms in Deﬁnition 19 as certiﬁably robust to stress that in outlier-robust estimation the notion of optimality has implications on the robustness to outliers of the resulting estimate. However, the deﬁnition

also applies to hard outlier-free problems such as [22], [23]. When applied to estimation problems in robotics or vision,
we note that an optimal solution is not necessarily a “good” solution (i.e., close to the ground truth for the quantity we want to estimate). For instance, if the data D we feed to the algorithm is completely random and uncorrelated with our unknown, solving P(D) would not bring us any closer to knowing the value of the unknown. In this sense, any meaningful performance guarantee will need to take assumptions on the generative model of the data. We refer to the corresponding theoretical results as “estimation contracts”, which, informally, say that as long as the data is informative about the ground truth, a certiﬁable algorithm can produce an estimate close to the ground truth. In other words, while a certiﬁable algorithm is one that can assess if it completed the assigned task to optimality, an estimation contract ensures that optimality is useful towards estimating a variable of interest.
Why certiﬁable algorithms? Besides being an intellectual pursuit towards the design of better algorithms for robot perception, we believe that the current adoption of algorithms that can fail without notice is a major cause of brittleness in modern robotics applications, ranging from self-driving cars to autonomous drones. This is true across spatial perception applications, including object detection [24] and Simultaneous Localization and Mapping [120]. A certiﬁable algorithm can detect failures before they cascade to other modules in the autonomy stack. Moreover, we hope that the development of estimation contracts can pave the way towards formal veriﬁcation and monitoring of complex perception systems involving multiple modules and algorithms, establishing connections with parallel efforts on safe autonomy and decision-making, e.g., [25], [26].

B. Choice of βi and c¯
The parameters βi and c¯ are straightforward to set in (6). In the following we discuss how to set them depending on the assumptions we make on the inlier noise.

Remark 20 (Probabilistic inlier noise). If we assume the inliers follow the generative model (4) with i ∼ N (03, σi2I3) and oi = 0, it holds:

1 σi2

bi − Rai

2=

1 σi2

i 2∼ χ2(3),

(29)

where χ2(3) is the Chi-squared distribution with three degrees

of freedom. Therefore, with desired probability p, the weighted

error

1 σi2

i 2 for the inliers satisﬁes:

2

P

i
σi2

≤ γ2

= p,

(30)

where γ2 is the quantile of the χ2 distribution with three

degrees of freedom and lower tail probability equal to p (e.g.,

γ = 3 for p = 0.97). Therefore, one can simply set the noise

bound βi in Problem (6) to be βi = σi, set the c¯ = γ. As mentioned in Section IV, from optimization standpoint, this

is equivalent to setting βi = γσi and setting c¯ = 1. The parameter γ monotonically increases with p; therefore, setting

p close to 1 makes the formulation (6) more prone to accept

22

measurements with large residuals, while a small p makes (6) more selective.

Remark 21 (Set membership inlier noise). If we assume
the inliers follow the generative model (4) with unknown-butbounded noise i ≤ βi, where βi is a given noise bound,9 it is easy to see that the inliers satisfy:

i ≤ βi ⇐⇒ bi − Rai 2≤ βi2

⇐⇒

1 βi2

bi − Rai

2≤ 1

(31)

Therefore, we directly plug βi into (6) and choose c¯ = 1.

C. Truncated Least Squares vs. Consensus Maximization
Truncated Least Squares (and Algorithm 2) are related to Consensus Maximization (MC), a popular approach for outlier rejection in vision [118], [28]. Consensus Maximization looks for an estimate that maximizes the number of inliers (or, equivalently, minimizes the number of outliers):

min |O|, s.t. ri(x) 2≤ c¯2 ∀ i ∈ M \ O (32)
O⊆M x∈X
where x is the variable we want to estimate (possibly belonging to some domain X ), M is the available set of measurements, ri(·) is a given residual error function, c¯ is the maximum admissible error for an inlier, and |·| denotes the cardinality of a set. Problem (32) looks for the smallest set of outliers (O) such that all the other measurements (i.e., the inliers M \ O) have residual error below c¯ for some x. While Consensus Maximization is intractable in general, by following the same lines of Theorem 7, it is easy to show it can be solved in polynomial time in the scalar case.
MC and TLS do not return the same solution in general, since TLS may prefer discarding measurements that induce a large bias in the estimate, as shown by the simple example below.

Example 22 (TLS = MC). Consider a simple scalar estimation problem, where we are given three measurements, and compare the two formulations:

min min (s − sk)2 , c¯2
s k∈M
min |O| s.t. (s − sk)2 ≤ c¯2 ∀k ∈ M \ O
O⊆M,s

(TLS) (33) (MC) (34)

where M = {1, 2, 3}. Assume s1 = s2 = 0, s3 = 3, and c¯ = 2. Then, it is possible to see that sMC = 1.5 attains a maximum consensus set including all measurements {1, 2, 3}, while the TLS estimate is sˆ = 0 which attains an optimal cost equal to 2, and has consensus set I(sˆ) = {1, 2}.

Note that Consensus Maximization is not necessarily preferable over TLS: indeed (i) a human might have arguably preferred to ﬂag s3 = 3 as an outlier in the toy example above, and more importantly (ii) in the experiments in Section XI-A we observe that often Consensus Maximization has a lower breakdown point than TLS (intuitively, TLS is more sensitive to the distribution of the inliers, since it penalizes their residuals).

9This is the typical setup assumed in set membership estimation [29].

While MC and TLS do not choose the same set of inliers in general, in cases where there is a large set of inliers, we expect TLS and MC to produce similar solutions, as shown below.
Lemma 23 (Necessary Conditions for TLS ≡ MC). Assume the maximum consensus set returned by Consensus Maximization (MC) has size NiMnC and the sum of the squared residual errors for the inliers is riMnC. If the second largest consensus set has size smaller than NiMnC − riMnC/c¯2, then TLS also selects the maximum consensus set as inliers.
Proof: The lemma establishes a general condition under which the set computed by (32) matches the set of inliers found by the TLS formulation:

min min ri(x) 2, c¯2

(35)

x∈X

i∈M

For the TLS formulation, the inliers are the measurements that

–at the optimal solution– have residual smaller than c¯. We

denote as fTLS(I) the value of the TLS cost in (35) for a

given choice of the consensus set I.

If

Denote the size of the OMC is the optimal

measurement set as N solution of (32), we

, i.e., N deﬁne

N=.oMu|CM t =|..

|OMC|. Lemma 23 denotes Consensus Maximization as

tNheiMnnCum=. b|eMr o\f

inliers found by O|= N − NoMuCt .

Moreover, Lemma 23 denotes the sum of the squared residual

errors for the inliers as:

riMnC =.

ri(x) 2

i∈M\OMC

(36)

Then the lemma claims that if the size of the second largest consensus set is smaller than NiMnC − riMnC/c¯2, TLS returns the maximum consensus set as inliers.
Assume by contradiction that the maximum consensus set IMC leads to a suboptimal TLS solution. Then, there exists another TLS solution with consensus set I such that:

fTLS(I ) < NoMuCt c¯2 + riMnC

(37)

which follows from the assumption that the solution corresponding to IMC (attaining cost NoMuCt c¯2 + riMnC) is suboptimal.
Now, if we call Nin = |I | and deﬁne Nout = N − Nin, the assumption that any consensus set other than IMC has size smaller than NiMnC − riMnC/c¯2 implies:

Nin < NiMn C − riMnC/c¯2 ⇐⇒

N − Nout < N − NoMuCt − riMnC/c¯2 ⇐⇒

Nout > NoMuCt + riMnC/c¯2

(38)

Using (38) and recalling that the residual error for the inliers is nonnegative (it is a sum of squares):

fTLS(I ) ≥ Noutc¯2 > (NoMuCt + riMnC/c¯2)c¯2 = NoMuCt c¯2 + riMnC (39)
We conclude the lemma by observing that (37) and (39) cannot be simultaneously satisﬁed, leading to contradiction.

23

Measurements Symbol
Deﬁnition
Generative model Noise bounds
Dependent transformations Number

Points ai, bi
-
bi = sRai + t + oi + i i ≤ βi
(s, R, t) N

TIMs a¯ij , b¯ij

a¯ij = aj − ai

b¯ij = b¯j − b¯i

b¯ij

=
ij

sRa¯ ij ≤ δij

+ =.

oij + ij βi + βj

(s, R)

K

≤

N (N −1) 2

TABLE III SUMMARY OF INVARIANT MEASUREMENTS.

TRIMs sij

sij =

b¯ij a¯ ij

|

sisji|j≤=αisj+=. oδsijij+/

s ij
a¯ ij

s

K

D. Proof of Theorem 4: Translation Invariant Measurements
Using the vector notation a ∈ R3N and b ∈ R3N already introduced in the statement of the theorem, we can write the generative model (4) compactly as:

b = s(IN ⊗ R)a + (1N ⊗ t) + o +

(40)

where o =. [oT1 . . . oTN ]T,

=.

[

T 1

...

T N

]T

,

and

1N

is

a column vector of ones of size N . Denote K = |E| as the

cadinality of E, such that A ∈ RK×N is the incidence matrix

of the graph with edges E. Let us now multiply both members

by (A ⊗ I3):

b¯ = (A ⊗ I3)[s(IN ⊗ R)a + (1N ⊗ t) + (o + )] (41)

Using the property of the Kronecker product we simplify:

(i) (A ⊗ I3)(IN ⊗ R)a = (A ⊗ R)a

= (IK ⊗ R)(A ⊗ I3)a = (IK ⊗ R)a¯

(42)

(ii) (A ⊗ I3)(1N ⊗ t) = (A1N ⊗ t) = 0

where we used the fact that 1N is in the Null space of the incidence matrix A [96]. Using (i) and (ii), eq. (41) becomes:

b¯ = s(IK ⊗ R)a¯ + (A ⊗ I3)(o + )

(43)

which is invariant to the translation t, concluding the proof.

E. Summary of Invariant Measurements
Table III provides a summary of the invariant measurements introduced in Section V.

F. Proof of Theorem 6: Max Clique Inlier Selection
Consider a graph G (V, E ) whose edges where selected as inliers during scale estimation, by pruning the complete graph G(V, E). An edge (i, j) (and the corresponding TIM) is an inlier if both i and j are correct correspondences (see discussion before Theorem 4). Therefore, G contains edges connecting all points for which we have inlier correspondences. Therefore, these points are vertices of a clique in the graph G and the edges (or equivalently the TIMs) connecting those points form a clique in G . We conclude the proof by observing that the clique formed by the inliers has to belong to at least one maximal clique of G .

G. Proof of Theorem 7: Optimal Scalar TLS Estimation

Let us ﬁrst prove that there are at most 2K −1 different non-

empty consensus sets. We attach a conﬁdence interval [sk −

αkc¯, sk +αkc¯] to each measurement sk, ∀k ∈ {1, . . . , K}. For

a given scalar s ∈ R, a measurement k is in the consensus set

of s if s ∈ [sk − αkc¯, sk + αkc¯] (satisﬁes

s−sk α2k

2

≤ c¯2),

see

Fig. 3(a). Therefore, the only points on the real line where the

consensus set may change are the boundaries (shown in red in

Fig. 3(a)) of the intervals [sk −αkc¯, sk +αkc¯], k ∈ {1, . . . , K}.

Since there are at most 2K −1 such intervals, there are at most

2K − 1 non-empty consensus sets (Fig. 3(b)), concluding the

ﬁrst part of the proof. The second part follows from the fact

that the consensus set of sˆ is necessarily one of the 2K − 1

possible consensus sets, and problem (11) simply computes the

least squares estimate of the measurements for every possible

consensus set (at most 2K − 1) and chooses the estimate that

induces the lowest cost as the optimal estimate.

H. Proof of Proposition 9: Binary Cloning

Here we prove the equivalence between the mixed-integer

program (17) and the optimization in (18) involving N + 1

quaternions. To do so, we note that since θk ∈ {+1, −1} and

1+θk 2

∈ {0, 1}, we

can

safely

move

1+θk 2

inside

the

squared

norm (because 0 = 02, 1 = 12) in each summand of the cost

function (17):

K 1+θk k=1 2

bˆk −q◦aˆ k ◦q−1 δk2

2

+

1−θk 2

c¯2

(44)

=

+ c¯ K

bˆk−q◦aˆk◦q−1+θkbˆk−q◦aˆk◦(θkq−1) 2

k=1

4δk2

1−θk 2 2

Now we introduce N new unit quaternions qk = θkq, k = 1, . . . , N by multiplying q by the N binary variables

θk ∈ {+1, −1}, a re-parametrization we called binary cloning. One can easily verify that qTqk = θk(qTq) = θk. Hence, by substituting θk = qTqk into (44), we can rewrite the mixed-
integer program (17) as:

min
q∈S3 qk ∈{±q}

K

bˆk−q◦aˆk◦q−1+qTqkbˆk−q◦aˆk◦qk−1 2

k=1

4δk2

+

1−qT 2

qk

c¯2

,

(45)

which is the same as the optimization in (18).

I. Proof of Proposition 10: Binary Cloning as a QCQP
Here we show that the optimization involving K + 1 quaternions in (18) can be reformulated as the QuadraticallyConstrained Quadratic Program (QCQP) in (19). Towards this

24

goal, we prove that the objective function and the constraints

in the QCQP are a re-parametrization of the ones in (18).

Equivalence of the objective functions. We start by de-

veloping sˆa¯k, i.e.,

the a¯ k

=s.qsˆua¯arkedfo2r -nnootramtiotnersmimipnlic(1it8y)):(we

scale

a¯ k

by

bˆk − q ◦ aˆk ◦ q−1 + qTqkbˆk − q ◦ aˆk ◦ qk−1 2 ( qTqkbˆk 2= bˆk 2= b¯k 2, bˆTk (qTqk)bˆk = qTqk b¯k 2)
( q ◦ aˆk ◦ q−1 2= Ra¯k 2= a¯k 2)
( q ◦ aˆk ◦ qk−1 2= θkRa¯k 2= a¯k 2)

( ) (q◦aˆk ◦q−1)T(q◦aˆk ◦qk−1)=(Ra¯k )T(θk Ra¯k )=qTqk a¯k 2 = 2 b¯k 2+2 a¯k 2+2qTqk b¯k 2+2qTqk a¯k 2
−2bˆTk (q ◦ aˆk ◦ q−1) − 2bˆTk (q ◦ aˆk ◦ qk−1) −2qTqkbˆTk (q ◦ aˆk ◦ q−1) − 2qTqkbˆTk (q ◦ aˆk ◦ qk−1)

(46)

( ) qTqk bˆTk (q◦aˆk ◦qk−1)=(θk )2bˆTk (q◦aˆk ◦q−1)=bˆTk (q◦aˆk ◦q−1) ( bˆTk (q ◦ aˆk ◦ qk−1) = qTqkbˆTk (q ◦ aˆk ◦ q−1) )
= 2 b¯k 2+2 a¯k 2+2qTqk b¯k 2+2qTqk a¯k 2 −4bˆTk (q ◦ aˆk ◦ q−1) − 4qTqkbˆTk (q ◦ aˆk ◦ q−1)

(47)

where we have used multiple times the binary cloning equalities qk = θkq, θk = qTqk, the equivalence between applying rotation to a homogeneous vector aˆk using quaternion product and using rotation matrix in eq. (3) from the main document,
as well as the fact that vector 2-norm is invariant to rotation
and homogenization (with zero padding).
Before moving to the next step, we make the following observation by combining eq. (2) and q−1 = [−vT, s]T:

Ω1(q−1) = ΩT1 (q), Ω2(q−1) = ΩT2 (q)

(48)

which states the linear operators Ω1(·) and Ω2(·) of q and its inverse q−1 are related by a simple transpose operation.

In the next step, we use the equivalence between quaternion

product and linear operators in Ω1(q) and Ω2(q) as deﬁned in eq. (1)-(2) to simplify bˆTk (q ◦ aˆk ◦ q−1) in eq. (47):

bˆTk (q ◦ aˆk ◦ q−1)

( ) q◦aˆk=Ω1(q)aˆk , Ω1(q)aˆk◦q−1=Ω2(q−1)Ω1(q)aˆk=ΩT2 (q)Ω1(q)aˆk

= bˆTk (ΩT2 (q)Ω1(q)aˆk)

(49)

(Ω2(q)bˆk = bˆk ◦ q = Ω1(bˆk)q , Ω1(q)aˆk = q ◦ aˆk = Ω2(aˆk)q)

= qTΩT1 (bˆk)Ω2(aˆk)q.

(50)

Now we can insert eq. (50) back to eq. (47) and write:

bˆk − q ◦ aˆk ◦ q−1 + qTqkbˆk − q ◦ aˆk ◦ qk−1 2 = 2 b¯k 2+2 a¯k 2+2qTqk b¯k 2+2qTqk a¯k 2 −4bˆTk (q ◦ aˆk ◦ q−1) − 4qTqkbˆTk (q ◦ aˆk ◦ q−1) (51) = 2 b¯k 2+2 a¯k 2+2qTqk b¯k 2+2qTqk a¯k 2 −4qTΩT1 (bˆk)Ω2(aˆk)q − 4qTqkqTΩT1 (bˆk)Ω2(aˆk)q (52)
( ) qTqk qTΩT1 (bˆk )Ω2(aˆk )q=θk qTΩT1 (bˆk )Ω2(aˆk )q=qTΩT1 (bˆk )Ω2(aˆk )qk = 2 b¯k 2+2 a¯k 2+2qTqk b¯k 2+2qTqk a¯k 2
−4qTΩT1 (bˆk)Ω2(aˆk)q − 4qTΩT1 (bˆk)Ω2(aˆk)qk (53) (−ΩT1 (bˆk) = Ω1(bˆk))
= 2 b¯k 2+2 a¯k 2+2qTqk b¯k 2+2qTqk a¯k 2 +4qTΩ1(bˆk)Ω2(aˆk)q + 4qTΩ1(bˆk)Ω2(aˆk)qk, (54)

which is quadratic in q and qk. Substituting eq. (54) back to (18), we can write the cost function as:

K
k=1


+ c¯ bˆk−q◦aˆk◦q−1+qTqkbˆk−q◦aˆk◦qk−1 2
4δk2

1−qTqk 2 2



=

K

qkT

  

(

k=1 

b¯k

2+

a¯ k

2)I4 + 2δk2

2Ω1 (bˆk )Ω2 (aˆ k )

+

c¯2 2

 I4


qk

:=Qkk





+2qT

 

(





b¯k

2+

a¯ k

2)I4 + 4δk2

2Ω1 (bˆk )Ω2 (aˆ k )

−

c¯2 4



I4

 



qk ,

:=Q0k

(55)

where we have used two facts: (i) qTAq = θk2qTAq = qkTAqk for any matrix A ∈ R4×4, (kk) c = cqTq = qT(cI4)q for any real constant c, which allowed writing the quadratic
forms of q and constant terms in the cost as quadratic forms
of qk. Since we have not changed the decision variables q and {qk}Kk=1, the optimization in (18) is therefore equivalent to the following optimization:

K

min
q∈S3 qk ∈{±q}

k=1

qkT Qkk qk

+

2q T Q0k qk

(56)

where Qkk and Q0k are the known 4 × 4 data matrices as deﬁned in eq. (55).
Now it remains to prove that the above optimization (56) is equivalent to the QCQP in (19). Recall that x is the column vector stacking all the K + 1 quaternions, i.e.,, x = [qT q1T . . . qNT ]T ∈ R4(K+1). Let us introduce symmetric matrices Qk ∈ R4(K+1)×4(K+1), k = 1, . . . , K and let the 4 × 4 sub-block of Qk corresponding to sub-vector u and v, be denoted as [Qk]uv; each Qk is deﬁned as:

 Qkk 
[Qk]uv = Q0k
04×4

if u = qk and v = qk
if u=q and v=qk or u=qk and v=q
otherwise

(57)

i.e., Qk has the diagonal 4 × 4 sub-block corresponding to (qk, qk) be Qkk, has the two off-diagonal 4 × 4 sub-blocks corresponding to (q, qk) and (qk, q) be Q0k, and has all the other 4 × 4 sub-blocks be zero. Then we can write the cost function in eq. (56) compactly using x and Qk:

K

K

qkTQkkqk + 2qTQ0kqk = xTQkx

(58)

k=1

k=1

Therefore, by denoting Q =

K k=1

Qk

,

we

proved

that

the

objective functions in (18) and the QCQP (19) are the same.

Equivalence of the constraints. We are only left to prove

that (18) and (19) have the same feasible set, i.e.,, the following

two sets of constraints are equivalent:

 q

∈

S3



xTq xq = 1

qk ∈ {±q},

⇔ xqk xTqk = xqxTq ,

(59)

k = 1, . . . , K

k = 1, . . . , K

25

We ﬁrst prove the (⇒) direction. Since q ∈ S3, it is obvious that xTq xq = qTq = 1. In addition, since qk ∈ {+q, −q}, it follows that xqk xTqk = qkqkT = qqT = xqxTq . Then we proof the reverse direction (⇐). Since xTq xq = qTq, so xTq xq = 1 implies qTq = 1 and therefore q ∈ S3. On the other hand, xqk xTqk = xqxTq means qkqkT = qqT. If we write qk = [qk1, qk2, qk3, qk4]T and q = [q1, q2, q3, q4], then the following matrix equality holds:

 qk21   

qk1 qk2 qk22

qk1 qk3 qk2 qk3
qk23

qk1qk4   q12

qk2 qk4 qk3 qk4

 

=

 



qk24

q1 q2 q22

q1 q3
q2 q3 q32

q1q4 

q2q4 

q3 q4

 

q42

(60)

First, from the diagonal equalities, we can get qki = θiqi, θi ∈ {+1, −1}, i = 1, 2, 3, 4. Then we look at the off-diagonal equality: qkiqkj = qiqj, i = j, since qki = θiqi and qkj = θjqj, we have qkiqkj = θiθjqjqk, from which we obtain θiθj = 1, ∀i = j. This implies that all the binary values {θi}4i=1 have the same sign, and therefore they are equal to each other. As a result, qk = θkq = {+q, −q}, showing the two sets of constraints in eq. (59) are indeed
equivalent. Therefore, the QCQP in eq. (19) is equivalent to
the optimization in (56), and the original optimization in (18) that involves K + 1 quaternions, concluding the proof.

J. Proof of Proposition 11: Matrix Formulation of Binary Cloning

Here we show that the non-convex QCQP written in terms of the vector x in Proposition 10 (and eq. (19)) is equivalent to the non-convex problem written using the matrix Z in Proposition 11 (and eq. (22)). We do so by showing that the objective function and the constraints in (22) are a reparametrization of the ones in (19).
Equivalence of the objective function. Since Z = xxT and using the cyclic property of the trace, we rewrite the objective in (19) as:

xTQx = tr QxxT = tr (QZ)

(61)

showing the equivalence of the objectives in (19) and (22).
Equivalence of the constraints. It is trivial to see that xTq xq = tr xqxTq = 1 is equivalent to tr ([Z]qq) = 1 by using the cyclic property of the trace operator and inspecting the structure of Z. In addition, xqk xTqk = xqxTq also directly maps to [Z]qkqk = [Z]qq for all i = 1, . . . , K. Lastly, requiring Z 0 and rank (Z) = 1 is equivalent to restricting Z to the form Z = xxT for some vector x ∈ R4(K+1).
Therefore, the constraint sets of eq. (19) and (22) are also
equivalent, concluding the proof.

K. Proof of Theorem 13: SDP Relaxation with Redundant Constraints and Global Optimality Guarantee
We ﬁrst restate the QCQP (19) and explicitly add the redundant constraints:

Lemma 24 (QCQP with Redundant Constraints). The QCQP (19) is equivalent to the following QCQP with redundant constraints:

min xTQx

(P)

x∈R4(K+1)

s.t. xTq xq = 1,

xqk xTqk = xqxTq , ∀k = 1, . . . , K,

xqi xTqj = (xqi xTqj )T, ∀0 ≤ i < j ≤ K,

where q0 =. q for notation simplicity.

The redundancy of the last set of constraints can be easily understood from:

xqi xTqj = (θiq)(θj qT) = (θj q)(θiqT) = (xqi xTqj )T. (62)

Since the original QCQP (19) is equivalent to the rankconstrained SDP (22), problem (P) is also equivalent to (22).
Now let us prove Theorem 13. While the SDP relaxation of (P) can be simply obtained by adding the redundant constraints and dropping the rank constraint in (22). Here we take a longer path, using Lagrangian duality, which is useful towards getting the guarantees stated in the theorem.

Lemma 25 (Dual of QCQP with Redundant Constraints). The Lagrangian dual problem of the QCQP with redundant constraints (P) is the following convex semideﬁnite program:

max µ

(D)

µ,Λ,W

s.t. (Q − µJ + Λ + W ) 0,

where J ∈ S4(K+1) is an all-zero matrix except the top-

left diagonal 4 × 4 block [J ]00 = I4; Λ ∈ S4(K+1) is

a block-diagonal matrix Λ = blkdiag(Λ00, Λ11, . . . , ΛKK ),

where each Λkk is a 4 × 4 symmetric matrix and the sum

K k=0

Λkk

=

0; and W

∈

S 4(K+1)

satisﬁes [W ]ij

=

0

for i = j and [W ]ij ∈ S4 (i.e., [W ]ij is a skew symmetric

matrix) for i = j.

The dual problem (D) is derived from the redundant QCQP (P) by following standard Lagrangian duality [108]. For any x that is feasible for (P), one can verify that:

xTJ x = 1, xTΛx = 0, xTW x = 0.

(63)

Lemma 26 (Dual of the Dual). The dual SDP of the dual problem (D) is the following SDP:

min tr (QZ)
Z0
s.t. tr ([Z]00) = 1 [Z]kk = [Z]00, ∀k = 1, . . . , K [Z]ij = [Z]Tij, ∀0 ≤ i < j ≤ K,

(DD)

which is the same as the SDP relaxation with redundant constraints (23).

Now we are ready to prove Theorem 13. First of all, we have the following weak duality:

fD = fDD ≤ fP ,

(64)

26

where fD = fDD (strong duality of the SDP pair) is due to

the fact that (DD) has a strictly feasible solution, by having

[Z ]00

=

[Z ]11

=

···

=

[Z ]K K

=

1 4

I4

and

[Z ]ij

=

0

for

i

=

j

(in

this

case

Z

=

1 4

I4(K

+1)

0), and fDD ≤ fP is

due to the fact that (DD) is a convex relaxation for (P).

If the optimal solution of (DD) satisﬁes rank (Z ) = 1, then

this means that Z is also feasible for the rank-constrained

SDP (22). Therefore, the minimum fDD can indeed be attained by a point in the feasible set of problem (22). Because the

rank-constrained problem (22) is equivalent to (P), fDD = fP must hold and the rank-1 decomposition of Z , x must be a

global minimizer of problem (P).

To prove the uniqueness of ±x as global minimizers of (P), denote µ = (x )TQ(x ) = tr (QZ ) as the global minimum

of (P), (D) and (DD), and Λ , W as the corresponding

dual variables. Let M = Q − µ J + Λ + W , we have

rank (M ) = 4(K + 1) − 1 and its kernel is ker(M ) =

{x : x = ax } from complementary slackness of the pair of

SDP duals. Denote the feasible set of (P) as Ω(P), then we have

ker(M )∩Ω(P) = ±x . Therefore, for any x ∈ Ω(P)/{±x },

we have:

xTM x > 0

(65)

⇔ xTQx − µ + xTΛ x + xTW x > 0

(66)

⇔ xTQx > µ ,

(67)

where xTΛ x = xTW x = 0 holds due to (63).

and H is deﬁned as:



K



H

=.

 
∆

∈

S 4(N +1)

[∆]kk = 0,

 
, (71)

k=1

 

[∆]ij

∈

S4,

∀0

≤

i

<

j

≤

 K.

The matrix J ∈ S4(K+1) is an all-zero matrix except the topleft 4 × 4 block [J ]00 = I4 (the same deﬁned in Appendix K).

Proof: Because M −Q+µˆJ ∈ H, and from the deﬁnition of H, we can write M as:

M = Q − µˆJ + Λ + W ,

(72)

where Λ, W ∈ S4(K+1) are the dual variables deﬁned in Lemma 25. From M 0, we have the following inequality for any x that is primal feasible for (P):

xTM x ≥ 0 ⇔ xT(Q − µˆJ + Λ + W )x ≥ 0

(73)

⇔ xTQx ≥ µˆ + xTΛx + xTW x = µˆ,

(74)

which states that µˆ (resp. xˆ) is the global minimum (resp. minimizer) of (P).
In addition, denote Zˆ = xˆxˆT, then Zˆ and (µˆ, Λ, W ) are a pair of primal-dual optimal solutions for (DD) and (D), which leads to the following SDP complementary slackness condition:

(Q − µˆJ + Λ + W )Zˆ = 0

(75)

⇔ (Q − µˆJ + Λ + W )xˆ = 0.

(76)

L. Proof of Theorem 14: Optimality Certiﬁcation
We prove Theorem 14 in three steps. First, we show that that proving global optimality of an estimate for the rotation estimation problem (15) relates to the existence of a matrix at the intersection of the positive semideﬁnite (PSD) cone and an afﬁne subspace (Appendix L1). Second, we show that we can compute a sub-optimality gap (this is the bound η in the theorem) for an estimate using any matrix in the afﬁne subspace (Appendix L2). Finally, we show that under the conditions of Theorem 14 such bound converges to zero when the provided estimate is globally optimal (Appendix L3).
1) Matrix Certiﬁcate for Global Optimality: Here we derive necessary and sufﬁcient conditions for global optimality of the TLS rotation estimation problem (15).
Theorem 27 (Sufﬁcient Condition for Global Optimality). Given a feasible solution (qˆ, θˆ1, . . . , θˆK ) of the TLS rotation estimation problem (15), denote xˆ = [qˆT, θˆ1qˆT, . . . , θˆK qˆT]T, and µˆ = xˆTQxˆ, as the corresponding solution and cost of the QCQP (P). Then µˆ (resp. (qˆ, θˆ1, . . . , θˆK )) is the global minimum (resp. minimizer) of problem (15) if there exists a matrix M that satisﬁes the following:

M ∈ L,

(68)

M 0,

(69)

where L is the following afﬁne subspace:

L =.

M

∈

S 4(K+1)

M

M xˆ = 0, − Q + µˆJ ∈

H.

(70)

Therefore, M xˆ = 0 must hold in eq. (68).

Corollary 28 (Sufﬁcient and Necessary Condition for
Global Optimality). If strong duality is achieved, i.e., fD = fDD = fP is attained, then the existence of a matrix M that satisﬁes eq. (68) and (69) is also a necessary condition for µˆ (resp. (qˆ, θˆ1, . . . , θˆK )) to be the global minimum (resp.
minimizer) of problem (15).

Proof: If strong duality is achieved, then there must exist dual variables Λ and W that satisfy Q − µˆJ + Λ + W 0. The rest follows from the proof of Theorem 27.
2) Sub-optimality Bound: Theorem 27 suggests that ﬁnding a matrix M that lies at the intersection of the PSD cone S+ and the afﬁne subspace L gives a certiﬁcate for xˆ to be globally optimal for (P). Corollary 29 below states that even if xˆ is not a globally optimal solution, ﬁnding any M ∈ L (not necessarily PSD) still gives a valid sub-optimality bound using the minimum eigenvalue of M .10

Corollary 29 (Relative Sub-optimality Bound). Given a feasible solution (qˆ, θˆ1, . . . , θˆK ) of the TLS rotation estimation problem (15), denote xˆ = [qˆT, θˆ1qˆT, . . . , θˆK qˆT]T, and µˆ = xˆTQxˆ, as the corresponding solution and cost of the
QCQP (P). Deﬁne the afﬁne subspace L as in (70). For any
M ∈ L, let λ1(M ) be its minimum eigenvalue. Then the relative sub-optimality gap of xˆ is bounded by:

µˆ − µ

≤

η

=.

|λ1(M )|(K

+

1) ,

(77)

µˆ

µˆ

10Note that the expression of L depends on the estimate xˆ as per (70).

27

where µ is the true global minimum of (P).

Proof: For any x that is primal feasible for (P), we have

x 2=

K k=1

qk

2=

K + 1.

Therefore,

for

any

x

in

the

feasible set of (P) and M ∈ L, the following holds:

xTM x ≥ λ1(M ) x 2= λ1(M )(K + 1) (78) ⇔(i) xT(Q − µˆJ + Λ + W )x ≥ λ1(M )(K + 1) (79)
(⇔ii) xTQx ≥ µˆ + λ1(M )(K + 1). (80)

where (i) follows from the fact that any M ∈ L can be written as Q − µˆJ + Λ + W (see eq. (72)), and (ii) follows from (63). Since µ is also achieved in the feasible set, eq. (80) also holds for µ :

Therefore, to certify optimality of an estimate, we have to search for a matrix M¯ ∈ L¯ ∩ S+, i.e., at the intersection between two convex sets. Although ﬁnding a point in the intersection of two convex sets can be expensive (e.g., ﬁnding M¯ ∈ L¯ ∩ S+ is equivalent to solving a feasibility SDP), the following lemma states that one can ﬁnd a point in the intersection of two convex sets by cleverly leveraging the individual projections onto the two convex sets, a method that is commonly known as Douglas-Rachford Splitting (DRS) [112], [35], [122].
Lemma 31 (Douglas-Rachford Splitting). Let X1, X2 be two closed convex sets in the Hilbert space X , and ΠX1 , ΠX2 be their corresponding projection maps, then for any x0 ∈ X , the following iterates:

µ ≥ µˆ + λ1(M )(K + 1)

(81)

µˆ − µ ⇔

≤

|λ1(M )|(K

+

1) ,

(82)

µˆ

µˆ

where in eq. (82) we have used λ1(M ) ≤ 0 since M has a zero eigenvalue with eigenvector xˆ (M xˆ = 0 in eq. (68)).
3) Douglas-Rachford Splitting and Algorithm 3: Theo-
rems 27-28 relate global optimality to the existence of a matrix M ∈ S+ ∩ L, while Theorem 29 obtains a suboptimality bound from any matrix M ∈ L. The next Theorem states that ﬁnding a matrix M ∈ S+ ∩ L is equivalent to ﬁnding a matrix M¯ ∈ S+ ∩ L¯, where L¯ is a “rotated” version of L (this will enable faster projections to this subspace).

(i) xXτ 1 = ΠX1 (xτ ),

(86)

(ii) xXτ 2 = ΠX2 (2xXτ 1 − xτ ),

(87)

(iii) xτ+1 = xτ + γ(xXτ 2 − xXτ 1 ),

(88)

generates a sequence {xτ }τ≥0 that converges to the intersection of X1 and X2 when 0 < γ < 2, provided that the intersection is non-empty.

Therefore, the DRS (line 11-line 13) in Algorithm 3 guarantee to converge to a matrix M¯ ∈ L¯ ∩ S+ when L¯ ∩ S+ = ∅, and the sub-optimality bound η converges to zero. Moreover,
Algorithm 3 produces a sub-optimality bound even if it fails
to certify global optimality, thanks to Corollary 29.

Theorem 30 (Rotated Afﬁne Subspace). Given a feasible

solution (qˆ, θˆ1, . . . , θˆK ) of the TLS rotation estimation prob-

lem (15), deﬁne the block-diagonal matrix Ωˆ q = Ω1(qˆ). Then a matrix M ∈ L ∩ S+ if and only

IK+1 if M¯

⊗=.

Ωˆ Tq M Ωˆ q ∈ L¯ ∩ S+ with L¯ deﬁned as:

M. Closed-form Projections
Although Lemma 31 is theoretically sound, in practice computing the projection onto an arbitrary convex set could be computationally expensive11. However, we will show that

L¯ =.

M¯

∈

S 4(K+1)

M¯

M¯ x¯ = 0, − Q¯ + µˆJ ∈

H.

,

both projections, ΠL¯ and ΠS+ in Algorithm 3 can be computed (83) efﬁciently in closed form.
The projection onto the PSD cone is presented in the

where x¯ = [1, θˆ1, . . . , θˆK ]T ⊗ e (with e = [0, 0, 0, 1]T) and following Lemma due to Higham [111].

Q¯ = Ωˆ Tq QΩˆ q. Moreover, M¯ and M produce the same relative sub-optimality bound in Theorem 29.

Lemma 32 (Projection onto S+n). Given any matrix M ∈ Sn, let M = U diag (λ1, . . . , λn) U T be its spectral decomposi-

Proof: Using the expression of Ω1(q) in (2), one can tion, then the projection of M onto the PSD cone S+n is:

verify that Ω1(q)Tq = e, Ωˆ Tq xˆ = x¯ and Ωˆ Tq Ωˆ q = Ωˆ qΩˆ Tq = I4(K+1). Therefore, we have the following equivalence:
M¯ x¯ = 0 ⇔ Ωˆ Tq M Ωˆ qΩˆ Tq xˆ = 0

ΠS+n (M ) = U diag (max(0, λ1), . . . , max(0, λn)) U T. (89)
Now let us focus on the afﬁne projection onto L¯. The expression of the projection ΠL¯ is given in Proposition 33

⇔ Ωˆ qM xˆ = 0 ⇔ M xˆ = 0.

(84) below. Before stating the result, let us introduce some notation. For two 4 × 4 matrices X ∈ S4 and Y ∈ S4, we partition X

In ∆¯

=a.ddΩˆitTqio∆n,Ωˆiqt

is ∈

easy to verify that ∆ ∈ H if and H, and Ωˆ Tq J Ωˆ q = J . Therefore,

M − Q − µˆJ ∈ H ⇔ M¯ − Q¯ − µˆJ ∈ H,

only if (85)

and Y as:

X=

Xm ∈ S3 (Xv )T

Xv ∈ R3 Xs ∈ R

,Y =

Y m ∈ S3 Y v ∈ R3

−(Y v )T

Ys =0

, (90)

where the superscripts m, v, s denotes the top-left 3×3 matrix

and we conclude that M ∈ L ⇔ M¯ ∈ L¯. Finally, part, the top-right 3 × 1 vector part and the bottom-right scalar

M¯ = Ωˆ Tq M Ωˆ q = Ωˆ −q 1M Ωˆ q is a similarity transform on part, respectively. For a matrix M ∈ S4(K+1), we partition

M and shares the same eigenvalues with M , which trivially it into (K + 1)2 blocks of size 4 × 4, and we create a set of

leads to M ∈ S+ ⇔ M¯ ∈ S+. In addition, since M and M¯ share the same eigenvalues, we have λ1(M¯ ) = λ1(M ), which means that M¯ and M produces the same relative sub-

11Even for an afﬁne subspace, A = {x ∈ Rn : Ax = b}, the naive projection of any x onto A is: ΠA(x) = x − AT(AAT)−1(Ax − b),
which could be expensive for large n due to the computation of the inverse

optimality bound in Corollary 29.

(AAT)−1.

28

L

=.

K (K +1) 2

ordered

indices

(left

to

right,

top

to

bottom)

that

N. Proof of Theorem 15: Estimation Contract with Noiseless

enumerate the upper-triangular off-diagonal blocks:

Inliers and Random Outliers

Z

=

{(0,

1),

.

.

.

,

(0,

K ),

(1,

2),

.

.

.

,

(1,

K ),

.

.

.

,

(K

−

1,

K )},

(91)

We under

prove Theorem 15 in two assumptions (iii)-(iv) of the

steps. First we show that theorem, the inliers found

where each [M ]Z(l), l = 1, . . . , L, takes a 4 × 4 off-diagonal block from M . Then, we deﬁne a linear map C : S4(K+1) → RL×3 that assembles the top-right 3 × 1 vector part of each
upper-triangular off-diagonal block into a matrix:

T

C(M ) = [M ]vZ(1), . . . , [M ]vZ(L) .

(92)

Given a solution xˆ = [qˆT, θˆ1qˆT, . . . , θˆK qˆT]T to problem (P), denote Rˆ to be the unique rotation matrix corresponding to qˆ, and deﬁne the following residual vectors:

ξˆk = RˆT(b¯k − Rˆa¯k), k = 1, . . . , K,

(93)

for each pair of TIMs (a¯k, b¯k) (scale a¯k by: a¯k ← sˆa¯k).

by TEASER match the maximum consensus solution. Then, we
prove that under assumptions (i)-(ii) the maximum consensus
solution is unique and recovers the ground truth, from which
the claims of the theorem follow.
1) TLS ≡ MC: Let the number of inliers in the maximum consensus be NiMnC, since the inliers are noise-free, the sum of the squared residual errors in the maximum consensus set, riMnC, must be zero. By Lemma 23, if TLS selects a different consensus set than the maximum consensus set, then the consensus set that TLS selects must have size NiTnLS greater than NiMn C − riMnC/c¯2, i.e., NiTnLS > NiMn C − riMnC/c¯2 = NiMn C, contradicting the fact that NiMnC is the size of the maximum consensus set. Therefore, TLS ≡ MC for each subproblem.
2) Exact recovery of maximum consensus: We now prove

Proposition S4(K+1), its

33 (Projection projection onto

onto L¯). Given L¯, denoted M¯

=a.nyΠmL¯(aMtrix),Mis:

∈

that maximum consensus recovers the ground truth under the assumptions of the theorem. Let us start with the scale subproblem. The inliers are noiseless and distinct (assumption

M¯ = ΠH¯ (H) + Q¯ − µˆJ ,

(94)

where H =. M −Q¯ +µˆJ and H¯ =. ΠH¯ (H) can be computed as follows:

(i) in the theorem), so each pair of inliers will produce a TRIM sk = s◦. Since we have at least 3 inliers, there are at least 3 such TRIMs. Since the outliers are in generic position
(assumption (ii) in the theorem), the event that more than 3

1) Project the matrix part of each diagonal block:

TRIMs formed by the outliers will have the same scale estimate s¯ that is different from s◦ happens with probability zero.

[H¯ ]m kk = [H]m kk −

Kk=0[H]m kk , ∀k = 0, . . . , K. K +1

(95)

Therefore, the maximum consensus solution coincides with the ground-truth scale. The same logic can be repeated for

2) Project the scalar part of each diagonal block:

the rotation and translation subproblems, leading to identical conclusions under the assumption that the rotation subproblem

[H¯ ]skk =

−(

1 4

θˆk

+

1 2

)

ξˆk

2

+(

1 4

θˆk

−

1 2

)c¯2

K k=1

(

1 4

θˆk

+

1 2

)

ξˆk

2

−(

1 4

θˆk

−

1 2

)c¯2

k=0

ﬁnds a certiﬁably optimal TLS estimate (condition (iv)).

(96) 3) Exact recovery of TEASER: Since under the assumptions

k = 0.

of the theorem, TEASER matches the maximum consensus

3) Project the matrix part of each off-diagonal block:

solution in each subproblem, and maximum consensus recovers the ground truth in each subproblem with probability 1,

[H¯ ]m Z(l)

=

[H ]m Z(l)

− ([H]m Z(l))T , ∀l 2

=

1, . . . , L.

(97)

TEASER almost surely recovers the ground-truth transformation, i.e., sˆ = s◦, Rˆ = R◦, tˆ = t◦, proving the theorem.

4) Project the scalar part of each off-diagonal block:

[H¯ ]sZ(l) = 0, ∀l = 1, . . . , L.

(98)

5) Project the vector part of each off-diagonal block:

C(H¯ ) = P · F(H),

(99)

where P ∈ SL and F (H) ∈ RL×3 are deﬁned in Appendix U. 6) Project the vector part of each diagonal block using the results of step 5:

[H¯ ]vkk = −

K i=0,i=k

θˆkθˆi[H¯ ]vki

+

φk ,

(100)

φk

=

 −


1 2

θˆk

K

k=1

+1

1 2

θˆk

[ξˆk ]× a¯ k + 1 [ξˆk]×a¯k

k = 1, . . . , K
(.101) k=0

A complete proof of Proposition 33 is algebraically involved and is given in Appendix U.

O. Proof of Theorem 16: Estimation Contract with Noiseless
Inliers and Adversarial Outliers
As for Theorem 15, we prove Theorem 16 in two steps. 1) TLS ≡ MC: The proof of this part is identical to the
corresponding proof in Theorem 15.
2) Exact recovery of maximum consensus: Different than
the case with random outliers, we need condition (i), a stronger
assumption on the number of inliers to guarantee recovery
of the ground truth. Let the ground-truth consensus set be C◦ whose size is at least Nin (the outliers can accidentally become inliers) in condition (i). Suppose an adversary wants to trick MC to return a solution (s¯, R¯, t¯) that is different from the ground truth, s/he must have a consensus set, C¯, of size N¯in that is no smaller than Nin, in which the measurements are consistent w.r.t. (s¯, R¯, t¯). Now because the total number of measurement is Nin + Nout, and both C◦ and C¯ have size at least Nin, this means C◦ and C¯ must share at least 2Nin − (Nin + Nout) = Nin − Nout common correspondences.

29

From condition (i), we know that Nin −Nout ≥ 3. However, if C◦ and C¯ share at least 3 common noise-free correspondences, then (s¯, R¯, t¯) ≡ (s◦, R◦, t◦) must be true. Therefore, we conclude that there is no chance for the adversary to trick MC, and MC must recover the ground-truth transformation when condition (i) holds.
3) Exact recovery of TEASER: Condition (iv) guarantees that TEASER returned a consensus set that satisﬁes condition (i). However, according to Section O2 only the ground truth can produce an estimate that satisﬁes condition (i), hence TEASER’s solution must match the ground truth, proving the theorem. Note that recovering the ground truth does not guarantee that all outliers are rejected, since in the adversarial regime outliers can be indistinguishable from inliers (i.e., an outlier i can satisfy bi = s◦R◦ai + t◦), a fact that, however, does not have negative repercussions on the accuracy of TEASER.

P. Proof of Theorem 17: Estimation Contract with Noisy Inliers and Adversarial Outliers
We prove Theorem 17 in two steps. First we show that under assumptions (ii) and (iii) of the theorem, the inliers produced by TEASER match the maximum consensus set and contain the set of true inliers. Then, we prove that under assumption (i) and (iv) the maximum consensus solution is also close to the ground truth, in the sense of inequalities (26)-(28).
1) TLS ≡ MC and inliers are preserved: We prove that under assumptions (ii) and (iii), the inliers produced by TEASER match the maximum consensus set and contain the set of true inliers. In each subproblem, Lemma 23 and assumption (iii) guarantee that TEASER computes a maximum consensus set. Assumption (iii) implies the maximum consensus set contains the inliers (and hence TEASER preserves the inliers).
2) Noisy recovery of maximum consensus: We now prove that maximum consensus produces a solution “close” to the ground truth under the assumptions of the theorem. Let us start with the scale subproblem. Assumption (ii) in the theorem ensures that the maximum consensus set contains all inliers in each subproblem. Let us now derive the bounds on the resulting estimate, starting from the scale estimation.
Scale error bound. Since the scale estimation selects all the inliers, plus potentially some outliers, it holds for some of the selected measurements i, j:

|s◦ − sˆ|= |s◦ − sˆ +

s k

−

s k

|

(a)

≤ |s◦ +

s k

−

sˆ|+|

s k

|

(b)

(c)

≤ |sk − sˆ|+αij ≤ 2αij

(102) (103) (104)

where in (a) we used the triangle inequality, in (b) we noticed

that for inliers i, j, it holds |

s k

|≤

αij

and s◦ +

s k

= sk, and in

(c) we noticed that for sk to be considered an inlier by TLS,

|sk − sˆ|≤ αij. Since the inequality has to hold for all the true

inliers, but we do not know which ones of the measurements

selected by TLS are true inliers, we substitute the bound with:

|s◦ − sˆ|≤ 2 max αij
ij

(105)

which proves the ﬁrst bound in (26). Rotation error bound. Let us now move to rotation
estimation. Since the rotation subproblem also selects all the inliers, plus potentially some outliers, it holds for some of the selected measurements i, j:

s◦R◦a¯ij − sˆRˆ a¯ij = s◦R◦a¯ij − sˆRˆ a¯ij + ij − ij (106)

(=a) b¯ij − sˆRˆ a¯ij − ij (107)

(b)
≤

b¯ij − sˆRˆ a¯ij

+

(c)
ij ≤ 2δij(108)

where in (a) we noticed that for inliers i, j, b¯ij = s◦R◦a¯ij +

ij, in (b) we used the triangle inequality, and in (c) we noticed

that it holds ij ≤ δij and that for (i, j) to be considered an

inlier by TLS, b¯ij − sˆRˆ a¯ij ≤ δij. Squaring the inequality and deﬁning

uij

=.

: a¯ ij
a¯ ij

s◦R◦a¯ij − sˆRˆ a¯ij 2≤ 4δi2j ⇐⇒

s◦R◦uij − sˆRˆ uij

2≤ 4

δi2j a¯ ij

2

= 4αi2j

(109) (110)

Since we have at least 4 inliers i, j, h, k, whose invariant measurements include a¯ij, a¯ih, a¯ik, and each invariant measurement satisﬁes (110), we can sum (member-wise) the 3 corresponding inequalities and obtain:

s◦R◦uij − sˆRˆ uij 2+ s◦R◦uih − sˆRˆ uih 2+ s◦R◦uik − sˆRˆ uik 2≤ 12αi2j
⇐⇒ (s◦R◦ − sˆRˆ )Uijhk 2F≤ 12αi2j

(111) (112) (113)

where Uijhk = [uij uih uik] and matches the deﬁnition in

the statement of the theorem. Now we note that for any two

matrices A and B it holds

AB

2 F

≥

σmin(B)2

A

2F, which

applied to (113) becomes:

(s◦R◦ − sˆRˆ )Uijhk 2F≥

σmin (Uij hk )2

s◦R◦ − sˆRˆ

2 F

(114) (115)

Chaining the inequality (115) back into (113):

σmin (Uij hk )2

s◦R◦ − sˆRˆ

2 F

≤

12αi2j

⇐⇒

s◦R◦ − sˆRˆ

F≤

√ 23

αij σmin (Uij hk )

(116) (117)

Since we do not know what are the true inliers, we have to take

the worst case over all possible i, j, h, k in (117), yielding:

s◦R◦ − sˆRˆ

F≤

√ 23

maxij αij minijhk σmin(Uijhk)

(118)

We ﬁnally observe that σmin(Uijhk) is different from zero as long as the vectors uij, uih, uik are linearly independent (i.e., do not lie in the same plane), which is the case when the four inliers are not coplanar. This proves the second bound in (27).
Translation error bound. Since the translation subproblem also selects all the inliers, plus potentially some outliers, it holds for each inlier point i:

bi = s◦R◦ai + t◦ + i bi = sˆRˆai + tˆ+ φi

and with

√ φi ≤ 3βi

(119)

where the ﬁrst follows from the deﬁnition of inlier, and the second from the maximum erro√r of a measurement considered inlier by TLS (the factor 3 comes from the fact

30

that translation is estimated component-wise, which means φi 2= [φi]21 + [φi]22 + [φi]23 ≤ 3βi2 ). Combining the
equalities (119):

sˆRˆ ai + tˆ+ φi = s◦R◦ai + t◦ + i ⇐⇒ t◦ − tˆ = (sˆRˆ − s◦R◦)ai + (φi − i)

(120)

Since we have at least 4 inliers i, j, h, k, each satisfying the equality (120), we subtract (member-wise) the ﬁrst equality (including inlier i) from the sum of the last 3 equalities (including points j, h, k):

2(t◦ − tˆ) =

(sˆRˆ − s◦R◦)(al − ai) (121)

l∈{j,h,k}

+

(φl − l − φi + i)

l∈{j,h,k}

(122)

Taking the norm of both members and using the triangle inequality:

2 t◦ − tˆ ≤

(sˆRˆ − s◦R◦)(al − ai) (123)

l∈{j,h,k}

+

( φl + l + φi + i )(124)

l∈{j,h,k}

Noting that (sˆRˆ − s◦R◦)(al − ai) = (sˆRˆ − s◦R◦)a¯il

must satisfy the inequality (108√), and the last 4 terms in (124)

have norm bounded by β and 3β:

2 t◦ − tˆ ≤

2δij +

√ (2 + 2 3)β (125)

l∈{j,h,k}

l∈{j,h,k}

Since we assume all the points to have the same noise bound

β, it hol√ds δij = 2β and the right-hand-side of (125) becomes

(18 + 6 3)β, from which it follows:

√

√

2 t◦ − tˆ ≤ (18 + 6 3)β ⇐⇒ t◦ − tˆ ≤ (9 + 3 3)β (126)

which proves the last bound in (28).

Q. TLS Translation Estimation: Component-wise vs. Full

Component-wise translation estimation is a simple approximation for the full translation estimate. The full TLS translation estimate can be obtained using techniques described in [94], [35]. In this section, we provide numerical experiments to show that the component-wise translation estimation is sufﬁciently close to the full TLS translation estimate.
The TLS estimator of the full translation vector t ∈ R3, given noisy and outlying observations ti, i = 1, . . . , N , is the globally optimal solution of the following non-convex optimization:

N
min min
t∈R3 i=1

t − ti βi2

2
, c¯2

,

(127)

where ti = bi − sˆRˆai in the context of TEASER (cf. eq. (13)). As presented in Section VI-C, instead of estimating the full

translation by solving problem (127), TEASER seeks to esti-

mate the translation vector component-wise by leveraging the

adaptive voting algorithm for solving the scalar TLS problem.

In this section, we show that component-wise translation es-

timation is sufﬁciently close to the full translation estimation.

100

TLS: SDP

TLS: GNC

Component-wise TLS

10-1

Component-wise Max Consensus

Translation Error [m]

10-2

10-3

0

20

40

60

70

80

90

Outlier Rate (%)

Fig. 12. Comparison of four robust translation estimation methods.

In order to show the accuracy of component-wise translation estimation, we design two methods to solve problem (127) globally. The two methods bear great similarity to the semidefinite relaxation [39] and graduated non-convexity (GNC) algorithms [34] presented for solving the rotation subproblem. (i) (TLS: SDP) We use Lasserre’s hierarchy of moment relaxations [123] to relax the nonconvex problem into a convex SDP. By solving the SDP, we can obtain an estimate of the full translation, together with a global optimality certiﬁcate. This method has similar ﬂavor as to the SDP relaxation we presented for estimating rotation, but is slightly more involved. The interested reader can refer to [35] for more technical details. (i) (TLS: GNC) We use GNC to solve the TLS translation estimation problem, where the non-minimal solver is simply the weighted average of the noisy measurements ti’s. GNC is very efﬁcient but it offers no optimality guarantees. Comparison of these two methods (for solving the full translation) with adaptive voting (for solving the component-wise translation) are shown in Fig. 12 (using the same experimental setup as in Section XI-A). We can see that: (i) TLS: SDP performs the best. It is robust to 90% outliers and returns the most accurate translation estimates. The estimates of TLS: SDP are indeed the globally optimal solutions, because the relaxation is empirically always tight (duality gap is zero). (ii) As expected, solving the translation component-wise is an approximation, and it is slightly less accurate than solving the whole translation. However, the estimates are sufﬁciently close to the globally optimal solutions (when the outlier rate is below 90%). (iii) The GNC method obtains the globally optimal solution when the outlier rate is below 90%. This is similar to the performance of the GNC method for solving the rotation subproblem presented in TEASER.

R. SDP Relaxations: Quaternion vs. Rotation Matrix
In Section XI, we compare the rotation estimation error between the quaternion-based SDP relaxation (eq. (23), [39]) and the rotation-matrix-based SDP relaxation ([38]) on the Bunny dataset. In this section, we also report the relative duality gap for the two relaxations on the Bunny dataset. Fig. 13(a) shows the relative duality gap (boxes in red, scale on the righthand y-axis) and the rotation error (boxes in blue, scale on the left-hand y-axis). The ﬁgure shows that the the quaternionbased relaxation is always tighter than the rotation-matrix-

31

Rotation error: Quaternion Rotation error: Rotation matrix

Duality gap: Quaternion Duality gap: Rotation matrix

100 10-5

Relative Duality Gap

Rotation Error [deg]

10-1

10-10

0

20

40

60

80

90

Outlier Rate (%)

(a) Results on the Bunny dataset.

Rotation error: Quaternion Rotation error: Rotation matrix

Duality gap: Quaternion Duality gap: Rotation matrix

102

Relative Duality Gap

Rotation Error [deg]

101 100 10-1

10-5 10-10

0

20

40

60

80

90

Outlier Rate (%)

(b) Results on random simulated unit vectors.

Fig. 13. Comparison of rotation errors (left-axis, blue) and relative duality gap (right-axis, red) between the quaternion-based SDP relaxation (ﬁlled boxplots) and the rotation-matrix-based SDP relaxation (empty boxplots) on (a) the Bunny dataset and (b) random simulated unit vectors.
based relaxation, although both relaxations achieve similar performance in terms of rotation estimation errors.
To further stress-test the two techniques, we compare their performance in a more randomized test setting [39]: at each Monte Carlo run, we ﬁrst randomly sample K = 40 unit vectors from the 3D unit sphere, then apply a random rotation and isotropic Gaussian noise (standard deviation σ = 0.01) to the K unit vectors. Fig. 13(b) shows the results under this setting. We can see that the quaternion-based relaxation remains tight, while the rotation-matrix-based relaxation has a much larger relative duality gap. In addition, a tighter relaxation translates to more accurate rotation estimation: the quaternionbased relaxation dominates the rotation-matrix relaxation and obtains much more accurate rotation estimates, especially at > 80% outlier rates.

S. Stanford Dataset: Extra Results
In all the following experiments, c¯2 = 1 for scale, rotation and translation estimation.
1) Benchmark on Synthetic Datasets: As stated in Section XI-B in the main document, we have benchmarked TEASER against four state-of-the-art methods in point cloud registration (FGR [55], GORE [19], RANSAC1K, RANSAC) at increasing level of outliers, using four datasets from the Stanford 3D Scanning Repository [97]: Bunny, Armadillo, Dragon

and Buddha. Results for the Bunny are showed in the main document and here we show the results for the other three datasets in Fig. 14. The ﬁgure conﬁrms that TEASER dominates the other techniques is insensitive to extreme outlier rates.
2) TEASER on High Noise: In the main document, the standard deviation σ of the isotropic Gaussian noise is set to be 0.01, with the point cloud scaled inside a unit cube [0, 1]3. In this section, we give a visual illustration of how corrupted the point cloud is after adding noise of such magnitude. In addition, we increase the noise standard deviation σ to be 0.1, visualize the corresponding corrupted point cloud, and show that TEASER can still recover the ground-truth transformation with reasonable accuracy. Fig. 17 illustrates a clean (noisefree) Bunny model (Fig. 17(a)) scaled inside the unit cube and its variants by adding different levels of isotropic Gaussian noise and outliers. From Fig. 17(b)(c), we can see σ = 0.01 (noise used in the main document) is a reasonable noise standard deviation for real-world applications, while σ = 0.1 destroys the geometric structure of the Bunny and it is beyond the noise typically encountered in robotics and computer vision applications. Fig. 17(d) shows the Bunny with high noise (σ = 0.1) and 50% outliers. Under this setup, we run TEASER to register the two point clouds and recover the relative transformation. TEASER can still return rotation and translation estimates that are close to the ground-truth transformation. However, due to the severe noise corruption, even a successful registration fails to yield a visually convincing registration result to human perception. For example, Fig. 15 shows a representative TEASER registration result with σ = 0.1 noise corruption and 50% outlier rate, where accurate transformation estimates are obtained (rotation error is 3.42◦ and translation error is 0.098m), but the Bunny in the cluttered scene is hardly visible, even when super-imposed to the clean model.
3) TEASER++ on Problems with Increasing Number of Correspondences: Here we provide more results for TEASER++ on problems with higher number of correspondences. We follow the same experimental setup as detailed in Section XI-B, with the exception that we ﬁxed our outlier rate at 95%, and test the algorithms with increasing numbers of correspondences. Fig. 16 shows the results of the experiments.
Overall, in terms of rotation errors and translation errors, TEASER++ dominates other methods by a large margin. In terms of speed, TEASER++ outperforms other robust estimation algorithms such as GORE. While TEASER++’s runtime scales with the number of correspondences, it still outperforms methods such as GORE on all cases. At 10000 correspondences, TEASER++ only takes around 2 seconds to solve the registration problem. This demonstrates that TEASER++ can be used to solve registration problems with high number of correspondences within reasonable amount of time.

32

102

TEASER RANSAC

GORE

RANSAC (1K)

FGR

101

102

TEASER RANSAC

GORE

RANSAC (1K)

FGR

101

102

TEASER RANSAC

GORE

RANSAC (1K)

FGR

101

Rotation error [deg]

Rotation error [deg]

Rotation error [deg]

100

100

100

10-1

0

0.2

0.4

0.6

0.7

0.8

0.9

Outlier ratio

TEASER RANSAC

100

GORE

RANSAC (1K)

FGR

10-1

10-1

0

0.2

0.4

0.6

0.7

0.8

0.9

Outlier ratio

100

TEASER RANSAC

GORE

RANSAC (1K)

FGR

10-1

10-1

0

0.2

0.4

0.6

0.7

0.8

0.9

Outlier ratio

100

TEASER RANSAC

GORE

RANSAC (1K)

FGR

10-1

Translation error [m]

Translation error [m]

Translation error [m]

10-2

10-2

10-2

10-3

10-3

10-3

0

0.2

0.4

0.6

0.7

0.8

0.9

Outlier ratio

0

0.2

0.4

0.6

0.7

0.8

0.9

Outlier ratio

0

0.2

0.4

0.6

0.7

0.8

0.9

Outlier ratio

(a) Armadillo

(b) Dragon

(c) Buddha

Fig. 14. Results for the Armadillo, Dragon and Buddha datasets at increasing levels of outliers. First row: example of putative correspondences with 50% outliers. Blue points are the model point cloud, red points are the transformed scene model. Second row: rotation error produced by ﬁve methods including TEASER. Third row: translation error produced by the ﬁve methods.

Fig. 15. A single representative TEASER registration result with σ = 0.1 noise corruption and 50% outliers, viewed from two distinctive perspectives.
Clean Bunny model showed in blue and cluttered scene showed in red. Although the rotation error compared to ground-truth is 3.42◦ and the translation error compared to ground-truth is 0.098m, it is challenging for
a human to conﬁrm the correctness of the registration result.

33

TEASER++ RANSAC10K

RANSAC1min FGR

GORE

Time [s]

10 0

10 -2 1000
10 2

2000

3000

4000 5000 6000 7000 Number of Correspondences

8000

(a) Runtimes vs. number of correspondences

TEASER++ RANSAC10K

RANSAC1min FGR

GORE

9000 10000

10 0

Rotation Error [deg]

1000 10 0

2000

3000

4000 5000 6000 7000 Number of Correspondences

8000

(b) Rotation errors vs. number of correspondences

TEASER++ RANSAC10K

RANSAC1min FGR

GORE

9000 10000

Translation Error [m]

10 -2

1000

2000

3000

4000 5000 6000 7000 Number of Correspondences

8000

9000 10000

(c) Translation errors vs. number of correspondences

Fig. 16. Comparison of different robust translation estimation methods with different number of correspondences.

34

(a) Bunny model

(b) Bunny scene, σ = 0.01

(c) Bunny scene, σ = 0.1

(d) Bunny scene, σ = 0.1, 50% outliers

Fig. 17. Bunny point cloud scaled inside unit cube [0, 1]3 and corrupted by different levels of noise and outliers, all viewed from the same perspective angle. (a) Clean Bunny model point cloud, scaled inside unit cube [0, 1]3. (b) Bunny scene, generated from (a) by adding isotropic Gaussian noise with standard
deviation σ = 0.01. (c) Bunny scene, generated from (a) by adding isotropic Gaussian noise with σ = 0.1. (d) Bunny scene, generated from (a) by adding
isotropic Gaussian noise with σ = 0.1 and 50% random outliers.

T. Object Pose Estimation: Extra Results
Fig. 18 shows the registration results for the 8 selected scenes from the University of Washington RGB-D datasets [36]. The inlier correspondence ratios for cereal box are all below 10% and typically below 5%. TEASER is able to compute a highly-accurate estimate of the relative pose using a handful of inliers. Due to the distinctive shape of cap, FPFH produces a higher number of inliers and TEASER is able to register the object-scene pair without problems.

scene-2, # of FPFH correspondences: 550, Inlier ratio: 4.55%, Rotation error: 0.120, Translation error: 0.052.

scene-4, # of FPFH correspondences: 636, Inlier ratio: 4.56%, Rotation error: 0.042, Translation error: 0.051. scene-5, # of FPFH correspondences: 685, Inlier ratio: 2.63%, Rotation error: 0.146, Translation error: 0.176.

scene-7, # of FPFH correspondences: 416, Inlier ratio: 3.13%, Rotation error: 0.058, Translation error: 0.097.
Fig. 18. Object pose estimation on the large-scale RGB-D dataset [36]. First column: FPFH correspondences, second column: inlier correspondences after TEASER, third column: registration result with the registered object highlighted in red. Scene number and related registration information are listed below each scene.

35
scene-9, # of FPFH correspondences: 651, Inlier ratio: 8.29%, Rotation error: 0.036, Translation error: 0.011. scene-11, # of FPFH correspondences: 445, Inlier ratio: 6.97%, Rotation error: 0.028, Translation error: 0.016.
scene-13, # of FPFH correspondences: 612, Inlier ratio: 5.23%, Rotation error: 0.036, Translation error: 0.064. scene-1, # of FPFH correspondences: 207, Inlier ratio: 16.91%, Rotation error: 0.066, Translation error: 0.090.
Fig. 18. Object pose estimation on the large-scale RGB-D dataset [36] (cont.).

36

U. Proof of Proposition 33: Projection onto L¯
Proof: We ﬁrst analyze the constraints deﬁned by the afﬁne subspace L¯ (83). For any matrix M¯ ∈ L¯, M¯ can be written as M¯ = Q¯ − µˆJ + ∆, with ∆ ∈ H and M¯ satisﬁes M¯ x¯ = 0, we write this in matrix form:

 

−µˆI4 − [Q¯ ]01

Kk=1[∆]kk − [∆]01

[Q¯]01 + [∆]01 [Q¯]11 + [∆]11

··· ···

[Q¯]0k + [∆]0k [∆]1k

··· ···

[Q¯]0K + [∆]0K [∆]1K

 e    θˆ1e 

0 0

  

...

 

[Q¯]0k − [∆]0k

... −[∆]1k

...

...

...

· · · [Q¯]kk + [∆]kk · · ·

... [∆]kK

    

... θˆk e





 

=

 



... 0





 

,



  

...

...

...

...

...

...

  

...

  

  

...

  

[Q¯]0K − [∆]0K

−[∆]1K

···

−[∆]kK

· · · [Q¯]KK + [∆]KK

θˆK e

0

(128)

where the expressions for [Q¯]0k and [Q¯]kk are given by the following:

 [Q¯]0k = 
[Q¯]kk = 

−[a¯ k ]2×

+

1 4

(

ξˆk

2−c¯2)I3

+

1 2

a¯ Tk

ξˆk

I3

−

1 2

[ξˆk

]× [a¯ k

]×

−

1 2

ξˆk

a¯ Tk

1 2

[ξˆk

]× a¯ k

1 2

([ξˆk

]× a¯ k

)T

1 4

(

ξˆk

2−c¯2)

−2[a¯ k ]2×

+

1 2

(

ξˆk

2+c¯2)I3 + a¯Tk ξˆkI3 − [ξˆk]×[a¯k]× − ξˆka¯Tk

[ξˆk ]× a¯ k

([ξˆk ]× a¯ k )T

1 2

(

ξˆk

2+c¯2)

, ∀k = 1, . . . , K. (129)

Denote θˆ0 = +1, we write the k-th (k = 1, . . . , K) block-row equality of (128):





K

θˆ0[Q¯]0k + θˆk[Q¯]kk + θˆk[∆]kk +

θˆi[∆]ki e = 0

i=0,i=k

⇔

θˆ0[Q¯]v0k + θˆk[Q¯]vkk + θˆk[∆]vkk +

K i=0,i=k

θˆi[∆]vki

θˆ0[Q¯ ]s0k + θˆk[Q¯ ]skk + θˆk[∆]skk

e=0

⇔

[∆]vkk = −θˆ0θˆk[Q¯]v0k − [Q¯]vkk −

K i=0,i=k

θˆi θˆk [∆]vki

=

−(

1 2

θˆ0θˆk

+

1)[ξˆk ]× a¯ k

−

[∆]skk

=

−θˆ0 θˆk [Q¯ ]s0k

−

[Q¯ ]skk

=

−(

1 4

θˆ0θˆk

+

1 2

)

ξˆk

2

+(

1 4

θˆ0

θˆk

−

1 2

)c¯2

K i=0,i=k

θˆiθˆk

[∆]vki

,

and from eq. (132), we can get the a sum of all [∆]vkk to be:

K
[∆]vkk
k=1

K
=−
k=1

1 2

θˆ0θˆk

+

1

K
[ξˆk]×a¯k −

θˆi θˆk [∆]vki

k=1 i=0,i=k

K
=−

1 2

θˆ0θˆk

+

1

K
[ξˆk]×a¯k + θˆ0θˆk[∆]v0k,

k=1

k=1

and the sum of all [∆]skk to be:

K

K

[∆]skk = −

1 4

θˆ0

θˆk

+

1 2

k=1

k=1

ξˆk 2+

1 4

θˆ0θˆk

−

1 2

c¯2.

Now we write the 0-th block row equality of (128):

(130) (131) (132)
(133) (134)

K

K

K

−θˆ0µˆI4 − θˆ0[∆]kk + θˆk[Q¯]0k + θˆk[∆]0k e = 0

k=1

k=1

k=1

⇔

−

K k=1

θˆ0[∆]vkk

+

K k=1

θˆk[Q¯ ]v0k

+

K k=1

θˆk

[∆]v0k

−θˆ0µˆ −

K k=1

θˆ0[∆]skk

+

K k=1

θˆk[Q¯ ]s0k

e=0

⇔

Kk=1[∆]vkk =

K k=1

θˆ0θˆk[Q¯ ]v0k

+

K k=1

θˆ0 θˆk [∆]v0k

Kk=1[∆]skk = −µˆ +

K k=1

θˆ0

θˆk

[Q¯ ]s0k

⇔

Kk=1[∆]vkk =

K k=1

1 2

θˆ0

θˆk

[ξˆk

]×

a¯ k

+

K k=1

θˆ0θˆk

[∆]v0k

Kk=1[∆]skk = −µˆ +

K k=1

1 4

θˆ0 θˆk (

ξˆk

2−c¯2)

,

(135) (136) (137) (138)

37

and compare eq. (138) with eq. (133):

K
−

1 2

θˆ0θˆk

+

1

[ξˆk]×a¯k +

K

θˆ0θˆk[∆]v0k =

K

1 2

θˆ0 θˆk [ξˆk ]× a¯ k

+

K

θˆ0 θˆk [∆]v0k

k=1

k=1

k=1

k=1

K
⇔−

1 2

θˆ0θˆk

+

1

[ξˆk]×a¯k =

K

1 2

θˆ0θˆk

[ξˆk

]×

a¯ k

k=1

k=1

⇔

−

3 2

[ξˆk ]× a¯ k

+

−

1 2

[ξˆk

]×

a¯ k

=

1 2

[ξˆk ]× a¯ k

+

−

1 2

[ξˆk

]×

a¯ k

θˆk =+1

θˆk =−1

θˆk =+1

θˆk =−1

⇔

[ξˆk]×a¯k = 0,

θˆk =+1

(139) (140) (141) (142)

which states that for inliers θˆk = +1, the sum θˆk=+1[ξˆk]×a¯k = 0 must hold12. We then compare eq. (138) with eq. (134):

K
⇔−
k=1

K
−

1 4

θˆ0

θˆk

+

1 2

k=1

ξˆk 2+

1 4

θˆ0

θˆk

−

1 2

c¯2 = −µˆ +

K

1 4

θˆ0θˆk

k=1

ξˆk 2−c¯2

1 4

θˆk

+

1 2

ξˆk 2+

1 4

θˆk

−

1 2

c¯2 = −

1 2

(θˆk

+

1)

ξˆk

2+

1 2

(1

−

θˆk

)c¯2

+

K

1 4

θˆk

k=1

ξˆk 2−c¯2

K
⇔−

1 4

θˆk

+

1 2

k=1

ξˆk 2+

1 4

θˆk

−

1 2

K
c¯2 = −

1 4

θˆk

+

1 2

k=1

ξˆk 2+

1 4

θˆk

−

1 2

c¯2,

(143) (144) (145)

which holds true without any condition. Therefore, we conclude that the equality constraint of the 0-th block row adds no extra constraints on the matrix ∆. This leads to the following Theorem.

Theorem 34 (Equivalent Projection to L¯). The projection of a matrix M onto L¯ is equivalent to:

where H¯ is deﬁned by:

ΠL¯(M ) = ΠH¯ (M − Q¯ + µˆJ ) + Q¯ − µˆJ , H¯ =. {∆ : ∆ ∈ H, ∆ satisﬁes eq. (132)}

(146) (147)

Next we prove that the projection steps in Proposition 33 deﬁne the correct projection ΠH¯ . To this end, we use H¯ to denote the projection of a matrix H ∈ S4(K+1) onto the set H¯. By the deﬁnition of projection, H¯ is the minimizer of the following optimization:

H¯

= arg min

H

−∆

2 F

.

∆∈H¯

(148)

Denoting the objective function of eq. (148) as f =

H −∆

2 F

,

we

separate

the

Frobenius

norm

of

H

−∆

into

a

sum

of

block-wise Frobenius norms:

f=

H

−∆

2 F

K

K

L

L

=

[∆]m kk − [H]m kk

2 F

+

[∆]skk − [H]skk

2 F

+2

[∆]m Z(l) − [H]m Z(l)

2 F

+

2

[∆]sZ(l) − [H]sZ(l)

2 F

k=0

k=0

l=1

l=1

(149)

fkmk: diagonal block matrix part
+2

fksk: diagonal block scalar part

fZm: off-diagonal block matrix part

fZs : off-diagonal block scalar part

K

L

L

[∆]vkk − [H]vkk

2 F

+

[∆]vW(l) − [H]vW(l)

2 F

+

[∆]vZ(l) − [H]vZ(l)

2 F

,

k=0

l=1

l=1

(150)

f v: diagonal and off-diagonal vector part

where W is the following set of L ordered indices that enumerate the lower-triangular blocks of any matrix ∆ ∈ S4(K+1):

W = {(1, 0), . . . , (K, 0), (2, 1), . . . , (K, 1), . . . , (K, K − 1)},

(151)

and W(l) equals to the ﬂipped copy of Z(l) (eq. (91)). Using the separated Frobenius norm (150), we can get the projection H¯ block by block in the following steps.

12This condition is always true because this is the ﬁrst-order stationary condition for the TLS optimization considering only the least squares part applied on inliers.

38

1) Diagonal block matrix part. From the expression of fkmk and the constraint that Kk=0[∆]m kk = 0, we can get the diagonal block matrix part of H¯ to be:

K

[H ]m kk

[H¯ ]m kk

=

[H ]m kk

−

k=0
K

+

1

,

∀k = 0, . . . , K,

(152)

which ﬁrst computes the mean of all diagonal block matrix parts,

K k=0

[H

]m kk

/(K

+

1),

and

then

subtracts

the

mean

from each individual diagonal block matrix part [H]m kk.

2) Diagonal block scalar part. From the expression of fksk and the constraint from eq. (132) (which ﬁxes the scalar parts

to be known constants), we can get the diagonal block scalar part of H¯ to be:

[H¯ ]skk =

−(

1 4

θˆk

+

1 2

)

ξˆk

2

+(

1 4

θˆk

−

1 2

)c¯2

Kk=1(

1 4

θˆk

+

1 2

)

ξˆk

2−(

1 4

θˆk

−

1 2

)c¯2

k = 1, . . . , K, ,
k = 0.

(153)

which ﬁrst computes the diagonal scalar parts for k = 1, . . . , K and then assigns the negative sum of all diagonal scalar parts to the scalar part of the top-left diagonal block ([H¯ ]s00). 3) Off-diagonal block matrix part. From the expression of fZm and the constraint that [∆]Z(l) is skew-symmetric, we can obtain the off-diagonal block matrix part of H¯ to be:

[H¯ ]m Z(l)

=

[H ]m Z(l)

− ([H]m Z(l))T , 2

∀l = 1, . . . , L,

(154)

which simply projects [H]m Z(l) to its nearest skew-symmetric matrix. Note that it sufﬁces to only project the upper-triangular off-diagonal blocks because both H and H¯ are symmetric matrices.
4) Off-diagonal block scalar part. The off-diagonal blocks are skew-symmetric matrices and the scalar parts must be zeros. Therefore, the off-diagonal block scalar parts of H¯ are:

[H¯ ]sZ(l) = 0, ∀l = 1, . . . , L.

(155)

5) Off-diagonal block vector part. The projection of the vector parts of each block is non-trivial due to the coupling between the diagonal block vector parts [∆]vkk and the off-diagonal block vector parts [∆]vZ(l), [∆]vW(l). We ﬁrst write the diagonal block vector parts as functions of the off-diagonal block vector parts by recalling eq. (132) and (133):

 [∆]vkk 

=

−(

1 2

θˆ0θˆk

+

1)[ξˆk ]× a¯ k

−







 

:=φk

i=0,i=k θˆiθˆk[∆]vki = −

i=0,i=k θˆiθˆk[∆]vki + φk

[∆]v00 = −   

K k=1

[∆]vkk

=

K

(

1 2

θˆ0θˆk

+ 1)[ξˆk]×a¯k −

k=1

K k=1

θˆ0 θˆk [∆]v0k

=

−

i=0,i=0 θˆiθˆk[∆]0i + φ0

,







:=φ0

(156)

which shows that we can write [∆]vkk, k = 0, . . . , K in a uniﬁed notation:

K

[∆]vkk = −

θˆkθˆi[∆]vki + φk,

i=0,i=k

∀k = 0, . . . , K.

(157)

Inserting eq. (157) into the expression of f v in eq. (150), and using the fact that [∆]vW(l) = −[∆]vZ(l), we obtain the following unconstrained optimization problem:

L
min
[∆]vZ(l) l=1

2
[∆]vZ(l) − [H]vZ(l) +

2K
[∆]vZ(l) + [H]vW(l) +
k=0

2 K
θˆkθˆi[∆]vki − φk + [H]vkk .
i=0,i=k

(158)

To solve problem (158), we ﬁrst derive the partial derivatives w.r.t. each variable [∆]vZ(l). For notation simplicity, we write Z(l) = (rl, cl), where rl < cl represent the block-row and block-column indices. In addition, we use u(r, c) to denote

39

the pointer that returns the index of (r, c) in the set Z. Using this notation, the partial derivative of f v w.r.t. [∆]vZ(l) is:

∂fv

∂

∂[∆]vZ(l) =

[∆]vZ(l)−[H]vZ(l) 2+ [∆]vZ(l)+[H]vW(l) 2+

K i=0,i=rl

θˆrl θˆi[∆]vrl,i−φrl +[H]vrl,rl

2+

K i=0,i=cl

θˆcl θˆi[∆]vcl,i−φcl +[H]vcl,cl

2

∂[∆]vZ(l)



2[∆]vZ(l) − [H]vZ(l) + [H]vW(l)+













K

K



=

2

θˆrl

θˆcl

 

θˆrl θˆi[∆]vrl,i −φrl

+

[H

]vrl

,rl

 

−

θˆcl θˆrl

 

θˆcl θˆi[∆]vcl,i

−

φcl

+



[H

]vcl

,cl

 

i=0 i=rl

i=0 i=cl





K

K

= 2 4[∆]vZ(l) −[H]vZ(l) + [H]uW(l) +

θˆcl θˆi[∆]vrl,i −

θˆrl θˆi[∆]vcl,i +θˆrl θˆcl φcl − φrl + [H ]vrl,rl − [H ]vcl,cl  .

i=rl ,cl

i=rl ,cl

(159) (160) (161)

Notice that not all of the [∆]vrl,i and [∆]vcl,i in eq. (161) belong to the upper-triangular part of ∆, therefore, we use the fact that [∆]vij = −[∆]vji for any i = j to convert all lower-triangular vectors to upper-triangular. In addition, we set the partial derivative to zero to obtain sufﬁcient and necessary conditions to solve the convex unconstrained optimization (158):

K

K

4[∆]vZ(l) +

θˆcl θˆi[∆]vrl,i −

θˆrl θˆi[∆]vcl,i = [H ]vZ(l) − [H ]vW(l) + θˆrl θˆcl φrl − φcl − [H ]vrl,rl + [H ]vcl,cl

i=rl ,cl

i=rl ,cl

:=Fl (H )

(162)

⇔ 4[∆]vZ(l) +

θˆcl θˆi[∆]vrl,i +

θˆcl θˆi[∆]vrl,i −

θˆrl θˆi[∆]vcl,i −

θˆrl θˆi[∆]vcl,i = Fl(H) (163)

i<rl

i>rl ,i=cl

i<cl ,i=rl

i>cl

⇔ 4[∆]vZ(l) −

θˆcl θˆi[∆]vu(i,rl) +

θˆcl θˆi[∆]vu(rl,i) +

θˆrl θˆi[∆]vu(i,cl) −

θˆrl θˆi[∆]vu(cl,i) = Fl(H) (164)

i<rl

i>rl ,i=cl

i<cl ,i=rl

i>cl

⇔ ATl C(∆) = FlT(H), (165)

where the linear map C : S4(K+1) → RL×3 is as deﬁned in eq. (92), and Fl(H) ∈ R3 is deﬁned as in eq. (162), and Al ∈ RL is the following vector:



4





−θˆcl θˆi

Al(u)

=

θˆcl θˆi θˆrl θˆi

−θˆrl θˆi



0

if u = l if u = u(i, rl), ∀0 ≤ i < rl if u = u(rl, i), ∀rl < i ≤ K, i = cl , if u = u(i, cl), ∀0 ≤ i < cl, i = rl if u = u(cl, i), ∀cl < i ≤ K otherwise

(166)

where Al(u) denotes the u-th entry of vector Al. Because the partial derivative w.r.t. each variable [∆]vZ(l), l = 1, . . . , L, must be zero at the global minimizer, H¯ , of problem (158), we have L linear equations of the form eq. (165), which allows us to write the following matrix form:

[A1, . . . , Al, . . . , AL]T C(H¯ ) = [F1(H), . . . , Fl(H), . . . , FL(H)]T

(167)

A∈S L
and we can compute C(H¯ ) by:

∈RL×3

F (H)∈RL×3

C(H¯ ) = A−1F (H).

(168)

Surprisingly, due to the structure of A, its inverse can be computed in closed form.

Theorem 35 (Closed-form Matrix Inverse). The inverse of matrix A (166), denoted P , can be computed in closed form, with each column of P being:



p1

θˆcl θˆip2

Pl(u)

=

−θˆcl θˆip2 −θˆrl θˆip2

θˆrl θˆip2



0

if u = l if u = u(i, rl), ∀0 ≤ i < rl if u = u(rl, i), ∀rl < i ≤ K, i = cl , if u = u(i, cl), ∀0 ≤ i < cl, i = rl if u = u(cl, i), ∀cl < i ≤ K otherwise

(169)

40

where p1 and p2 are the following constants:

K +1

1

p1

=

2K

+

, 6

p2

=

2K

+

. 6

(170)

Proof: To show P is the inverse of A, we need to show AP = IL. To do so, we ﬁrst show that ATl Pl = 1, which means the diagonal entries of AP are all equal to 1. Note that from the deﬁnition of Al in (166) and Pl in (169), one can see that there are 2(K − 1) + 1 non-zero entries in Al and Pl and Pl(u) = −p2Al(u) when u = l, and Pl(u) = p1, Al(u) = 4 when u = l, therefore, we have:

ATl Pl

=

4p1

− 2(K

−

1)p2

=

4K 2K

+4 +6

−

2K 2K

−2 +6

=

2K 2K

+ +

6 6

=

1.

(171)

We then show that ATl Pm = 0 for any l = m. The non-zero indices of Al, Pm and their corresponding entries are:

u u(rl, cl) u(i, rl), ∀0 ≤ i < rl u(rl, i), ∀rl < i ≤ K, i = cl u(i, cl), ∀0 ≤ i < cl, i = rl u(cl, i), ∀cl < i ≤ K,

Al(u)
4
−θˆcl θˆi θˆcl θˆi θˆrl θˆi −θˆrl θˆi

u u(rm, cm) u(i, rm), ∀0 ≤ i < rm u(rm, i), ∀rm < i ≤ K, i = cm u(i, cm), ∀0 ≤ i < cm, i = rm u(cm, i), ∀cm < i ≤ K,

Pm(u)

p1

θˆcm θˆip2 −θˆcm θˆip2

,

−θˆrm θˆip2

θˆrm θˆip2

(172)

where we have used Z(l) = (rl, cl) and Z(m) = (rm, cm). Now depending on the 12 different relations between (rl, cl) and (rm, cm), Al and Pm can have different ways of sharing common indices whose corresponding entries in both Al and Pm are non-zero. We next show that in all these 12 different cases, ATl Pm = 0 holds true.
a) rl < cl < rm < cm: Al and Pm share 4 non-zero entries with the same indices:

u
u(rl, rm) u(rl, cm) u(cl, rm) u(cl, cm)

Al(u)
θˆcl θˆrm θˆcl θˆcm −θˆrl θˆrm −θˆrl θˆcm

Pm(u)
θˆcm θˆrl p2 −θˆrm θˆrl p2 . θˆcm θˆcl p2 −θˆrm θˆcl p2

(173)

Using the above table, we easily see that ATl Pm = 0. b) rl < cl = rm < cm: Al and Pm share the following common indices whose corresponding entries are non-zero:

u
u(rl, cl) u(rl, cm) u(i, cl), ∀0 ≤ i < cl, i = rl u(cl, i), ∀cl < i ≤ K, i = cm u(cl, cm)

Al(u)
4
θˆcl θˆcm θˆrl θˆi −θˆrl θˆi −θˆrl θˆcm

Pm(u)
θˆcm θˆrl p2 −θˆcl θˆrl p2 θˆcm θˆip2 −θˆcm θˆip2
p1

(174)

Using the table above, we can calculate ATl Pm:
ATl Pm = 4θˆcm θˆrl p2 − θˆrl θˆcm p2 + (cl − 1)θˆrl θˆcm p2 + (K − cl − 1)θˆrl θˆcm p2 − θˆrl θˆcm p1 = ((K + 1)p2 − p1)θˆrl θˆcm = 0,

(175)

which ends up being 0 due to p1 = (K + 1)p2 (cf. eq. (170)). c) rl < rm < cl < cm: Al and Pm share 4 common indices with non-zero entries:

u
u(rl, rm) u(rl, cm) u(rm, cl) u(cl, cm)

Al(u)
θˆcl θˆrm θˆcl θˆcm θˆrl θˆrm −θˆrl θˆcm

Pm(u)
θˆcm θˆrl p2 −θˆrm θˆrl p2 −θˆcm θˆcl p2 −θˆrm θˆcl p2

(176)

from which we can compute that ATl Pm = 0.

41

d) rl = rm < cl < cm: Al and Pm share the following common indices with non-zero entries:

u
u(rl, cl) u(i, rl), ∀0 ≤ i < rl u(rl, i), ∀rl < i ≤ K, i = cl, cm
u(rl, cm) u(cl, cm)

Al(u)
4
−θˆcl θˆi θˆcl θˆi θˆcl θˆcm −θˆrl θˆcm

Pm(u) −θˆcm θˆcl p2
θˆcm θˆip2 −θˆcm θˆip2
p1 −θˆrk θˆcl p2

from which we can compute ATl Pm:

ATl Pm = (−4p2 − rlp2 − (K − rl − 2)p2 + p1 + p2)θˆcl θˆcm = (p1 − (K + 1)p2)θˆcl θˆcm = 0

(177) (178)

e) rl < rm < cl = cm: Al and Pm share the following common indices with non-zero entries:

u
u(rl, cl) u(rl, rm) u(i, cl), ∀0 ≤ i < cl, i = rl, rm u(rm, cl) u(cl, i)∀cl < i ≤ K

Al(u)
4
θˆcl θˆrm θˆrl θˆi θˆrl θˆrm −θˆrl θˆi

Pm(u) −θˆrm θˆrl p2
θˆcl θˆrl p2 −θˆrm θˆip2
p1 θˆrm θˆip2

from which we compute ATl Pm:

ATl Pm = (−4p2 + p2 − (cl − 2)p2 + p1 − (K − cl)p2)θˆrl θˆrm = (p1 − (K + 1)p2)θˆrl θˆrm = 0.

(179) (180)

f) rm < rl < cl < cm: Al and Pm share 4 common indices with non-zero entries:

u
u(rm, rl) u(rl, cm) u(rm, cl) U (cl, cm)

Al(u)
−θˆcl θˆrm θˆcl θˆcm θˆrl θˆrm −θˆrl θˆcm

Pm(u)
−θˆcm θˆcm θˆrl p2 −θˆrm θˆrl p2 −θˆcm θˆcl p2 −θˆrm θˆcl p2

(181)

from which we easily see that ATl Pm = 0. g) rm < rl < cl = cm: same as rl < rm < cm = cl by symmetry (switch m and l). h) rm < rl < cm < cl: same as rl < rm < cl < cm by symmetry. i) rm < rl = cm < cl: same as rl < rm = cl < cm by symmetry. j) rm < cm < rl < cl: same as rl < cl < rm < cm by symmetry. k) rl < rm < cm < cl: same as rm < rl < cl < cm by symmetry. l) rm = rl < cm < cl: same as rl = rm < cl < cm by symmetry.
As we show above, in all cases, AlPm = 0, when l = m. Therefore, we prove that AP = IL and P is the closed-form inverse of A. Therefore, using Theorem 35, we can ﬁrst generate P directly (using eq. (169)) and then apply eq. (168) to obtain the off-diagonal block vector parts of H¯ . 6) Diagonal block vector part. After obtaining off-diagonal block vector parts, we can insert them back into eq. (157) to
obtain the diagonal block vector parts:

K

[H¯ ]vkk = −

θˆkθˆi[H¯ ]vki + φk,

i=0,i=k



φk

=

− 

1 2

θˆk

K k=1

+1

1 2

θˆk

[ξˆk ]× a¯ k + 1 [ξˆk]×a¯k

k = 1, . . . , K .
k=0

(182) (183)

V. Initial Guess for Algorithm 3 We describe the initial guess M¯ 0 as:

M¯ 0 = Q¯ − µˆJ + ∆0,

(184)

42

where ∆0 ∈ H¯ is the following:

Off-diagonal blocks:

[∆]U(l) = 0, ∀l = 1, . . . , L

Diagonal block scalar part: Diagonal block vector part:

[∆]skk =

−(

1 4

θˆk

+

1 2

)

ξˆk

2

+(

1 4

θˆk

−

1 2

)c¯2

Kk=1(

1 4

θˆk

+

1 2

)

ξˆk

2

−(

1 4

θˆk

−

1 2

)c¯2

k = 1, . . . , K, k = 0.



[∆]vkk

=

− 

1 2

θˆk

K

k=1

+1

1 2

θˆk

[ξˆk ]× a¯ k + 1 [ξˆk]×a¯k

k = 1, . . . , K, k=0

Diagonal block matrix part:

[∆]m kk

=

 −[Q¯ ]0k
K
 k=1

−

1 4

θˆk

+

1 4

[Q¯]0k +

1 4

θˆk

ξˆk

+

1 4

2I3 − ξˆk

1 2

c¯2

I3

2I3 +

1 2

c¯2

I3

k = 1, . . . , K .
k=0

One can verify that ∆0 satisﬁes all the constraints deﬁned by set H¯ (147).

(185) (186) (187) (188)

W. Tighter Bounds for Theorem 17
In this section, we improve over the bounds presented in Theorem 17 and derive tighter but more complex bounds for scale, rotation, and translation estimation.

Theorem 36 (Estimation Contract with Noisy Inliers and Adversarial Outliers). Assume (i) the set of correspondences contains at least 3 distinct and non-collinear inliers, where for any inlier i, bi − s◦R◦ai − t◦ ≤ βi, and (s◦, R◦, t◦) denotes the ground truth transformation. Assume (ii) the inliers belong to the maximum consensus set in each subproblem13, and (iii)
the second largest consensus set is “sufﬁciently smaller” than the maximum consensus set (as formalized in Lemma 23). If (iv) the rotation subproblem produces a valid certiﬁcate (as per Theorems 13-14), then the output (sˆ, Rˆ, tˆ) of TEASER satisﬁes:

|sˆ − s◦|≤ min{ζis, i ∈ I},

s◦(1 − cos(θR)) ≤

i∈I (ζiR)2 − (sˆ − s◦)2 AI 2sˆ (σ12(AI ) + σ22(AI ))

2
F,

|tˆl − t◦l |≤ min (sˆ − s◦)2 + 2sˆs◦(1 − cos(θR)) ai 2+[ζit]l : i ∈ I

, ∀l = 1, 2, 3.

(189) (190) (191)

where I is the set of ground-truth inliers with size NI , i.e., ground-truth TRIMs in the scale bound, ground-truth TIMs in the rotation bound, and ground-truth correspondences in the translation bound; tˆl, t◦l , l = 1, 2, 3, denotes the l-th entry of the translation vector; ζis, ζiR and [ζit]l are deﬁned as follows:

ζis = |si − sˆ|+αi

 ζiR =

b¯i −sˆRˆ a¯ i a¯ i

+ αi

[ζit]l = |[bi]l − [sˆRˆ ai]l − tˆl|+βi

, ∀i ∈ I, ∀l = 1, 2, 3;

(192)

θR is the angular (geodesic) distance between Rˆ and R◦; AI =

a¯ i1 a¯ i

,...,

a¯ iNI a¯ NI

∈ R3×NI is the matrix that assembles all

normalized TIMs in the set I, and σ1(AI), σ2(AI) denotes the smallest and second smallest singular values of matrix AI.

Note that since there are at least 3 non-collinear ground-truth inliers, rank (AI) ≥ 2 and σ1(AI) + σ2(AI) > 0, meaning the

rotation bound is well deﬁned. Since the bounds rely on the knowledge of the inliers I, to compute the worst-case bounds, one

has to enumerate over all possible subsets of NI = 3 inliers to compute the largest bounds for scale, rotation and translation.

Proof: We will prove the bounds for scale, rotation, and translation separately. Scale Error Bound. Let us denote with C¯ the consensus set associated with the optimal solution of the TLS scale estimation problem, with |C¯|= N¯in. To establish a connection with the ground-truth scale s◦, we partition C¯ into two sets: I that contains
all the indices of the true inliers, and J that contains all the indices of the false inliers, which are not generated from the true inlier model (TRIM) where classiﬁed as inliers by TLS and ended up in C¯. Let NI = |I| and NJ = |J |, and we have NI + NJ = N¯in. For scale measurements that are in I and J , we have the following relations:

si = s◦ + i = sˆ + φi

∀i ∈ I ,

sj = sˆ + φj

∀j ∈ J

(193)

where φi’s and φj’s are residuals that can be computed from si, sj, and sˆ, and i’s are unknown noise terms. Note that we

have | i|≤ αi, |φi|≤ αi, |φj|≤ αj. From the ﬁrst equation in eq. (193), we have:

sˆ − s◦ = i − φi ⇒ |sˆ − s◦|≤ |φi|+αi =. ζis, ∀i ∈ I,

(194)

⇒ |sˆ − s◦|≤ min{ζis, i ∈ I}

(195)

13In other words: the inliers belong to the largest set S such that for any i, j ∈ S, |sij − s|≤ αij , b¯ij − sRa¯ij ≤ δij , and |[bi − sRai − t]l| ≤ βi, ∀l = 1, 2, 3 for any transformation (s, R, t). Note that for translation the assumption is applied component-wise.

43

where we have deﬁned a new quantity ζis = |φi|+αi, that can be computed for every TRIM in the consensus set C¯. Eq. (194) states that the scale error is bounded by the minimum ζi in the set of true inliers. Therefore, if we know what the set of true inliers I is, then we can simply take the minimum of the set {ζi, i ∈ I}. However, if we do not know the true inlier set, assuming NI ≥ 3, we get the worst-case bound:

|sˆ − s◦|≤ ζms 3 ,

(196)

where ζm3 is the third-largest ζks in the set {ζks = |sk − sˆ|+αk : k ∈ C¯}. Rotation Error Bound. Similar to the previous case, we use C¯ to denote the consensus set of TIM at the optimal TLS

estimate. We also use I to denote the set of true inliers and use J to denote the set false inliers, i.e., outliers that TLS selected

as inliers. We have the following equations for the true inliers and false inliers:

b¯i = s◦R◦a¯i + i = sˆRˆ a¯i + φi b¯j = sˆRˆa¯j + φj

∀i ∈ I ,
∀j ∈ J

(197)

where φi, φj are the residuals that can be computed and i is the unknown noise term. From the ﬁrst equation of eq. (197), we have:

(sˆRˆ − s◦R◦)a¯i =

i − φi ⇔ (sˆRˆ − s◦R◦)

a¯ i a¯ i

=

i − φi a¯ i

(198)

⇒

(sˆRˆ − s◦R◦)ui ≤

φi a¯ i

+ αi =. ζiR, ∀i ∈ I,

(199)

where ui =.

a¯ i a¯ i

and we have deﬁned a new quantity ζiR =.

φi / a¯i +αi that can be computed from the TLS solution.

Since eq. (199) holds for every TIM in I, we can aggregate all inequalities:

(sˆRˆ − s◦R◦)AI

2 F

≤

(ζiR)2,

i∈I

(200)

where AI = [ui1 , . . . , uiNI ] ∈ R3×NI , i1, . . . , iNI ∈ I, assembles all normalized TIMs ui in the set I. Now we want to get a

lower bound on

(sˆRˆ − s◦R◦)AI

2 F

so that we can upper-bound the rotation error.

Towards this goal, we ﬁrst analyze the singular values of (sˆRˆ − s◦R◦). Let the three singular values of (sˆRˆ − s◦R◦) be

σl, l = 1, 2, 3, then we know that σl2, l = 1, 2, 3, are equal to the eigenvalues of the matrix B = (sˆRˆ − s◦R◦)T(sˆRˆ − s◦R◦).

Expanding B, we get:

B = (sˆRˆ T − s◦(R◦)T)(sˆRˆ − s◦R◦) = (sˆ2 + (s◦)2)I3 − sˆs◦ RˆTR◦ + (R◦)TRˆ = (sˆ2 + (s◦)2)I3 − sˆs◦ Rs + RsT ,(201)

where we have denoted Rs = RˆTR◦ ∈ SO(3), which is the composition of two rotation matrices. Let the axis-angle
representation of Rs be (Φs, θR), then the axis-angle representation for Rs is simply (Φs, −θR). According to the Rodrigues’ rotation formula, we can write Rs and RsT as:

Rs = cos(θR)I3 + sin(θR)[Φs]× + (1 − cos(θR))ΦsΦTs RsT = cos(−θR)I3 + sin(−θR)[Φs]× + (1 − cos(−θR))ΦsΦTs = cos(θR)I3 − sin(θR)[Φs]× + (1 − cos(θR))ΦsΦTs

.(202)

Inserting eq. (202) back into eq. (201), we have:

B = (sˆ2 + (s◦)2)I3 − sˆs◦ 2 cos(θR)I3 + 2(1 − cos(θR))ΦsΦTs ,

(203)

from which we can see that the three eigenvectors of B are Φs and Φ⊥1 and Φ⊥2 , where we pick Φ⊥1 and Φ⊥2 to be 2 unit orthogonal vectors in the plane that is orthogonal to Φs:

BΦs = (sˆ2 + (s◦)2) − 2sˆs◦ Φs = (sˆ − s◦)2Φs BΦ⊥l = sˆ2 + (s◦)2 − 2sˆs◦ cos(θR) Φ⊥l , ∀l = 1, 2

.

(204)

Therefore, we have σ1 = σ2 = sˆ2 + (s◦)2 − 2sˆs◦ cos(θR) = (sˆ − s◦)2 + 2sˆs◦(1 − cos(θR)) and σ3 = (sˆ − s◦)2. Let the SVD

of (sˆRˆ − s◦R◦) be (sˆRˆ − s◦R◦) = U∆S∆V∆T, we lower-bound

(sˆRˆ − s◦R◦)AI

2 F

:

 σ1 0 0   (V∆TAI )1  2

(sˆRˆ − s◦R◦)AI

2 F

=

U∆S∆V∆TAI

2 F

=

S∆V∆TAI

2 F

=



0

σ2

0   (V∆TAI )2 

0 0 σ3

(V∆TAI )3 F

= (sˆ − s◦)2 (V∆TAI )1 2+ (V∆TAI )2 2+ (V∆TAI )3 2 + 2sˆs◦(1 − cos(θR)) (V∆TAI )1 2+ (V∆TAI )2 2

= (sˆ − s◦)2

AI

2 F

+2sˆs◦(1

−

cos(θR

))

AI

2 F

−

(V∆TAI )3

2

≥ (sˆ − s◦)2

AI

2 F

+2sˆs◦(1

−

cos(θR))

σ12(AI ) + σ22(AI )

,

(205)
(206) (207) (208)

44

where we have used

V∆AI

2 F

=

AI

2 F

=

σ12(AI )

+

σ22(AI )

+

σ32(AI ),

with

σ1(AI )

≤

σ2(AI )

≤

σ3(AI )

being

the

three

singular values of AI in ascending order. The last inequality in eq. (208) follows from:

0 0 0

2

0 0 0 2

(V∆TAI )3 2=  0 0 0  V∆TAI ≤ σ32(AI )  0 0 0  V∆T = σ32(AI ).

001

F

001

F

(209)

Now we combine eq. (208) and (200) to bound 1 − cos(θR):

(sˆ − s◦)2

AI

2 F

+2sˆs◦(1

−

cos(θR

))

σ12(AI ) + σ22(AI )

≤

(ζiR)2

(210)

i∈I

⇔ s◦(1 − cos(θR)) ≤

i∈I (ζiR)2 − (sˆ − s◦)2 AI 2sˆ (σ12(AI ) + σ22(AI ))

2
F,

(211)

where we note that (i) the existence of at least 3 non-collinear inliers guarantees that rank (AI) ≥ 2, which means σ12(AI) + σ22(AI) > 0 and the upper bound is well-deﬁned; (ii) since (Φs, θR) is the axis-angle representation for RˆTR◦, θR is the angular (geodesic) distance between Rˆ and R◦; (iii) the rotation bound is dependent on the scale bound |sˆ − s◦| (195).

Again, when the true inlier set I is known, one can directly compute the rotation bound using eq. (211); when the true inlier

set I is unknown, one has to enumerate over all possible subsets of 3 TIMs (formed by a triplet of (i, j, k) correspondences) in C¯ to compute the worst-case bound.
Translation Error Bound. Let t◦ = [t◦1, t◦2, t◦3]T and tˆ = [tˆ1, tˆ2, tˆ3]T be the ground-truth and estimated translation. Let C¯l, l = 1, 2, 3, be the set of measurements (ak, bk) that pass the l-th component-wise adaptive voting, i.e., |[bk − sˆRˆak − tˆ]l|≤ βk, ∀k ∈ C¯l, l = 1, 2, 3. Again, we let Il and Jl be the set of true and false inliers within C¯l, l = 1, 2, 3. From condition (ii),
we have that I1 = I2 = I3 = I, which says that the true inliers pass all three component-wise adaptive voting. However,
Jl, l = 1, 2, 3, could potentially be different. Then we write the following relations:

[bi]l = [sˆRˆ ai]l + tˆl + [φi]l = [s◦R◦ai]l + t◦l + [ i]l [bj]l = [sˆRˆ aj]l + tˆl + [φj]l

∀i ∈ I, l = 1, 2, 3 ,
∀j ∈ Jl, l = 1, 2, 3

(212)

where i is the unknown noise that satisﬁes i ≤ βi and [φi]l and [φj]l are the residuals that can be computed from the TLS estimate and satisfy |[φi]l|≤ βi and |[φj]l|≤ βj. From the ﬁrst equation in (212), we have that:

tˆl − t◦l = [s◦R◦ai]l − [sˆRˆ ai]l + [ i]l − [φi]l ⇒ |tˆl − t◦l |≤ |[s◦R◦ai]l − [sˆRˆ ai]l|+[φi]l + βi = |[s◦R◦ai]l − [sˆRˆ ai]l|+[ζit]l,

(213) (214)

where we have deﬁned a new quantity [ζit]l = [φi]l + βi that can be computed from the solution. Now we want to upper-bound |[s◦R◦ai]l − [sˆRˆai]l|, using a similar SVD-based approach as in eq. (208) (but this time we upper-bound):

([s◦R◦ai]l − [sˆRˆ ai]l)2 ≤ (sˆRˆ − s◦R◦)ai 2= U∆S∆V∆Tai 2= S∆V∆Tai 2 = (sˆ − s◦)2 ai 2+2sˆs◦(1 − cos(θR))((V∆Tai)21 + (V∆Tai)22) ≤ (sˆ − s◦)2 + 2sˆs◦(1 − cos(θR)) ai 2

(215) (216)

Inserting the inequality (216) back into eq. (214), we get the following translation bound for each i ∈ I

|tˆl − t◦l |≤ (sˆ − s◦)2 + 2sˆs◦(1 − cos(θR)) ai 2+[ζit]l, ∀i ∈ I, ∀l = 1, 2, 3,

(217)

there the ﬁnal translation bound is:

|tˆl − t◦l |≤ min (sˆ − s◦)2 + 2sˆs◦(1 − cos(θR)) ai 2+[ζit]l : i ∈ I , ∀l = 1, 2, 3.

(218)

We note that the translation bound depends on both the scale bound (195) and the rotation bound (211). When the true inlier set
I is known, one can directly compute the bound using eq. (217); when the true inlier set I is unknown, one has to enumerate over all possible subsets of 3 inliers in C¯ to compute the worst-case translation bound.

